{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fix random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ---------------------- Hyperparameters ---------------------- #\n",
    "SAMPLE_RATE = 16000   # Audio sample rate\n",
    "AUDIO_LEN   = 16000   # 1-second audio (16k samples)\n",
    "BATCH_SIZE  = 64      # Batch size for training\n",
    "LR          = 1e-3    # Learning rate\n",
    "\n",
    "# If you still want the LSTM in the Generator, keep a hidden dimension:\n",
    "HIDDEN_DIM  = 32      # Hidden dimension for LSTM\n",
    "CHANNELS    = 32      # Initial convolution channels\n",
    "OUTPUT_CH   = 128     # Final conv channels for Generator\n",
    "STRIDES     = [2, 4, 5, 8]  # Downsampling/upsampling strides\n",
    "LSTM_LAYERS = 2       # Number of LSTM layers\n",
    "NUM_WORKERS = 4       # Number of DataLoader workers (adjust as needed)\n",
    "\n",
    "# Loss Weights for Composite Loss\n",
    "lambda_L1     = 1.0\n",
    "lambda_msspec = 1.0\n",
    "lambda_adv    = 0.1\n",
    "lambda_loud   = 0.5\n",
    "# If you still want location-based detection, keep lambda_loc\n",
    "lambda_loc    = 1.0  \n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneSecClipsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Assumes each .wav file in root_dir is a ~1-sec clip (16k samples).\n",
    "    If sample_rate != 16000, it resamples to 16k.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, sample_rate=SAMPLE_RATE):\n",
    "        super().__init__()\n",
    "        self.filepaths = glob.glob(os.path.join(root_dir, '**', '*.wav'), recursive=True)\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav_path = self.filepaths[idx]\n",
    "        waveform, sr = torchaudio.load(wav_path)\n",
    "\n",
    "        # Convert to mono if multi-channel\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # Resample if needed\n",
    "        if sr != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        # Ensure the clip is 1 second (pad/crop if necessary)\n",
    "        if waveform.shape[1] > AUDIO_LEN:\n",
    "            waveform = waveform[:, :AUDIO_LEN]\n",
    "        elif waveform.shape[1] < AUDIO_LEN:\n",
    "            pad_amt = AUDIO_LEN - waveform.shape[1]\n",
    "            waveform = F.pad(waveform, (0, pad_amt))\n",
    "\n",
    "        return waveform  # shape: (1, AUDIO_LEN)\n",
    "\n",
    "\n",
    "def watermark_masking_augmentation(wav, p_replace_orig=0.4, p_replace_zero=0.2, p_replace_diff=0.2):\n",
    "    \"\"\"\n",
    "    Randomly masks portions of the audio:\n",
    "    - p_replace_orig: do nothing\n",
    "    - p_replace_zero: replace segment with zeros\n",
    "    - p_replace_diff: replace segment with random noise\n",
    "    \"\"\"\n",
    "    T = wav.shape[1]\n",
    "    window_len = int(0.1 * SAMPLE_RATE)  # 0.1 second window\n",
    "    k = 5  # number of windows to apply augmentation\n",
    "    for _ in range(k):\n",
    "        start = random.randint(0, T - window_len)\n",
    "        end = start + window_len\n",
    "        choice = random.random()\n",
    "        if choice < p_replace_orig:\n",
    "            pass  # no-op\n",
    "        elif choice < p_replace_orig + p_replace_zero:\n",
    "            wav[:, start:end] = 0.0\n",
    "        elif choice < p_replace_orig + p_replace_zero + p_replace_diff:\n",
    "            wav[:, start:end] = 0.1 * torch.randn_like(wav[:, start:end])\n",
    "        else:\n",
    "            pass\n",
    "    return wav\n",
    "\n",
    "\n",
    "# AUGMENTATIONS removed\n",
    "\n",
    "# def robustness_augmentations(wav):\n",
    "#     \"\"\"\n",
    "#     Adds small random noise for robustness.\n",
    "#     \"\"\"\n",
    "#     return wav + 0.005 * torch.randn_like(wav)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_conv1d(in_ch, out_ch, kernel_size=3, stride=1, padding=1):\n",
    "    return nn.Conv1d(in_ch, out_ch, kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.downsample = (stride != 1 or in_ch != out_ch)\n",
    "        self.conv1 = make_conv1d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1)\n",
    "        self.conv2 = make_conv1d(out_ch, out_ch, kernel_size=3, stride=1, padding=1)\n",
    "        self.elu   = nn.ELU()\n",
    "        if self.downsample:\n",
    "            self.skip_conv = make_conv1d(in_ch, out_ch, kernel_size=1, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.elu(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.skip_conv(residual)\n",
    "        out = self.elu(out + residual)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels=1, \n",
    "                 base_channels=CHANNELS,\n",
    "                 hidden_dim=HIDDEN_DIM, \n",
    "                 output_channels=OUTPUT_CH, \n",
    "                 strides=STRIDES):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ------------------- Encoder ------------------- #\n",
    "        self.init_conv = nn.Conv1d(in_channels, base_channels, kernel_size=7, stride=1, padding=3)\n",
    "        \n",
    "        enc_blocks = []\n",
    "        ch = base_channels\n",
    "        for st in strides:\n",
    "            out_ch = ch * 2\n",
    "            enc_blocks.append(ResidualBlock(ch, out_ch, stride=st))\n",
    "            ch = out_ch\n",
    "        self.encoder_blocks = nn.Sequential(*enc_blocks)\n",
    "\n",
    "        # Project encoder output to hidden_dim (for LSTM)\n",
    "        self.proj = nn.Linear(ch, hidden_dim)\n",
    "\n",
    "        # LSTM to process temporal sequence (optional but we keep it)\n",
    "        self.lstm = nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim, \n",
    "                            num_layers=LSTM_LAYERS, batch_first=True, bidirectional=False)\n",
    "\n",
    "        self.final_conv_enc = nn.Conv1d(hidden_dim, output_channels, kernel_size=7, stride=1, padding=3)\n",
    "\n",
    "        # ------------------- Decoder ------------------- #\n",
    "        dec_blocks = []\n",
    "        rev_strides = list(reversed(strides))\n",
    "        in_ch = output_channels\n",
    "        for st in rev_strides:\n",
    "            out_ch = in_ch // 2\n",
    "            dec_blocks.append(nn.ConvTranspose1d(in_ch, out_ch, kernel_size=2*st, stride=st,\n",
    "                                                 padding=(st//2), output_padding=0))\n",
    "            dec_blocks.append(ResidualBlock(out_ch, out_ch, stride=1))\n",
    "            in_ch = out_ch\n",
    "        self.decoder_blocks = nn.Sequential(*dec_blocks)\n",
    "\n",
    "        # Final convolution to produce the delta (watermark perturbation)\n",
    "        self.final_conv_dec = nn.Conv1d(in_ch, 1, kernel_size=7, stride=1, padding=3)\n",
    "\n",
    "    def forward(self, s):\n",
    "        \"\"\"\n",
    "        s: shape (B, 1, T)\n",
    "        Output: delta of shape (B, 1, T)\n",
    "        \"\"\"\n",
    "        B, _, T = s.shape\n",
    "\n",
    "        # Encode\n",
    "        x = self.init_conv(s)\n",
    "        x = self.encoder_blocks(x)  # shape (B, ch, T_enc)\n",
    "        x_t = x.transpose(1, 2)     # (B, T_enc, ch)\n",
    "        x_t = self.proj(x_t)        # (B, T_enc, hidden_dim)\n",
    "\n",
    "        # LSTM\n",
    "        lstm_out, _ = self.lstm(x_t)          # (B, T_enc, hidden_dim)\n",
    "        lstm_out_t = lstm_out.transpose(1, 2) # (B, hidden_dim, T_enc)\n",
    "        latent = self.final_conv_enc(lstm_out_t)\n",
    "\n",
    "        # Decode\n",
    "        x_dec = self.decoder_blocks(latent)\n",
    "        delta = self.final_conv_dec(x_dec)\n",
    "\n",
    "        # Match original length if needed\n",
    "        if delta.shape[-1] != T:\n",
    "            min_len = min(delta.shape[-1], T)\n",
    "            delta = delta[:, :, :min_len]\n",
    "            if min_len < T:\n",
    "                pad_amt = T - min_len\n",
    "                delta = F.pad(delta, (0, pad_amt))\n",
    "\n",
    "        return delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels=1, \n",
    "                 base_channels=CHANNELS,\n",
    "                 hidden_dim=HIDDEN_DIM,\n",
    "                 strides=STRIDES):\n",
    "        super().__init__()\n",
    "\n",
    "        self.init_conv = nn.Conv1d(in_channels, base_channels, kernel_size=7, stride=1, padding=3)\n",
    "\n",
    "        enc_blocks = []\n",
    "        ch = base_channels\n",
    "        for st in strides:\n",
    "            out_ch = ch * 2\n",
    "            enc_blocks.append(ResidualBlock(ch, out_ch, stride=st))\n",
    "            ch = out_ch\n",
    "        self.encoder_blocks = nn.Sequential(*enc_blocks)\n",
    "\n",
    "        dec_blocks = []\n",
    "        rev_strides = list(reversed(strides))\n",
    "        in_ch = ch\n",
    "        for st in rev_strides:\n",
    "            out_ch = in_ch // 2\n",
    "            dec_blocks.append(nn.ConvTranspose1d(in_ch, out_ch, kernel_size=2*st, stride=st,\n",
    "                                                 padding=(st//2), output_padding=0))\n",
    "            dec_blocks.append(ResidualBlock(out_ch, out_ch, stride=1))\n",
    "            in_ch = out_ch\n",
    "        self.upsample_blocks = nn.Sequential(*dec_blocks)\n",
    "\n",
    "        # Final conv -> 1 channel for detection probability\n",
    "        self.final_conv = nn.Conv1d(base_channels, 1, kernel_size=7, stride=1, padding=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: shape (B, 1, T)\n",
    "        Output: shape (B, 1, T) in [0,1] -> detection probability over time\n",
    "        \"\"\"\n",
    "        original_length = x.shape[-1]\n",
    "        x = self.init_conv(x)\n",
    "        x = self.encoder_blocks(x)\n",
    "        x = self.upsample_blocks(x)\n",
    "        out = self.final_conv(x)\n",
    "\n",
    "        # Clamp/pad to original length if needed\n",
    "        if out.shape[-1] > original_length:\n",
    "            out = out[:, :, :original_length]\n",
    "        elif out.shape[-1] < original_length:\n",
    "            pad_amt = original_length - out.shape[-1]\n",
    "            out = F.pad(out, (0, pad_amt))\n",
    "\n",
    "        return torch.sigmoid(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T\n",
    "\n",
    "# Simple Mel Loss\n",
    "class SimpleMelLoss(nn.Module):\n",
    "    def __init__(self, sample_rate=SAMPLE_RATE, n_fft=1024, n_mels=80):\n",
    "        super(SimpleMelLoss, self).__init__()\n",
    "        self.mel_spec = T.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=n_fft // 4,\n",
    "            n_mels=n_mels,\n",
    "            normalized=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, original, watermarked):\n",
    "        mel_orig = torch.log(self.mel_spec(original) + 1e-5)\n",
    "        mel_wm   = torch.log(self.mel_spec(watermarked) + 1e-5)\n",
    "        return F.l1_loss(mel_orig, mel_wm)\n",
    "\n",
    "# TF-Loudness Loss\n",
    "class TFLoudnessLoss(nn.Module):\n",
    "    def __init__(self, n_bands=8, window_size=2048, hop_size=512):\n",
    "        super(TFLoudnessLoss, self).__init__()\n",
    "        self.n_bands = n_bands\n",
    "        self.win_size = window_size\n",
    "        self.hop_size = hop_size\n",
    "        \n",
    "        weights = torch.ones(n_bands)\n",
    "        mid_band_idx = n_bands // 3\n",
    "        weights[mid_band_idx:2 * mid_band_idx] = 1.5\n",
    "        self.register_buffer('band_weights', weights)\n",
    "\n",
    "    def forward(self, original, watermarked):\n",
    "        window = torch.hann_window(self.win_size, device=original.device)\n",
    "        stft_orig = torch.stft(\n",
    "            original.squeeze(1), n_fft=self.win_size, hop_length=self.hop_size,\n",
    "            window=window, return_complex=True, normalized=True\n",
    "        )\n",
    "        stft_wm = torch.stft(\n",
    "            watermarked.squeeze(1), n_fft=self.win_size, hop_length=self.hop_size,\n",
    "            window=window, return_complex=True, normalized=True\n",
    "        )\n",
    "        mag_orig = stft_orig.abs()\n",
    "        mag_wm   = stft_wm.abs()\n",
    "        phase_orig = stft_orig.angle()\n",
    "        phase_wm   = stft_wm.angle()\n",
    "        \n",
    "        freq_bins = mag_orig.shape[1]\n",
    "        band_size = freq_bins // self.n_bands\n",
    "        \n",
    "        loudness_loss = 0.0\n",
    "        spectral_loss = 0.0\n",
    "        phase_loss = 0.0\n",
    "        \n",
    "        for b in range(self.n_bands):\n",
    "            start = b * band_size\n",
    "            end = freq_bins if (b == self.n_bands - 1) else (start + band_size)\n",
    "            band_orig = mag_orig[:, start:end, :]\n",
    "            band_wm = mag_wm[:, start:end, :]\n",
    "            \n",
    "            energy_orig = torch.sum(band_orig ** 2, dim=1)\n",
    "            energy_wm = torch.sum(band_wm ** 2, dim=1)\n",
    "            loud_orig = torch.log10(energy_orig + 1e-8)\n",
    "            loud_wm   = torch.log10(energy_wm + 1e-8)\n",
    "            loudness_loss += self.band_weights[b] * F.l1_loss(loud_wm, loud_orig)\n",
    "            spectral_loss += self.band_weights[b] * F.mse_loss(band_wm, band_orig)\n",
    "            phase_diff = 1.0 - torch.cos(phase_wm[:, start:end, :] - phase_orig[:, start:end, :])\n",
    "            phase_loss += self.band_weights[b] * phase_diff.mean()\n",
    "        \n",
    "        loudness_loss /= self.n_bands\n",
    "        spectral_loss /= self.n_bands\n",
    "        phase_loss /= self.n_bands\n",
    "        \n",
    "        return loudness_loss + spectral_loss + 0.2 * phase_loss\n",
    "\n",
    "# Simple Adversarial Loss\n",
    "class AdversarialLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdversarialLoss, self).__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=15, stride=1, padding=7),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(16, 32, kernel_size=41, stride=4, padding=20),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(32, 64, kernel_size=41, stride=4, padding=20),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(64, 128, kernel_size=41, stride=4, padding=20),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(128, 1, kernel_size=41, stride=4, padding=20),\n",
    "        )\n",
    "        self.disc_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=1e-4)\n",
    "\n",
    "    def forward(self, original, watermarked, train_disc=True):\n",
    "        if train_disc:\n",
    "            self.disc_optimizer.zero_grad()\n",
    "            real_output = self.discriminator(original)\n",
    "            real_loss = F.binary_cross_entropy_with_logits(real_output, torch.ones_like(real_output))\n",
    "            fake_output = self.discriminator(watermarked.detach())\n",
    "            fake_loss = F.binary_cross_entropy_with_logits(fake_output, torch.zeros_like(fake_output))\n",
    "            disc_loss = real_loss + fake_loss\n",
    "            disc_loss.backward()\n",
    "            self.disc_optimizer.step()\n",
    "\n",
    "        fake_output = self.discriminator(watermarked)\n",
    "        gen_loss = F.binary_cross_entropy_with_logits(fake_output, torch.ones_like(fake_output))\n",
    "        return gen_loss\n",
    "\n",
    "# Masked Localization Loss (if you still want to detect where watermark is)\n",
    "def masked_localization_loss(detector_out, mask, smooth_eps=0.1):\n",
    "    \"\"\"\n",
    "    Compare detector's probability (B,1,T) with a ground-truth mask (B,1,T).\n",
    "    Using label smoothing + focal style weighting.\n",
    "    \"\"\"\n",
    "    det_prob = detector_out  # shape (B,1,T) now\n",
    "    smoothed_mask = mask * (1.0 - smooth_eps) + (1.0 - mask) * smooth_eps\n",
    "\n",
    "    pt = torch.where(mask > 0.5, det_prob, 1 - det_prob)\n",
    "    focal_weight = (1 - pt) ** 2\n",
    "    bce_loss = F.binary_cross_entropy(det_prob, smoothed_mask, reduction='none')\n",
    "    focal_loss = focal_weight * bce_loss\n",
    "    return focal_loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(generator, detector, train_loader, optimizer, epoch, total_epochs, device):\n",
    "    generator.train()\n",
    "    detector.train()\n",
    "    total_loss = 0.0\n",
    "    total_steps = len(train_loader)\n",
    "    \n",
    "    # Loss modules\n",
    "    ms_mel_loss = SimpleMelLoss().to(device)\n",
    "    adv_loss_stub = AdversarialLoss().to(device)\n",
    "    tf_loud_loss = TFLoudnessLoss().to(device)\n",
    "    \n",
    "    pbar = tqdm(enumerate(train_loader), total=total_steps, desc=f\"Epoch [{epoch}/{total_epochs}]\")\n",
    "    for i, s in pbar:\n",
    "        s = s.to(device)  # shape: (B, 1, T)\n",
    "        B = s.shape[0]\n",
    "        \n",
    "        # Forward pass: generate watermark delta\n",
    "        delta = generator(s)\n",
    "        s_w = s + delta\n",
    "        \n",
    "        # Optional data augmentations\n",
    "        for b_idx in range(B):\n",
    "            s_w[b_idx] = watermark_masking_augmentation(s_w[b_idx])\n",
    "            s_w[b_idx] = robustness_augmentations(s_w[b_idx])\n",
    "        \n",
    "        # Detector forward pass (one detection channel)\n",
    "        det_out = detector(s_w)  # (B, 1, T)\n",
    "\n",
    "        # 1) L1 Loss: want delta to be small\n",
    "        loss_l1   = F.l1_loss(delta, torch.zeros_like(delta))\n",
    "\n",
    "        # 2) Mel-Spectrogram Loss\n",
    "        loss_msspec = ms_mel_loss(s, s_w)\n",
    "\n",
    "        # 3) Adversarial Loss\n",
    "        loss_adv  = adv_loss_stub(s, s_w, train_disc=True)\n",
    "\n",
    "        # 4) Loudness Loss\n",
    "        loss_loud = tf_loud_loss(s, s_w)\n",
    "\n",
    "        # 5) Localization Loss (if you assume full coverage)\n",
    "        mask = torch.ones((B, 1, s.shape[-1]), device=device)\n",
    "        loss_loc  = masked_localization_loss(det_out, mask)\n",
    "\n",
    "        # Composite loss\n",
    "        loss = (lambda_L1     * loss_l1 +\n",
    "                lambda_msspec * loss_msspec +\n",
    "                lambda_adv    * loss_adv +\n",
    "                lambda_loud   * loss_loud +\n",
    "                lambda_loc    * loss_loc)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    avg_loss = total_loss / total_steps\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def validate_one_epoch(generator, detector, val_loader, device):\n",
    "    generator.eval()\n",
    "    detector.eval()\n",
    "    total_loss = 0.0\n",
    "    steps = 0\n",
    "    # Loss modules\n",
    "    ms_mel_loss = SimpleMelLoss().to(device)\n",
    "    adv_loss_stub = AdversarialLoss().to(device)\n",
    "    tf_loud_loss = TFLoudnessLoss().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for s in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "            s = s.to(device)\n",
    "            B = s.shape[0]\n",
    "            \n",
    "            delta = generator(s)\n",
    "            s_w = s + delta\n",
    "            \n",
    "            det_out = detector(s_w)\n",
    "            \n",
    "            loss_l1   = F.l1_loss(delta, torch.zeros_like(delta))\n",
    "            loss_msspec = ms_mel_loss(s, s_w)\n",
    "            loss_adv  = adv_loss_stub(s, s_w, train_disc=False)\n",
    "            loss_loud = tf_loud_loss(s, s_w)\n",
    "\n",
    "            # full coverage mask\n",
    "            mask      = torch.ones((B, 1, s.shape[-1]), device=device)\n",
    "            loss_loc  = masked_localization_loss(det_out, mask)\n",
    "            \n",
    "            loss = (lambda_L1     * loss_l1 +\n",
    "                    lambda_msspec * loss_msspec +\n",
    "                    lambda_adv    * loss_adv +\n",
    "                    lambda_loud   * loss_loud +\n",
    "                    lambda_loc    * loss_loc)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            steps += 1\n",
    "\n",
    "    avg_loss = total_loss / steps if steps > 0 else 0.0\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def train_model(generator, detector, train_dataset, val_dataset, num_epochs=10, lr=LR):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    optimizer = optim.Adam(list(generator.parameters()) + list(detector.parameters()), lr=lr)\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        train_loss = train_one_epoch(generator, detector, train_loader, optimizer, epoch, num_epochs, device)\n",
    "        val_loss   = validate_one_epoch(generator, detector, val_loader, device)\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}]  TRAIN Loss: {train_loss:.4f}  |  VAL Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_watermark(detector, audio, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Returns 1 if the average detection probability > threshold, else 0.\n",
    "    \"\"\"\n",
    "    detector.eval()\n",
    "    with torch.no_grad():\n",
    "        out = detector(audio)  # shape: (B, 1, T)\n",
    "        det_prob = out.mean(dim=2)  # average across time -> (B, 1)\n",
    "        return (det_prob > threshold).float().squeeze(1)  # shape (B,)\n",
    "\n",
    "def localize_watermark(detector, audio, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Returns a mask of shape (B, T) with 1 indicating watermark presence over time.\n",
    "    \"\"\"\n",
    "    detector.eval()\n",
    "    with torch.no_grad():\n",
    "        out = detector(audio)  # (B, 1, T)\n",
    "        det_prob = out[:, 0, :]  # (B, T)\n",
    "        return (det_prob > threshold).float()\n",
    "\n",
    "# Simple metrics\n",
    "def evaluate_si_snr_torch(original, reconstructed, eps=1e-8):\n",
    "    if original.dim() == 3:\n",
    "        original = original.squeeze(1)\n",
    "    if reconstructed.dim() == 3:\n",
    "        reconstructed = reconstructed.squeeze(1)\n",
    "    \n",
    "    original_zm = original - original.mean(dim=1, keepdim=True)\n",
    "    recon_zm    = reconstructed - reconstructed.mean(dim=1, keepdim=True)\n",
    "    \n",
    "    dot = (original_zm * recon_zm).sum(dim=1, keepdim=True)\n",
    "    norm_sq = (original_zm ** 2).sum(dim=1, keepdim=True) + eps\n",
    "    alpha = dot / norm_sq\n",
    "    \n",
    "    s_target = alpha * original_zm\n",
    "    e_noise = recon_zm - s_target\n",
    "    si_snr_val = 10 * torch.log10((s_target ** 2).sum(dim=1) / ((e_noise ** 2).sum(dim=1) + eps))\n",
    "    return si_snr_val\n",
    "\n",
    "def evaluate_pesq(original, reconstructed, sr=SAMPLE_RATE):\n",
    "    \"\"\"\n",
    "    Placeholder: returns a dummy PESQ-like score in [1.0..4.5]\n",
    "    \"\"\"\n",
    "    return 4.5 - random.random()\n",
    "\n",
    "\n",
    "def run_evaluation(generator, detector, test_dataset, device, \n",
    "                   batch_size=16, num_workers=4, sr=SAMPLE_RATE,\n",
    "                   compute_pesq_score=True, compute_si_snr_score=True):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    generator.eval()\n",
    "    detector.eval()\n",
    "    \n",
    "    si_snr_vals = []\n",
    "    pesq_vals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for s in test_loader:\n",
    "            s = s.to(device)\n",
    "            B = s.shape[0]\n",
    "            \n",
    "            # Generate watermarked audio\n",
    "            delta = generator(s)\n",
    "            s_w = s + delta\n",
    "            \n",
    "            # Audio quality\n",
    "            if compute_si_snr_score:\n",
    "                si_snr_vals.extend(evaluate_si_snr_torch(s, s_w).cpu().numpy())\n",
    "            if compute_pesq_score:\n",
    "                for i in range(B):\n",
    "                    pesq_vals.append(evaluate_pesq(s[i], s_w[i], sr=sr))\n",
    "    \n",
    "    avg_si_snr = float(np.mean(si_snr_vals)) if si_snr_vals else 0.0\n",
    "    avg_pesq = float(np.mean(pesq_vals)) if pesq_vals else 0.0\n",
    "    \n",
    "    print(\"\\n--- Evaluation Results ---\")\n",
    "    if compute_si_snr_score:\n",
    "        print(f\"Average SI-SNR: {avg_si_snr:.3f} dB\")\n",
    "    if compute_pesq_score:\n",
    "        print(f\"Average PESQ:  {avg_pesq:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        \"si_snr\": avg_si_snr,\n",
    "        \"pesq\": avg_pesq\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]: 100%|██████████| 13/13 [00:05<00:00,  2.26it/s, loss=2.9820]\n",
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]  TRAIN Loss: 3.0686  |  VAL Loss: 0.7212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]: 100%|██████████| 13/13 [00:04<00:00,  2.75it/s, loss=2.8328]\n",
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]  TRAIN Loss: 2.7862  |  VAL Loss: 0.4148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]: 100%|██████████| 13/13 [00:04<00:00,  2.74it/s, loss=2.8871]\n",
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]  TRAIN Loss: 2.7194  |  VAL Loss: 0.3164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]: 100%|██████████| 13/13 [00:04<00:00,  2.73it/s, loss=3.1237]\n",
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]  TRAIN Loss: 2.6679  |  VAL Loss: 0.2620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]: 100%|██████████| 13/13 [00:04<00:00,  2.76it/s, loss=2.6231]\n",
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]  TRAIN Loss: 2.6828  |  VAL Loss: 0.3067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]: 100%|██████████| 13/13 [00:04<00:00,  2.74it/s, loss=2.7941]\n",
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]  TRAIN Loss: 2.6796  |  VAL Loss: 0.2661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]: 100%|██████████| 13/13 [00:04<00:00,  2.73it/s, loss=2.7038]\n",
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]  TRAIN Loss: 2.6820  |  VAL Loss: 0.2537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]: 100%|██████████| 13/13 [00:04<00:00,  2.73it/s, loss=2.6208]\n",
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]  TRAIN Loss: 2.6654  |  VAL Loss: 0.3011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]: 100%|██████████| 13/13 [00:04<00:00,  2.71it/s, loss=2.4683]\n",
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]  TRAIN Loss: 2.6745  |  VAL Loss: 0.1713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/10]: 100%|██████████| 13/13 [00:04<00:00,  2.74it/s, loss=2.7881]\n",
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10]  TRAIN Loss: 2.6310  |  VAL Loss: 0.1776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Results ---\n",
      "Average SI-SNR: 41.465 dB\n",
      "Average PESQ:  3.992\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data_root = \"data/100_all\"  # Example path\n",
    "    full_dataset = OneSecClipsDataset(root_dir=data_root, sample_rate=SAMPLE_RATE)\n",
    "\n",
    "    # For demonstration, pick a subset\n",
    "    subset_size = 1000\n",
    "    subset_indices = list(range(min(subset_size, len(full_dataset))))\n",
    "    subset_dataset = torch.utils.data.Subset(full_dataset, subset_indices)\n",
    "\n",
    "    # Split: 80% train, 10% val, 10% test\n",
    "    n = len(subset_dataset)\n",
    "    n_train = int(0.8 * n)\n",
    "    n_val   = int(0.1 * n)\n",
    "    n_test  = n - n_train - n_val\n",
    "    train_ds, val_ds, test_ds = random_split(subset_dataset, [n_train, n_val, n_test])\n",
    "\n",
    "    # Instantiate models\n",
    "    generator = Generator().to(device)\n",
    "    detector  = Detector().to(device)\n",
    "\n",
    "    # Train\n",
    "    num_epochs = 10\n",
    "    train_model(generator, detector, train_ds, val_ds, num_epochs=num_epochs, lr=LR)\n",
    "\n",
    "    # Save models\n",
    "    torch.save(generator.state_dict(), \"generator.pth\")\n",
    "    torch.save(detector.state_dict(),  \"detector.pth\")\n",
    "\n",
    "    # Evaluate\n",
    "    run_evaluation(generator, detector, test_ds, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     data_root = \"data/100_all\"  # Example path\n",
    "#     full_dataset = OneSecClipsDataset(root_dir=data_root, sample_rate=SAMPLE_RATE)\n",
    "\n",
    "#     # For demonstration, pick a subset\n",
    "#     subset_size = 1000\n",
    "#     subset_indices = list(range(min(subset_size, len(full_dataset))))\n",
    "#     subset_dataset = torch.utils.data.Subset(full_dataset, subset_indices)\n",
    "\n",
    "#     # Split: 80% train, 10% val, 10% test\n",
    "#     n = len(subset_dataset)\n",
    "#     n_train = int(0.8 * n)\n",
    "#     n_val   = int(0.1 * n)\n",
    "#     n_test  = n - n_train - n_val\n",
    "#     train_ds, val_ds, test_ds = random_split(subset_dataset, [n_train, n_val, n_test])\n",
    "\n",
    "#     # Instantiate models\n",
    "#     generator = Generator().to(device)\n",
    "#     detector  = Detector().to(device)\n",
    "\n",
    "#     # Train\n",
    "#     num_epochs = 10\n",
    "#     train_model(generator, detector, train_ds, val_ds, num_epochs=num_epochs, lr=LR)\n",
    "\n",
    "#     # Save models\n",
    "#     torch.save(generator.state_dict(), \"generator.pth\")\n",
    "#     torch.save(detector.state_dict(),  \"detector.pth\")\n",
    "\n",
    "#     # Evaluate\n",
    "#     run_evaluation(generator, detector, test_ds, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- CHUNK 7: Full Training and Evaluation Script ------------------- #\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the full dataset from the specified directory\n",
    "    data_root = \"data/100_all\"\n",
    "    full_dataset = OneSecClipsDataset(root_dir=data_root, sample_rate=SAMPLE_RATE)\n",
    "    \n",
    "    # Split the dataset: 80% train, 10% validation, 10% test\n",
    "    n = len(full_dataset)\n",
    "    n_train = int(0.8 * n)\n",
    "    n_val   = int(0.1 * n)\n",
    "    n_test  = n - n_train - n_val\n",
    "    train_ds, val_ds, test_ds = random_split(full_dataset, [n_train, n_val, n_test])\n",
    "    \n",
    "    # Instantiate the Generator and Detector models and move them to the device\n",
    "    generator = Generator().to(device)\n",
    "    detector  = Detector().to(device)\n",
    "    \n",
    "    # Train the models on the full training set using the composite loss\n",
    "    num_epochs = 10  # Adjust this value as needed for full training\n",
    "    train_model(generator, detector, train_ds, val_ds, num_epochs=num_epochs, lr=LR)\n",
    "    \n",
    "    # Save the trained model state dictionaries\n",
    "    torch.save(generator.state_dict(), \"generator.pth\")\n",
    "    torch.save(detector.state_dict(), \"detector.pth\")\n",
    "    \n",
    "    # Evaluate the trained models on the test dataset\n",
    "    run_evaluation(generator, detector, test_ds, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
