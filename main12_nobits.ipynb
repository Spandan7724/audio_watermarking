{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fix random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ---------------------- Hyperparameters ---------------------- #\n",
    "SAMPLE_RATE = 16000   # Audio sample rate\n",
    "AUDIO_LEN   = 16000   # 1-second audio (16k samples)\n",
    "BATCH_SIZE  = 32      # Batch size for training\n",
    "LR          = 1e-3    # Learning rate\n",
    "HIDDEN_DIM  = 32      # Hidden dimension for LSTM\n",
    "CHANNELS    = 32      # Initial convolution channels\n",
    "OUTPUT_CH   = 128     # Final conv channels for Generator\n",
    "STRIDES     = [2, 4, 5, 8]  # Downsampling/upsampling strides\n",
    "LSTM_LAYERS = 2       # Number of LSTM layers\n",
    "NUM_WORKERS = 4       # Number of DataLoader workers (adjust as needed)\n",
    "\n",
    "# Loss Weights for Composite Loss\n",
    "lambda_L1     = 1.0\n",
    "lambda_msspec = 1.0\n",
    "lambda_adv    = 0.1\n",
    "lambda_loud   = 0.5\n",
    "lambda_loc    = 1.0  # Used for detection/localization BCE\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneSecClipsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Assumes each .wav file in root_dir is a ~1-sec clip (16k samples).\n",
    "    If sample_rate != 16000, it resamples to 16k.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, sample_rate=SAMPLE_RATE):\n",
    "        super().__init__()\n",
    "        self.filepaths = glob.glob(os.path.join(root_dir, '**', '*.wav'), recursive=True)\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav_path = self.filepaths[idx]\n",
    "        waveform, sr = torchaudio.load(wav_path)\n",
    "\n",
    "        # Convert to mono if multi-channel\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # Resample if needed\n",
    "        if sr != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        # Ensure the clip is 1 second (pad/crop if necessary)\n",
    "        if waveform.shape[1] > AUDIO_LEN:\n",
    "            waveform = waveform[:, :AUDIO_LEN]\n",
    "        elif waveform.shape[1] < AUDIO_LEN:\n",
    "            pad_amt = AUDIO_LEN - waveform.shape[1]\n",
    "            waveform = F.pad(waveform, (0, pad_amt))\n",
    "\n",
    "        return waveform  # shape: (1, AUDIO_LEN)\n",
    "\n",
    "\n",
    "def watermark_masking_augmentation(wav, p_replace_orig=0.4, p_replace_zero=0.2, p_replace_diff=0.2):\n",
    "    \"\"\"\n",
    "    Randomly masks portions of the audio:\n",
    "    - p_replace_orig: do nothing\n",
    "    - p_replace_zero: replace segment with zeros\n",
    "    - p_replace_diff: replace segment with random noise\n",
    "    \"\"\"\n",
    "    T = wav.shape[1]\n",
    "    window_len = int(0.1 * SAMPLE_RATE)  # 0.1 second window\n",
    "    k = 5  # number of windows to apply augmentation\n",
    "    for _ in range(k):\n",
    "        start = random.randint(0, T - window_len)\n",
    "        end = start + window_len\n",
    "        choice = random.random()\n",
    "        if choice < p_replace_orig:\n",
    "            pass  # no-op\n",
    "        elif choice < p_replace_orig + p_replace_zero:\n",
    "            wav[:, start:end] = 0.0\n",
    "        elif choice < p_replace_orig + p_replace_zero + p_replace_diff:\n",
    "            wav[:, start:end] = 0.1 * torch.randn_like(wav[:, start:end])\n",
    "        else:\n",
    "            pass\n",
    "    return wav\n",
    "#AUGMENTATIONS removed \n",
    "\n",
    "# def robustness_augmentations(wav):\n",
    "#     \"\"\"\n",
    "#     Adds small random noise for robustness.\n",
    "#     \"\"\"\n",
    "#     return wav + 0.005 * torch.randn_like(wav)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_conv1d(in_ch, out_ch, kernel_size=3, stride=1, padding=1):\n",
    "    return nn.Conv1d(in_ch, out_ch, kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.downsample = (stride != 1 or in_ch != out_ch)\n",
    "        self.conv1 = make_conv1d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1)\n",
    "        self.conv2 = make_conv1d(out_ch, out_ch, kernel_size=3, stride=1, padding=1)\n",
    "        self.elu   = nn.ELU()\n",
    "        if self.downsample:\n",
    "            self.skip_conv = make_conv1d(in_ch, out_ch, kernel_size=1, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.elu(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.skip_conv(residual)\n",
    "        out = self.elu(out + residual)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels=1, \n",
    "                 base_channels=CHANNELS,\n",
    "                 hidden_dim=HIDDEN_DIM, \n",
    "                 output_channels=OUTPUT_CH, \n",
    "                 strides=STRIDES):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ------------------- Encoder ------------------- #\n",
    "        self.init_conv = nn.Conv1d(in_channels, base_channels, kernel_size=7, stride=1, padding=3)\n",
    "        \n",
    "        enc_blocks = []\n",
    "        ch = base_channels\n",
    "        for st in strides:\n",
    "            out_ch = ch * 2\n",
    "            enc_blocks.append(ResidualBlock(ch, out_ch, stride=st))\n",
    "            ch = out_ch\n",
    "        self.encoder_blocks = nn.Sequential(*enc_blocks)\n",
    "\n",
    "        # Project encoder output to hidden_dim (for LSTM)\n",
    "        self.proj = nn.Linear(ch, hidden_dim)\n",
    "\n",
    "        # LSTM to process temporal sequence (optional but we keep it)\n",
    "        self.lstm = nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim, \n",
    "                            num_layers=LSTM_LAYERS, batch_first=True, bidirectional=False)\n",
    "\n",
    "        self.final_conv_enc = nn.Conv1d(hidden_dim, output_channels, kernel_size=7, stride=1, padding=3)\n",
    "\n",
    "        # ------------------- Decoder ------------------- #\n",
    "        dec_blocks = []\n",
    "        rev_strides = list(reversed(strides))\n",
    "        in_ch = output_channels\n",
    "        for st in rev_strides:\n",
    "            out_ch = in_ch // 2\n",
    "            dec_blocks.append(nn.ConvTranspose1d(in_ch, out_ch, kernel_size=2*st, stride=st,\n",
    "                                                 padding=(st//2), output_padding=0))\n",
    "            dec_blocks.append(ResidualBlock(out_ch, out_ch, stride=1))\n",
    "            in_ch = out_ch\n",
    "        self.decoder_blocks = nn.Sequential(*dec_blocks)\n",
    "\n",
    "        # Final convolution to produce the delta (watermark perturbation)\n",
    "        self.final_conv_dec = nn.Conv1d(in_ch, 1, kernel_size=7, stride=1, padding=3)\n",
    "\n",
    "    def forward(self, s):\n",
    "        \"\"\"\n",
    "        s: shape (B, 1, T)\n",
    "        Output: delta of shape (B, 1, T)\n",
    "        \"\"\"\n",
    "        B, _, T = s.shape\n",
    "\n",
    "        # Encode\n",
    "        x = self.init_conv(s)\n",
    "        x = self.encoder_blocks(x)  # shape (B, ch, T_enc)\n",
    "        x_t = x.transpose(1, 2)     # (B, T_enc, ch)\n",
    "        x_t = self.proj(x_t)        # (B, T_enc, hidden_dim)\n",
    "\n",
    "        # LSTM\n",
    "        lstm_out, _ = self.lstm(x_t)          # (B, T_enc, hidden_dim)\n",
    "        lstm_out_t = lstm_out.transpose(1, 2) # (B, hidden_dim, T_enc)\n",
    "        latent = self.final_conv_enc(lstm_out_t)\n",
    "\n",
    "        # Decode\n",
    "        x_dec = self.decoder_blocks(latent)\n",
    "        delta = self.final_conv_dec(x_dec)\n",
    "\n",
    "        # Match original length if needed\n",
    "        if delta.shape[-1] != T:\n",
    "            min_len = min(delta.shape[-1], T)\n",
    "            delta = delta[:, :, :min_len]\n",
    "            if min_len < T:\n",
    "                pad_amt = T - min_len\n",
    "                delta = F.pad(delta, (0, pad_amt))\n",
    "\n",
    "        return delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels=1, \n",
    "                 base_channels=CHANNELS,\n",
    "                 hidden_dim=HIDDEN_DIM,\n",
    "                 strides=STRIDES):\n",
    "        super().__init__()\n",
    "\n",
    "        self.init_conv = nn.Conv1d(in_channels, base_channels, kernel_size=7, stride=1, padding=3)\n",
    "\n",
    "        enc_blocks = []\n",
    "        ch = base_channels\n",
    "        for st in strides:\n",
    "            out_ch = ch * 2\n",
    "            enc_blocks.append(ResidualBlock(ch, out_ch, stride=st))\n",
    "            ch = out_ch\n",
    "        self.encoder_blocks = nn.Sequential(*enc_blocks)\n",
    "\n",
    "        dec_blocks = []\n",
    "        rev_strides = list(reversed(strides))\n",
    "        in_ch = ch\n",
    "        for st in rev_strides:\n",
    "            out_ch = in_ch // 2\n",
    "            dec_blocks.append(nn.ConvTranspose1d(in_ch, out_ch, kernel_size=2*st, stride=st,\n",
    "                                                 padding=(st//2), output_padding=0))\n",
    "            dec_blocks.append(ResidualBlock(out_ch, out_ch, stride=1))\n",
    "            in_ch = out_ch\n",
    "        self.upsample_blocks = nn.Sequential(*dec_blocks)\n",
    "\n",
    "        # Final conv -> 1 channel for detection probability\n",
    "        self.final_conv = nn.Conv1d(base_channels, 1, kernel_size=7, stride=1, padding=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: shape (B, 1, T)\n",
    "        Output: shape (B, 1, T) in [0,1] -> detection probability over time\n",
    "        \"\"\"\n",
    "        original_length = x.shape[-1]\n",
    "        x = self.init_conv(x)\n",
    "        x = self.encoder_blocks(x)\n",
    "        x = self.upsample_blocks(x)\n",
    "        out = self.final_conv(x)\n",
    "\n",
    "        # Clamp/pad to original length if needed\n",
    "        if out.shape[-1] > original_length:\n",
    "            out = out[:, :, :original_length]\n",
    "        elif out.shape[-1] < original_length:\n",
    "            pad_amt = original_length - out.shape[-1]\n",
    "            out = F.pad(out, (0, pad_amt))\n",
    "\n",
    "        return torch.sigmoid(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T\n",
    "\n",
    "# Simple Mel Loss\n",
    "class SimpleMelLoss(nn.Module):\n",
    "    def __init__(self, sample_rate=SAMPLE_RATE, n_fft=1024, n_mels=80):\n",
    "        super(SimpleMelLoss, self).__init__()\n",
    "        self.mel_spec = T.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=n_fft // 4,\n",
    "            n_mels=n_mels,\n",
    "            normalized=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, original, watermarked):\n",
    "        mel_orig = torch.log(self.mel_spec(original) + 1e-5)\n",
    "        mel_wm   = torch.log(self.mel_spec(watermarked) + 1e-5)\n",
    "        return F.l1_loss(mel_orig, mel_wm)\n",
    "\n",
    "# TF-Loudness Loss\n",
    "class TFLoudnessLoss(nn.Module):\n",
    "    def __init__(self, n_bands=8, window_size=2048, hop_size=512):\n",
    "        super(TFLoudnessLoss, self).__init__()\n",
    "        self.n_bands = n_bands\n",
    "        self.win_size = window_size\n",
    "        self.hop_size = hop_size\n",
    "        \n",
    "        weights = torch.ones(n_bands)\n",
    "        mid_band_idx = n_bands // 3\n",
    "        weights[mid_band_idx:2 * mid_band_idx] = 1.5\n",
    "        self.register_buffer('band_weights', weights)\n",
    "\n",
    "    def forward(self, original, watermarked):\n",
    "        window = torch.hann_window(self.win_size, device=original.device)\n",
    "        stft_orig = torch.stft(\n",
    "            original.squeeze(1), n_fft=self.win_size, hop_length=self.hop_size,\n",
    "            window=window, return_complex=True, normalized=True\n",
    "        )\n",
    "        stft_wm = torch.stft(\n",
    "            watermarked.squeeze(1), n_fft=self.win_size, hop_length=self.hop_size,\n",
    "            window=window, return_complex=True, normalized=True\n",
    "        )\n",
    "        mag_orig = stft_orig.abs()\n",
    "        mag_wm   = stft_wm.abs()\n",
    "        phase_orig = stft_orig.angle()\n",
    "        phase_wm   = stft_wm.angle()\n",
    "        \n",
    "        freq_bins = mag_orig.shape[1]\n",
    "        band_size = freq_bins // self.n_bands\n",
    "        \n",
    "        loudness_loss = 0.0\n",
    "        spectral_loss = 0.0\n",
    "        phase_loss = 0.0\n",
    "        \n",
    "        for b in range(self.n_bands):\n",
    "            start = b * band_size\n",
    "            end = freq_bins if (b == self.n_bands - 1) else (start + band_size)\n",
    "            band_orig = mag_orig[:, start:end, :]\n",
    "            band_wm = mag_wm[:, start:end, :]\n",
    "            \n",
    "            energy_orig = torch.sum(band_orig ** 2, dim=1)\n",
    "            energy_wm = torch.sum(band_wm ** 2, dim=1)\n",
    "            loud_orig = torch.log10(energy_orig + 1e-8)\n",
    "            loud_wm   = torch.log10(energy_wm + 1e-8)\n",
    "            loudness_loss += self.band_weights[b] * F.l1_loss(loud_wm, loud_orig)\n",
    "            spectral_loss += self.band_weights[b] * F.mse_loss(band_wm, band_orig)\n",
    "            phase_diff = 1.0 - torch.cos(phase_wm[:, start:end, :] - phase_orig[:, start:end, :])\n",
    "            phase_loss += self.band_weights[b] * phase_diff.mean()\n",
    "        \n",
    "        loudness_loss /= self.n_bands\n",
    "        spectral_loss /= self.n_bands\n",
    "        phase_loss /= self.n_bands\n",
    "        \n",
    "        return loudness_loss + spectral_loss + 0.2 * phase_loss\n",
    "\n",
    "# Simple Adversarial Loss\n",
    "class AdversarialLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdversarialLoss, self).__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=15, stride=1, padding=7),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(16, 32, kernel_size=41, stride=4, padding=20),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(32, 64, kernel_size=41, stride=4, padding=20),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(64, 128, kernel_size=41, stride=4, padding=20),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(128, 1, kernel_size=41, stride=4, padding=20),\n",
    "        )\n",
    "        self.disc_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=1e-4)\n",
    "\n",
    "    def forward(self, original, watermarked, train_disc=True):\n",
    "        if train_disc:\n",
    "            self.disc_optimizer.zero_grad()\n",
    "            real_output = self.discriminator(original)\n",
    "            real_loss = F.binary_cross_entropy_with_logits(real_output, torch.ones_like(real_output))\n",
    "            fake_output = self.discriminator(watermarked.detach())\n",
    "            fake_loss = F.binary_cross_entropy_with_logits(fake_output, torch.zeros_like(fake_output))\n",
    "            disc_loss = real_loss + fake_loss\n",
    "            disc_loss.backward()\n",
    "            self.disc_optimizer.step()\n",
    "\n",
    "        fake_output = self.discriminator(watermarked)\n",
    "        gen_loss = F.binary_cross_entropy_with_logits(fake_output, torch.ones_like(fake_output))\n",
    "        return gen_loss\n",
    "\n",
    "def masked_localization_loss(detector_out, mask, smooth_eps=0.1):\n",
    "    \"\"\"\n",
    "    Per-sample BCE for detection. 'mask' can be 1 for watermarked, 0 for clean.\n",
    "    \"\"\"\n",
    "    # detector_out: (B,1,T)\n",
    "    # mask: (B,1,T)\n",
    "    det_prob = detector_out\n",
    "    smoothed_mask = mask * (1.0 - smooth_eps) + (1.0 - mask) * smooth_eps\n",
    "\n",
    "    pt = torch.where(mask > 0.5, det_prob, 1 - det_prob)\n",
    "    focal_weight = (1 - pt) ** 2\n",
    "    bce_loss = F.binary_cross_entropy(det_prob, smoothed_mask, reduction='none')\n",
    "    focal_loss = focal_weight * bce_loss\n",
    "    return focal_loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_one_epoch(generator, detector, train_loader, optimizer, epoch, total_epochs, device):\n",
    "#     generator.train()\n",
    "#     detector.train()\n",
    "#     total_loss = 0.0\n",
    "#     total_steps = len(train_loader)\n",
    "    \n",
    "#     ms_mel_loss = SimpleMelLoss().to(device)\n",
    "#     adv_loss_stub = AdversarialLoss().to(device)\n",
    "#     tf_loud_loss = TFLoudnessLoss().to(device)\n",
    "    \n",
    "#     pbar = tqdm(enumerate(train_loader), total=total_steps, desc=f\"Epoch [{epoch}/{total_epochs}]\")\n",
    "#     for i, batch_data in pbar:\n",
    "#         s = batch_data.to(device)  # shape: (B, 1, T)\n",
    "#         B = s.shape[0]\n",
    "        \n",
    "#         # 1) Generate watermarked audio => \"positive\"\n",
    "#         delta = generator(s)  \n",
    "#         s_w   = s + delta  \n",
    "        \n",
    "#         # Optional data augmentations on watermarked audio\n",
    "#         for b_idx in range(B):\n",
    "#             s_w[b_idx] = watermark_masking_augmentation(s_w[b_idx])\n",
    "        \n",
    "#         # 2) \"Negative\" examples = the clean audio 's' (no watermark)\n",
    "#         #    We can optionally do some augmentations on s if desired, \n",
    "#         #    but typically you leave it as normal audio.\n",
    "        \n",
    "#         # 3) Combine positives & negatives for the DETECTOR\n",
    "#         detector_input = torch.cat([s_w, s], dim=0)  # shape (2B,1,T)\n",
    "#         # Build ground-truth label mask => 1 for watermarked, 0 for clean\n",
    "#         label_mask = torch.cat([\n",
    "#             torch.ones_like(s),\n",
    "#             torch.zeros_like(s)\n",
    "#         ], dim=0).to(device)  # shape (2B,1,T)\n",
    "        \n",
    "#         det_out = detector(detector_input)  # shape (2B,1,T)\n",
    "        \n",
    "#         # -------------------- Compute Losses -------------------- #\n",
    "#         # (A) L1\n",
    "#         loss_l1     = F.l1_loss(delta, torch.zeros_like(delta))\n",
    "#         # (B) Mel\n",
    "#         loss_msspec = ms_mel_loss(s, s_w)\n",
    "#         # (C) Adversarial\n",
    "#         loss_adv    = adv_loss_stub(s, s_w, train_disc=True)\n",
    "#         # (D) Loudness\n",
    "#         loss_loud   = tf_loud_loss(s, s_w)\n",
    "#         # (E) Detector BCE\n",
    "#         loss_loc    = masked_localization_loss(det_out, label_mask)\n",
    "\n",
    "#         # Composite\n",
    "#         loss = (lambda_L1     * loss_l1 +\n",
    "#                 lambda_msspec * loss_msspec +\n",
    "#                 lambda_adv    * loss_adv +\n",
    "#                 lambda_loud   * loss_loud +\n",
    "#                 lambda_loc    * loss_loc)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         total_loss += loss.item()\n",
    "#         pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "#     avg_loss = total_loss / total_steps\n",
    "#     return avg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(255, 0, 128)\">Logging individual losses</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(generator, detector, train_loader, optimizer, epoch, total_epochs, device):\n",
    "    generator.train()\n",
    "    detector.train()\n",
    "    total_loss = 0.0\n",
    "    total_steps = len(train_loader)\n",
    "    \n",
    "    ms_mel_loss = SimpleMelLoss().to(device)\n",
    "    adv_loss_stub = AdversarialLoss().to(device)\n",
    "    tf_loud_loss = TFLoudnessLoss().to(device)\n",
    "    \n",
    "    pbar = tqdm(enumerate(train_loader), total=total_steps, desc=f\"Epoch [{epoch}/{total_epochs}]\")\n",
    "    for i, batch_data in pbar:\n",
    "        s = batch_data.to(device)  # shape: (B, 1, T)\n",
    "        B = s.shape[0]\n",
    "        \n",
    "        # 1) Generate watermarked audio (\"positive\")\n",
    "        delta = generator(s)  \n",
    "        s_w   = s + delta  \n",
    "        \n",
    "        # Optional data augmentation on watermarked audio\n",
    "        for b_idx in range(B):\n",
    "            s_w[b_idx] = watermark_masking_augmentation(s_w[b_idx])\n",
    "        \n",
    "        # 2) \"Negative\" examples: clean audio 's'\n",
    "        # 3) Combine positive and negative examples for the detector\n",
    "        detector_input = torch.cat([s_w, s], dim=0)  # shape (2B, 1, T)\n",
    "        label_mask = torch.cat([\n",
    "            torch.ones_like(s),\n",
    "            torch.zeros_like(s)\n",
    "        ], dim=0).to(device)  # shape (2B, 1, T)\n",
    "        \n",
    "        det_out = detector(detector_input)  # shape (2B, 1, T)\n",
    "        \n",
    "        # -------------------- Compute Individual Losses -------------------- #\n",
    "        loss_l1     = F.l1_loss(delta, torch.zeros_like(delta))\n",
    "        loss_msspec = ms_mel_loss(s, s_w)\n",
    "        loss_adv    = adv_loss_stub(s, s_w, train_disc=True)\n",
    "        loss_loud   = tf_loud_loss(s, s_w)\n",
    "        loss_loc    = masked_localization_loss(det_out, label_mask)\n",
    "        \n",
    "        # Composite loss\n",
    "        loss = (lambda_L1     * loss_l1 +\n",
    "                lambda_msspec * loss_msspec +\n",
    "                lambda_adv    * loss_adv +\n",
    "                lambda_loud   * loss_loud +\n",
    "                lambda_loc    * loss_loc)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({\n",
    "            \"Total\": f\"{loss.item():.4f}\",\n",
    "            \"L1\": f\"{loss_l1.item():.4f}\",\n",
    "            \"Mel\": f\"{loss_msspec.item():.4f}\",\n",
    "            \"Adv\": f\"{loss_adv.item():.4f}\",\n",
    "            \"Loud\": f\"{loss_loud.item():.4f}\",\n",
    "            \"Loc\": f\"{loss_loc.item():.4f}\"\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / total_steps\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def validate_one_epoch(generator, detector, val_loader, device):\n",
    "#     generator.eval()\n",
    "#     detector.eval()\n",
    "#     total_loss = 0.0\n",
    "#     steps = 0\n",
    "    \n",
    "#     ms_mel_loss = SimpleMelLoss().to(device)\n",
    "#     adv_loss_stub = AdversarialLoss().to(device)\n",
    "#     tf_loud_loss = TFLoudnessLoss().to(device)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for batch_data in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "#             s = batch_data.to(device)  # (B,1,T)\n",
    "#             B = s.shape[0]\n",
    "\n",
    "#             # Watermarked => positive\n",
    "#             delta = generator(s)\n",
    "#             s_w   = s + delta\n",
    "\n",
    "#             # Combine with negative => clean\n",
    "#             detector_input = torch.cat([s_w, s], dim=0)  # shape (2B,1,T)\n",
    "#             label_mask = torch.cat([\n",
    "#                 torch.ones_like(s),\n",
    "#                 torch.zeros_like(s)\n",
    "#             ], dim=0).to(device)\n",
    "\n",
    "#             det_out = detector(detector_input)\n",
    "\n",
    "#             # (A) L1\n",
    "#             loss_l1   = F.l1_loss(delta, torch.zeros_like(delta))\n",
    "#             # (B) Mel\n",
    "#             loss_msspec = ms_mel_loss(s, s_w)\n",
    "#             # (C) Adv\n",
    "#             loss_adv  = adv_loss_stub(s, s_w, train_disc=False)\n",
    "#             # (D) Loud\n",
    "#             loss_loud = tf_loud_loss(s, s_w)\n",
    "#             # (E) Detector BCE\n",
    "#             loss_loc  = masked_localization_loss(det_out, label_mask)\n",
    "\n",
    "#             # Composite\n",
    "#             loss = (lambda_L1     * loss_l1 +\n",
    "#                     lambda_msspec * loss_msspec +\n",
    "#                     lambda_adv    * loss_adv +\n",
    "#                     lambda_loud   * loss_loud +\n",
    "#                     lambda_loc    * loss_loc)\n",
    "            \n",
    "#             total_loss += loss.item()\n",
    "#             steps += 1\n",
    "    \n",
    "#     avg_loss = total_loss / steps if steps > 0 else 0.0\n",
    "#     return avg_loss\n",
    "\n",
    "\n",
    "# def train_model(generator, detector, train_dataset, val_dataset, num_epochs=10, lr=LR):\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "#     val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "#     optimizer = optim.Adam(list(generator.parameters()) + list(detector.parameters()), lr=lr)\n",
    "    \n",
    "#     for epoch in range(1, num_epochs+1):\n",
    "#         train_loss = train_one_epoch(generator, detector, train_loader, optimizer, epoch, num_epochs, device)\n",
    "#         val_loss   = validate_one_epoch(generator, detector, val_loader, device)\n",
    "#         print(f\"Epoch [{epoch}/{num_epochs}]  TRAIN Loss: {train_loss:.4f}  |  VAL Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(255, 0, 128)\">Logging individual losses</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch(generator, detector, val_loader, device):\n",
    "    generator.eval()\n",
    "    detector.eval()\n",
    "    total_loss = 0.0\n",
    "    steps = 0\n",
    "\n",
    "    ms_mel_loss = SimpleMelLoss().to(device)\n",
    "    adv_loss_stub = AdversarialLoss().to(device)\n",
    "    tf_loud_loss = TFLoudnessLoss().to(device)\n",
    "    \n",
    "    # For averaging individual losses\n",
    "    total_loss_l1 = 0.0\n",
    "    total_loss_msspec = 0.0\n",
    "    total_loss_adv = 0.0\n",
    "    total_loss_loud = 0.0\n",
    "    total_loss_loc = 0.0\n",
    "\n",
    "    pbar = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch_data in pbar:\n",
    "            s = batch_data.to(device)  # (B, 1, T)\n",
    "            B = s.shape[0]\n",
    "\n",
    "            # Generate watermarked audio (\"positive\")\n",
    "            delta = generator(s)\n",
    "            s_w   = s + delta\n",
    "\n",
    "            # Combine watermarked (positive) and clean (negative) examples\n",
    "            detector_input = torch.cat([s_w, s], dim=0)  # shape (2B, 1, T)\n",
    "            label_mask = torch.cat([\n",
    "                torch.ones_like(s),\n",
    "                torch.zeros_like(s)\n",
    "            ], dim=0).to(device)\n",
    "\n",
    "            det_out = detector(detector_input)\n",
    "\n",
    "            # Compute individual losses\n",
    "            loss_l1     = F.l1_loss(delta, torch.zeros_like(delta))\n",
    "            loss_msspec = ms_mel_loss(s, s_w)\n",
    "            loss_adv    = adv_loss_stub(s, s_w, train_disc=False)\n",
    "            loss_loud   = tf_loud_loss(s, s_w)\n",
    "            loss_loc    = masked_localization_loss(det_out, label_mask)\n",
    "\n",
    "            # Composite loss\n",
    "            loss = (lambda_L1     * loss_l1 +\n",
    "                    lambda_msspec * loss_msspec +\n",
    "                    lambda_adv    * loss_adv +\n",
    "                    lambda_loud   * loss_loud +\n",
    "                    lambda_loc    * loss_loc)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_loss_l1 += loss_l1.item()\n",
    "            total_loss_msspec += loss_msspec.item()\n",
    "            total_loss_adv += loss_adv.item()\n",
    "            total_loss_loud += loss_loud.item()\n",
    "            total_loss_loc += loss_loc.item()\n",
    "            steps += 1\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"Total\": f\"{loss.item():.4f}\",\n",
    "                \"L1\": f\"{loss_l1.item():.4f}\",\n",
    "                \"Mel\": f\"{loss_msspec.item():.4f}\",\n",
    "                \"Adv\": f\"{loss_adv.item():.4f}\",\n",
    "                \"Loud\": f\"{loss_loud.item():.4f}\",\n",
    "                \"Loc\": f\"{loss_loc.item():.4f}\"\n",
    "            })\n",
    "\n",
    "    avg_loss = total_loss / steps if steps > 0 else 0.0\n",
    "    print(f\"Validation - Total: {avg_loss:.4f}, L1: {total_loss_l1/steps:.4f}, Mel: {total_loss_msspec/steps:.4f}, \"\n",
    "          f\"Adv: {total_loss_adv/steps:.4f}, Loud: {total_loss_loud/steps:.4f}, Loc: {total_loss_loc/steps:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def train_model(generator, detector, train_dataset, val_dataset, num_epochs=10, lr=LR):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    optimizer = optim.Adam(list(generator.parameters()) + list(detector.parameters()), lr=lr)\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        train_loss = train_one_epoch(generator, detector, train_loader, optimizer, epoch, num_epochs, device)\n",
    "        val_loss   = validate_one_epoch(generator, detector, val_loader, device)\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}]  TRAIN Loss: {train_loss:.4f}  |  VAL Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_classification_metrics(y_true, y_score, threshold=0.5):\n",
    "    \"\"\"\n",
    "    y_true: 1D numpy array of 0 or 1 ground-truth labels\n",
    "    y_score: 1D numpy array of predicted probabilities in [0,1]\n",
    "    threshold: decision threshold for classification\n",
    "    Returns: dict with TPR, FPR, ACC\n",
    "    \"\"\"\n",
    "    y_pred = (y_score >= threshold).astype(int)\n",
    "\n",
    "    tp = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    fp = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    tn = np.sum((y_pred == 0) & (y_true == 0))\n",
    "    fn = np.sum((y_pred == 0) & (y_true == 1))\n",
    "\n",
    "    tpr = tp / (tp + fn + 1e-8)  # recall / sensitivity\n",
    "    fpr = fp / (fp + tn + 1e-8)\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn + 1e-8)\n",
    "\n",
    "    return {\n",
    "        'TPR': tpr,\n",
    "        'FPR': fpr,\n",
    "        'Accuracy': acc\n",
    "    }\n",
    "\n",
    "def compute_auc(y_true, y_score):\n",
    "    \"\"\"\n",
    "    Simple AUC calculation by sweeping thresholds from 0 to 1.\n",
    "    If you have scikit-learn, you could use: \n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        return roc_auc_score(y_true, y_score)\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(0, 1, 50)\n",
    "    tprs, fprs = [], []\n",
    "    for thr in thresholds:\n",
    "        metrics = compute_classification_metrics(y_true, y_score, threshold=thr)\n",
    "        tprs.append(metrics['TPR'])\n",
    "        fprs.append(metrics['FPR'])\n",
    "    fprs = np.array(fprs)\n",
    "    tprs = np.array(tprs)\n",
    "    order = np.argsort(fprs)\n",
    "    fprs = fprs[order]\n",
    "    tprs = tprs[order]\n",
    "    auc_value = np.trapz(tprs, fprs)  # trapezoid rule\n",
    "    return auc_value\n",
    "\n",
    "def evaluate_detector(generator, detector, dataset, device, batch_size=16):\n",
    "    \"\"\"\n",
    "    Creates watermarked (+) and clean (–) examples for each sample in dataset,\n",
    "    passes them to the detector, collects predictions, and computes TPR, FPR, Accuracy, AUC.\n",
    "    \"\"\"\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    generator.eval()\n",
    "    detector.eval()\n",
    "\n",
    "    y_true_all = []\n",
    "    y_score_all = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for s in loader:\n",
    "            s = s.to(device)  # shape (B,1,T)\n",
    "            B = s.shape[0]\n",
    "\n",
    "            # Watermarked\n",
    "            delta = generator(s)\n",
    "            s_w = s + delta\n",
    "\n",
    "            # Detector input => (2B,1,T)\n",
    "            # first B => watermarked => label=1, second B => clean => label=0\n",
    "            combined = torch.cat([s_w, s], dim=0)\n",
    "            out = detector(combined)  # shape (2B,1,T)\n",
    "\n",
    "            # Average detection probability over time => single score per sample\n",
    "            scores = out.mean(dim=2).squeeze(1).cpu().numpy()  # shape (2B,)\n",
    "\n",
    "            # Ground truth labels\n",
    "            gt = np.concatenate([\n",
    "                np.ones(B),   # watermarked => 1\n",
    "                np.zeros(B)   # clean => 0\n",
    "            ], axis=0)\n",
    "\n",
    "            y_true_all.append(gt)\n",
    "            y_score_all.append(scores)\n",
    "\n",
    "    y_true_all = np.concatenate(y_true_all, axis=0)\n",
    "    y_score_all = np.concatenate(y_score_all, axis=0)\n",
    "\n",
    "    # TPR, FPR, Accuracy at threshold=0.5\n",
    "    metrics_05 = compute_classification_metrics(y_true_all, y_score_all, threshold=0.5)\n",
    "    # AUC\n",
    "    auc_value  = compute_auc(y_true_all, y_score_all)\n",
    "\n",
    "    print(\"Detection Metrics @ threshold=0.5:\")\n",
    "    print(f\"  TPR:      {metrics_05['TPR']:.3f}\")\n",
    "    print(f\"  FPR:      {metrics_05['FPR']:.3f}\")\n",
    "    print(f\"  Accuracy: {metrics_05['Accuracy']:.3f}\")\n",
    "    print(f\"AUC: {auc_value:.3f}\")\n",
    "\n",
    "    return {\n",
    "        \"TPR\": metrics_05['TPR'],\n",
    "        \"FPR\": metrics_05['FPR'],\n",
    "        \"Accuracy\": metrics_05['Accuracy'],\n",
    "        \"AUC\": auc_value\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_si_snr_torch(original, reconstructed, eps=1e-8):\n",
    "    if original.dim() == 3:\n",
    "        original = original.squeeze(1)\n",
    "    if reconstructed.dim() == 3:\n",
    "        reconstructed = reconstructed.squeeze(1)\n",
    "    \n",
    "    original_zm = original - original.mean(dim=1, keepdim=True)\n",
    "    recon_zm    = reconstructed - reconstructed.mean(dim=1, keepdim=True)\n",
    "    \n",
    "    dot = (original_zm * recon_zm).sum(dim=1, keepdim=True)\n",
    "    norm_sq = (original_zm ** 2).sum(dim=1, keepdim=True) + eps\n",
    "    alpha = dot / norm_sq\n",
    "    \n",
    "    s_target = alpha * original_zm\n",
    "    e_noise = recon_zm - s_target\n",
    "    si_snr_val = 10 * torch.log10((s_target ** 2).sum(dim=1) / ((e_noise ** 2).sum(dim=1) + eps))\n",
    "    return si_snr_val\n",
    "\n",
    "# def evaluate_pesq(original, reconstructed, sr=SAMPLE_RATE):\n",
    "#     \"\"\"\n",
    "#     Placeholder: returns a dummy PESQ-like score in [1.0..4.5].\n",
    "#     If you have the real PESQ library, replace with actual call.\n",
    "#     \"\"\"\n",
    "#     return 4.5 - random.random()\n",
    "\n",
    "def run_evaluation(generator, detector, dataset, device, \n",
    "                   batch_size=16, compute_pesq_score=True, compute_si_snr_score=True):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    generator.eval()\n",
    "    detector.eval()\n",
    "    \n",
    "    si_snr_vals = []\n",
    "    pesq_vals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for s in loader:\n",
    "            s = s.to(device)\n",
    "            B = s.shape[0]\n",
    "            \n",
    "            # Generate watermarked audio\n",
    "            delta = generator(s)\n",
    "            s_w = s + delta\n",
    "            \n",
    "            # Audio quality\n",
    "            if compute_si_snr_score:\n",
    "                si_snr_vals.extend(evaluate_si_snr_torch(s, s_w).cpu().numpy())\n",
    "            # if compute_pesq_score:\n",
    "            #     for i in range(B):\n",
    "            #         pesq_vals.append(evaluate_pesq(s[i], s_w[i]))\n",
    "\n",
    "    avg_si_snr = float(np.mean(si_snr_vals)) if si_snr_vals else 0.0\n",
    "    # avg_pesq = float(np.mean(pesq_vals)) if pesq_vals else 0.0\n",
    "    \n",
    "    print(\"\\n--- Audio Quality Results ---\")\n",
    "    if compute_si_snr_score:\n",
    "        print(f\"Average SI-SNR: {avg_si_snr:.3f} dB\")\n",
    "    # if compute_pesq_score:\n",
    "    #     print(f\"Average PESQ:  {avg_pesq:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        \"si_snr\": avg_si_snr,\n",
    "        # \"pesq\": avg_pesq\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(generator, dataset, device, num_epochs=10, lr=1e-3,\n",
    "                    lambda_L1=1.0, lambda_msspec=1.0, lambda_loud=0.5):\n",
    "    \"\"\"\n",
    "    Trains ONLY the Generator, ignoring adversarial or detection losses.\n",
    "    Minimizes a combination of L1, MelSpectrogram, and Loudness losses\n",
    "    so that watermarked audio remains close to original.\n",
    "\n",
    "    Args:\n",
    "        generator: your Generator model\n",
    "        dataset: a torch Dataset or Subset (1-sec audio clips)\n",
    "        device: 'cuda' or 'cpu'\n",
    "        num_epochs: how many epochs to run\n",
    "        lr: learning rate\n",
    "        lambda_L1, lambda_msspec, lambda_loud: weighting factors\n",
    "    \"\"\"\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "    gen_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "\n",
    "    ms_mel_loss = SimpleMelLoss().to(device)\n",
    "    tf_loud_loss = TFLoudnessLoss().to(device)\n",
    "\n",
    "    generator.train()\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        total_loss = 0.0\n",
    "        for batch_idx, s in enumerate(loader):\n",
    "            s = s.to(device)  # (B,1,T)\n",
    "            delta = generator(s)      # (B,1,T)\n",
    "            s_w = s + delta           # watermarked\n",
    "\n",
    "            # Possibly do robustness/augmentations if you want the generator to\n",
    "            # learn to produce robust watermarks from the start:\n",
    "            # for i in range(s_w.shape[0]):\n",
    "            #     s_w[i] = watermark_masking_augmentation(s_w[i])\n",
    "            #     s_w[i] = robustness_augmentations(s_w[i])\n",
    "\n",
    "            # --- 1) L1 on delta\n",
    "            loss_l1 = F.l1_loss(delta, torch.zeros_like(delta))\n",
    "\n",
    "            # --- 2) Mel Spectrogram\n",
    "            loss_msspec = ms_mel_loss(s, s_w)\n",
    "\n",
    "            # --- 3) Loudness\n",
    "            loss_loud = tf_loud_loss(s, s_w)\n",
    "\n",
    "            loss = (lambda_L1 * loss_l1 +\n",
    "                    lambda_msspec * loss_msspec +\n",
    "                    lambda_loud * loss_loud)\n",
    "\n",
    "            gen_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            gen_optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        print(f\"[Gen-Only Epoch {epoch}/{num_epochs}]  Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    print(\"Generator pre-training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data_root = \"data/100_all\"\n",
    "    full_dataset = OneSecClipsDataset(root_dir=data_root, sample_rate=SAMPLE_RATE)\n",
    "\n",
    "    # Pick a subset for demonstration\n",
    "    subset_size = 1000\n",
    "    subset_indices = list(range(min(subset_size, len(full_dataset))))\n",
    "    subset_dataset = torch.utils.data.Subset(full_dataset, subset_indices)\n",
    "\n",
    "    # Split: 80% train, 10% val, 10% test\n",
    "    n = len(subset_dataset)\n",
    "    n_train = int(0.8 * n)\n",
    "    n_val   = int(0.1 * n)\n",
    "    n_test  = n - n_train - n_val\n",
    "    train_ds, val_ds, test_ds = random_split(subset_dataset, [n_train, n_val, n_test])\n",
    "\n",
    "    # Instantiate models\n",
    "    generator = Generator().to(device)\n",
    "    detector  = Detector().to(device)\n",
    "\n",
    "    # Train\n",
    "    num_epochs = 10\n",
    "    train_model(generator, detector, train_ds, val_ds, num_epochs=num_epochs, lr=LR)\n",
    "\n",
    "    # Save models\n",
    "    torch.save(generator.state_dict(), \"generator.pth\")\n",
    "    torch.save(detector.state_dict(),  \"detector.pth\")\n",
    "\n",
    "    # Evaluate classification (watermarked vs. clean)\n",
    "    print(\"\\n--- Detector Classification Metrics ---\")\n",
    "    detection_metrics = evaluate_detector(generator, detector, test_ds, device, batch_size=16)\n",
    "    print(detection_metrics)\n",
    "\n",
    "    # Evaluate audio quality (SI-SNR, PESQ, etc.)\n",
    "    print(\"\\n--- Audio Quality Metrics ---\")\n",
    "    run_evaluation(generator, detector, test_ds, device,\n",
    "                   batch_size=16, compute_pesq_score=True, compute_si_snr_score=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
