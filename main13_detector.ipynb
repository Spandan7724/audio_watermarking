{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fix random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ---------------------- Hyperparameters ---------------------- #\n",
    "SAMPLE_RATE = 16000   # Audio sample rate\n",
    "AUDIO_LEN   = 16000   # 1-second audio (16k samples)\n",
    "BATCH_SIZE  = 256      # Batch size for training\n",
    "LR          = 1e-3    # Learning rate\n",
    "\n",
    "HIDDEN_DIM  = 32      # Hidden dimension for LSTM in Generator\n",
    "CHANNELS    = 32      # Initial convolution channels\n",
    "OUTPUT_CH   = 128     # Final conv channels for Generator\n",
    "STRIDES     = [2, 4, 5, 8]  # Downsampling strides\n",
    "LSTM_LAYERS = 2       # Number of LSTM layers\n",
    "NUM_WORKERS = 16       # DataLoader workers\n",
    "\n",
    "# Loss Weights for this Phase 1 (Generator-only)\n",
    "lambda_L1     = 1.0\n",
    "lambda_msspec = 1.0\n",
    "lambda_loud   = 0.5\n",
    "lambda_loc    = 1.0\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneSecClipsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Assumes each .wav file in root_dir is a ~1-sec clip (16k samples).\n",
    "    If sample_rate != 16000, it resamples to 16k.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, sample_rate=SAMPLE_RATE):\n",
    "        super().__init__()\n",
    "        self.filepaths = glob.glob(os.path.join(root_dir, '**', '*.wav'), recursive=True)\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav_path = self.filepaths[idx]\n",
    "        waveform, sr = torchaudio.load(wav_path)\n",
    "\n",
    "        # Convert to mono if multi-channel\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # Resample if needed\n",
    "        if sr != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        # Ensure the clip is 1 second (pad/crop if necessary)\n",
    "        if waveform.shape[1] > AUDIO_LEN:\n",
    "            waveform = waveform[:, :AUDIO_LEN]\n",
    "        elif waveform.shape[1] < AUDIO_LEN:\n",
    "            pad_amt = AUDIO_LEN - waveform.shape[1]\n",
    "            waveform = F.pad(waveform, (0, pad_amt))\n",
    "\n",
    "        return waveform  # shape: (1, AUDIO_LEN)\n",
    "\n",
    "def watermark_masking_augmentation(wav, p_replace_orig=0.4, p_replace_zero=0.2, p_replace_diff=0.2):\n",
    "    \"\"\"\n",
    "    Randomly masks portions of the audio:\n",
    "    - p_replace_orig: do nothing\n",
    "    - p_replace_zero: replace segment with zeros\n",
    "    - p_replace_diff: replace segment with random noise\n",
    "    \"\"\"\n",
    "    T = wav.shape[1]\n",
    "    window_len = int(0.1 * SAMPLE_RATE)\n",
    "    k = 5\n",
    "    for _ in range(k):\n",
    "        start = random.randint(0, T - window_len)\n",
    "        end = start + window_len\n",
    "        choice = random.random()\n",
    "        if choice < p_replace_orig:\n",
    "            pass\n",
    "        elif choice < p_replace_orig + p_replace_zero:\n",
    "            wav[:, start:end] = 0.0\n",
    "        elif choice < p_replace_orig + p_replace_zero + p_replace_diff:\n",
    "            wav[:, start:end] = 0.1 * torch.randn_like(wav[:, start:end])\n",
    "        else:\n",
    "            pass\n",
    "    return wav\n",
    "\n",
    "def robustness_augmentations(wav):\n",
    "    \"\"\"\n",
    "    Adds small random noise for robustness.\n",
    "    \"\"\"\n",
    "    return wav + 0.005 * torch.randn_like(wav)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_conv1d(in_ch, out_ch, kernel_size=3, stride=1, padding=1):\n",
    "    return nn.Conv1d(in_ch, out_ch, kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.downsample = (stride != 1 or in_ch != out_ch)\n",
    "        self.conv1 = make_conv1d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1)\n",
    "        self.conv2 = make_conv1d(out_ch, out_ch, kernel_size=3, stride=1, padding=1)\n",
    "        self.elu   = nn.ELU()\n",
    "        if self.downsample:\n",
    "            self.skip_conv = make_conv1d(in_ch, out_ch, kernel_size=1, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.elu(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.skip_conv(residual)\n",
    "        out = self.elu(out + residual)\n",
    "        return out\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels=1, \n",
    "                 base_channels=CHANNELS,\n",
    "                 hidden_dim=HIDDEN_DIM, \n",
    "                 output_channels=OUTPUT_CH, \n",
    "                 strides=STRIDES):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ---------- Encoder ----------\n",
    "        self.init_conv = nn.Conv1d(in_channels, base_channels, kernel_size=7, stride=1, padding=3)\n",
    "        \n",
    "        enc_blocks = []\n",
    "        ch = base_channels\n",
    "        for st in strides:\n",
    "            out_ch = ch * 2\n",
    "            enc_blocks.append(ResidualBlock(ch, out_ch, stride=st))\n",
    "            ch = out_ch\n",
    "        self.encoder_blocks = nn.Sequential(*enc_blocks)\n",
    "\n",
    "        # Project encoder output to hidden_dim (for LSTM)\n",
    "        self.proj = nn.Linear(ch, hidden_dim)\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim, \n",
    "                            num_layers=LSTM_LAYERS, batch_first=True, bidirectional=False)\n",
    "\n",
    "        self.final_conv_enc = nn.Conv1d(hidden_dim, output_channels, kernel_size=7, stride=1, padding=3)\n",
    "\n",
    "        # ---------- Decoder ----------\n",
    "        dec_blocks = []\n",
    "        rev_strides = list(reversed(strides))\n",
    "        in_ch = output_channels\n",
    "        for st in rev_strides:\n",
    "            out_ch = in_ch // 2\n",
    "            dec_blocks.append(nn.ConvTranspose1d(in_ch, out_ch, kernel_size=2*st, stride=st,\n",
    "                                                 padding=(st//2), output_padding=0))\n",
    "            dec_blocks.append(ResidualBlock(out_ch, out_ch, stride=1))\n",
    "            in_ch = out_ch\n",
    "        self.decoder_blocks = nn.Sequential(*dec_blocks)\n",
    "\n",
    "        # Final conv -> 1 channel for the delta\n",
    "        self.final_conv_dec = nn.Conv1d(in_ch, 1, kernel_size=7, stride=1, padding=3)\n",
    "\n",
    "    def forward(self, s):\n",
    "        \"\"\"\n",
    "        s: shape (B, 1, T)\n",
    "        Output: delta (B, 1, T)\n",
    "        \"\"\"\n",
    "        B, _, T = s.shape\n",
    "\n",
    "        # Encode\n",
    "        x = self.init_conv(s)\n",
    "        x = self.encoder_blocks(x)  \n",
    "        x_t = x.transpose(1, 2)     # (B, T_enc, ch)\n",
    "        x_t = self.proj(x_t)        # (B, T_enc, hidden_dim)\n",
    "\n",
    "        # LSTM\n",
    "        lstm_out, _ = self.lstm(x_t)           # (B, T_enc, hidden_dim)\n",
    "        lstm_out_t = lstm_out.transpose(1, 2)  # (B, hidden_dim, T_enc)\n",
    "        latent = self.final_conv_enc(lstm_out_t)\n",
    "\n",
    "        # Decode\n",
    "        x_dec = self.decoder_blocks(latent)\n",
    "        delta = self.final_conv_dec(x_dec)\n",
    "\n",
    "        # Adjust shape if needed\n",
    "        if delta.shape[-1] != T:\n",
    "            min_len = min(delta.shape[-1], T)\n",
    "            delta = delta[:, :, :min_len]\n",
    "            if min_len < T:\n",
    "                pad_amt = T - min_len\n",
    "                delta = F.pad(delta, (0, pad_amt))\n",
    "\n",
    "        return delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T\n",
    "\n",
    "class SimpleMelLoss(nn.Module):\n",
    "    def __init__(self, sample_rate=SAMPLE_RATE, n_fft=1024, n_mels=80):\n",
    "        super(SimpleMelLoss, self).__init__()\n",
    "        self.mel_spec = T.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=n_fft // 4,\n",
    "            n_mels=n_mels,\n",
    "            normalized=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, original, watermarked):\n",
    "        mel_orig = torch.log(self.mel_spec(original) + 1e-5)\n",
    "        mel_wm   = torch.log(self.mel_spec(watermarked) + 1e-5)\n",
    "        return F.l1_loss(mel_orig, mel_wm)\n",
    "\n",
    "class TFLoudnessLoss(nn.Module):\n",
    "    def __init__(self, n_bands=8, window_size=2048, hop_size=512):\n",
    "        super(TFLoudnessLoss, self).__init__()\n",
    "        self.n_bands = n_bands\n",
    "        self.win_size = window_size\n",
    "        self.hop_size = hop_size\n",
    "        \n",
    "        weights = torch.ones(n_bands)\n",
    "        mid_band_idx = n_bands // 3\n",
    "        weights[mid_band_idx:2 * mid_band_idx] = 1.5\n",
    "        self.register_buffer('band_weights', weights)\n",
    "\n",
    "    def forward(self, original, watermarked):\n",
    "        window = torch.hann_window(self.win_size, device=original.device)\n",
    "        stft_orig = torch.stft(\n",
    "            original.squeeze(1), n_fft=self.win_size, hop_length=self.hop_size,\n",
    "            window=window, return_complex=True, normalized=True\n",
    "        )\n",
    "        stft_wm = torch.stft(\n",
    "            watermarked.squeeze(1), n_fft=self.win_size, hop_length=self.hop_size,\n",
    "            window=window, return_complex=True, normalized=True\n",
    "        )\n",
    "        mag_orig = stft_orig.abs()\n",
    "        mag_wm   = stft_wm.abs()\n",
    "        phase_orig = stft_orig.angle()\n",
    "        phase_wm   = stft_wm.angle()\n",
    "        \n",
    "        freq_bins = mag_orig.shape[1]\n",
    "        band_size = freq_bins // self.n_bands\n",
    "        \n",
    "        loudness_loss = 0.0\n",
    "        spectral_loss = 0.0\n",
    "        phase_loss = 0.0\n",
    "        \n",
    "        for b in range(self.n_bands):\n",
    "            start = b * band_size\n",
    "            end = freq_bins if (b == self.n_bands - 1) else (start + band_size)\n",
    "            band_orig = mag_orig[:, start:end, :]\n",
    "            band_wm = mag_wm[:, start:end, :]\n",
    "            \n",
    "            energy_orig = torch.sum(band_orig ** 2, dim=1)\n",
    "            energy_wm = torch.sum(band_wm ** 2, dim=1)\n",
    "            loud_orig = torch.log10(energy_orig + 1e-8)\n",
    "            loud_wm   = torch.log10(energy_wm + 1e-8)\n",
    "            loudness_loss += self.band_weights[b] * F.l1_loss(loud_wm, loud_orig)\n",
    "            spectral_loss += self.band_weights[b] * F.mse_loss(band_wm, band_orig)\n",
    "            phase_diff = 1.0 - torch.cos(phase_wm[:, start:end, :] - phase_orig[:, start:end, :])\n",
    "            phase_loss += self.band_weights[b] * phase_diff.mean()\n",
    "        \n",
    "        loudness_loss /= self.n_bands\n",
    "        spectral_loss /= self.n_bands\n",
    "        phase_loss /= self.n_bands\n",
    "        \n",
    "        return loudness_loss + spectral_loss + 0.2 * phase_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(\n",
    "    generator,\n",
    "    train_ds,\n",
    "    val_ds,\n",
    "    device,\n",
    "    num_epochs=10,\n",
    "    lr=1e-3,\n",
    "    lambda_L1=1.0,\n",
    "    lambda_msspec=1.0,\n",
    "    lambda_loud=0.5,\n",
    "    use_robustness=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains only the Generator (no adversarial or detector),\n",
    "    with a separate validation set.\n",
    "    \"\"\"\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "    optimizer = optim.Adam(generator.parameters(), lr=lr)\n",
    "    generator.train()\n",
    "\n",
    "    ms_mel_loss_fn = SimpleMelLoss().to(device)\n",
    "    tf_loud_loss_fn = TFLoudnessLoss().to(device)\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        # ------------------- TRAINING LOOP ------------------- #\n",
    "        generator.train()\n",
    "        train_pbar = tqdm(train_loader, desc=f\"[Epoch {epoch}/{num_epochs} - Train]\", leave=True)\n",
    "        \n",
    "        running_l1, running_mel, running_loud, running_total = 0.0, 0.0, 0.0, 0.0\n",
    "        num_batches = len(train_loader)\n",
    "\n",
    "        for batch_data in train_pbar:\n",
    "            s = batch_data.to(device)  # (B,1,T)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass in full precision (no autocast)\n",
    "            delta = generator(s)\n",
    "            s_w = s + delta\n",
    "\n",
    "            if use_robustness:\n",
    "                for i in range(s_w.shape[0]):\n",
    "                    s_w[i] = watermark_masking_augmentation(s_w[i])\n",
    "                    s_w[i] = robustness_augmentations(s_w[i])\n",
    "\n",
    "            loss_l1 = F.l1_loss(delta, torch.zeros_like(delta))\n",
    "            loss_mel = ms_mel_loss_fn(s, s_w)\n",
    "            loss_loud = tf_loud_loss_fn(s, s_w)\n",
    "\n",
    "            loss_total = (lambda_L1 * loss_l1 +\n",
    "                          lambda_msspec * loss_mel +\n",
    "                          lambda_loud * loss_loud)\n",
    "\n",
    "            # Standard backward pass\n",
    "            loss_total.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            running_l1    += loss_l1.item()\n",
    "            running_mel   += loss_mel.item()\n",
    "            running_loud  += loss_loud.item()\n",
    "            running_total += loss_total.item()\n",
    "\n",
    "            train_pbar.set_postfix({\n",
    "                \"L1\": f\"{loss_l1.item():.4f}\",\n",
    "                \"Mel\": f\"{loss_mel.item():.4f}\",\n",
    "                \"Loud\": f\"{loss_loud.item():.4f}\",\n",
    "                \"Total\": f\"{loss_total.item():.4f}\"\n",
    "            })\n",
    "\n",
    "        avg_l1    = running_l1 / num_batches\n",
    "        avg_mel   = running_mel / num_batches\n",
    "        avg_loud  = running_loud / num_batches\n",
    "        avg_total = running_total / num_batches\n",
    "\n",
    "        # ------------------- VALIDATION LOOP ------------------- #\n",
    "        generator.eval()\n",
    "        val_pbar = tqdm(val_loader, desc=\"[Validation]\", leave=False)\n",
    "        \n",
    "        val_l1, val_mel, val_loud, val_total = 0.0, 0.0, 0.0, 0.0\n",
    "        val_steps = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_data in val_pbar:\n",
    "                s_val = batch_data.to(device)\n",
    "                delta_val = generator(s_val)\n",
    "                s_w_val   = s_val + delta_val\n",
    "\n",
    "                l1_val   = F.l1_loss(delta_val, torch.zeros_like(delta_val))\n",
    "                mel_val  = ms_mel_loss_fn(s_val, s_w_val)\n",
    "                loud_val = tf_loud_loss_fn(s_val, s_w_val)\n",
    "                total_val = (lambda_L1 * l1_val +\n",
    "                             lambda_msspec * mel_val +\n",
    "                             lambda_loud * loud_val)\n",
    "\n",
    "                val_l1    += l1_val.item()\n",
    "                val_mel   += mel_val.item()\n",
    "                val_loud  += loud_val.item()\n",
    "                val_total += total_val.item()\n",
    "                val_steps += 1\n",
    "\n",
    "        if val_steps > 0:\n",
    "            val_l1    /= val_steps\n",
    "            val_mel   /= val_steps\n",
    "            val_loud  /= val_steps\n",
    "            val_total /= val_steps\n",
    "\n",
    "        print(f\"\\nEpoch [{epoch}/{num_epochs}] Summary:\")\n",
    "        print(f\"  Train => L1: {avg_l1:.4f}, Mel: {avg_mel:.4f}, Loud: {avg_loud:.4f}, Total: {avg_total:.4f}\")\n",
    "        print(f\"  Valid => L1: {val_l1:.4f}, Mel: {val_mel:.4f}, Loud: {val_loud:.4f}, Total: {val_total:.4f}\\n\")\n",
    "\n",
    "    print(\"Generator-only training with validation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_si_snr_torch(original, reconstructed, eps=1e-8):\n",
    "    if original.dim() == 3:\n",
    "        original = original.squeeze(1)\n",
    "    if reconstructed.dim() == 3:\n",
    "        reconstructed = reconstructed.squeeze(1)\n",
    "    \n",
    "    original_zm = original - original.mean(dim=1, keepdim=True)\n",
    "    recon_zm    = reconstructed - reconstructed.mean(dim=1, keepdim=True)\n",
    "    \n",
    "    dot = (original_zm * recon_zm).sum(dim=1, keepdim=True)\n",
    "    norm_sq = (original_zm ** 2).sum(dim=1, keepdim=True) + eps\n",
    "    alpha = dot / norm_sq\n",
    "    \n",
    "    s_target = alpha * original_zm\n",
    "    e_noise = recon_zm - s_target\n",
    "    si_snr_val = 10 * torch.log10((s_target ** 2).sum(dim=1) / ((e_noise ** 2).sum(dim=1) + eps))\n",
    "    return si_snr_val\n",
    "\n",
    "\n",
    "def run_evaluation_generator_only(generator, dataset, device, batch_size=16):\n",
    "    \"\"\"\n",
    "    Evaluate how much the generator changes the audio \n",
    "    by measuring SI-SNR or any custom metric.\n",
    "    \"\"\"\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    generator.eval()\n",
    "\n",
    "    si_snr_vals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for s in loader:\n",
    "            s = s.to(device)  # (B,1,T)\n",
    "            delta = generator(s)\n",
    "            s_w = s + delta\n",
    "\n",
    "            # measure SI-SNR\n",
    "            vals = evaluate_si_snr_torch(s, s_w).cpu().numpy()\n",
    "            si_snr_vals.extend(vals)\n",
    "    \n",
    "    avg_si_snr = float(np.mean(si_snr_vals)) if si_snr_vals else 0.0\n",
    "    return avg_si_snr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################################################################################\n",
    "# 1) DETECTOR ARCHITECTURE\n",
    "################################################################################\n",
    "\n",
    "class Detector(nn.Module):\n",
    "    def __init__(self, in_channels=1, base_channels=32, strides=[2,4,5,8]):\n",
    "        super().__init__()\n",
    "        self.init_conv = nn.Conv1d(in_channels, base_channels, kernel_size=7, stride=1, padding=3)\n",
    "        enc_blocks = []\n",
    "        ch = base_channels\n",
    "        for st in strides:\n",
    "            out_ch = ch * 2\n",
    "            enc_blocks.append(ResidualBlock(ch, out_ch, stride=st))\n",
    "            ch = out_ch\n",
    "        self.encoder_blocks = nn.Sequential(*enc_blocks)\n",
    "\n",
    "        dec_blocks = []\n",
    "        rev_strides = list(reversed(strides))\n",
    "        in_ch = ch\n",
    "        for st in rev_strides:\n",
    "            out_ch = in_ch // 2\n",
    "            dec_blocks.append(nn.ConvTranspose1d(\n",
    "                in_ch, out_ch,\n",
    "                kernel_size=2*st, stride=st,\n",
    "                padding=(st//2), output_padding=0\n",
    "            ))\n",
    "            dec_blocks.append(ResidualBlock(out_ch, out_ch, stride=1))\n",
    "            in_ch = out_ch\n",
    "        self.decoder_blocks = nn.Sequential(*dec_blocks)\n",
    "\n",
    "        self.final_conv = nn.Conv1d(in_ch, 1, kernel_size=7, stride=1, padding=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_length = x.shape[-1]\n",
    "        x = self.init_conv(x)\n",
    "        x = self.encoder_blocks(x)\n",
    "        x = self.decoder_blocks(x)\n",
    "        out = self.final_conv(x)\n",
    "        if out.shape[-1] > original_length:\n",
    "            out = out[:, :, :original_length]\n",
    "        elif out.shape[-1] < original_length:\n",
    "            pad_amt = original_length - out.shape[-1]\n",
    "            out = F.pad(out, (0, pad_amt))\n",
    "        return torch.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMelLoss(nn.Module): \n",
    "    def __init__(self, sample_rate=SAMPLE_RATE, n_fft=1024, n_mels=80):\n",
    "        super().__init__()\n",
    "        self.mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=n_fft // 4,\n",
    "            n_mels=n_mels,\n",
    "            normalized=True\n",
    "        )\n",
    "\n",
    "    def forward(self, original, watermarked):\n",
    "        mel_orig = torch.log(self.mel_spec(original) + 1e-5)\n",
    "        mel_wm   = torch.log(self.mel_spec(watermarked) + 1e-5)\n",
    "        return F.l1_loss(mel_orig, mel_wm)\n",
    "\n",
    "class TFLoudnessLoss(nn.Module):\n",
    "    def __init__(self, n_bands=8, window_size=2048, hop_size=512):\n",
    "        super().__init__()\n",
    "        self.n_bands = n_bands\n",
    "        self.win_size = window_size\n",
    "        self.hop_size = hop_size\n",
    "        \n",
    "        weights = torch.ones(n_bands)\n",
    "        mid_band_idx = n_bands // 3\n",
    "        weights[mid_band_idx:2 * mid_band_idx] = 1.5\n",
    "        self.register_buffer('band_weights', weights)\n",
    "\n",
    "    def forward(self, original, watermarked):\n",
    "        window = torch.hann_window(self.win_size, device=original.device)\n",
    "        stft_orig = torch.stft(\n",
    "            original.squeeze(1), n_fft=self.win_size, hop_length=self.hop_size,\n",
    "            window=window, return_complex=True, normalized=True\n",
    "        )\n",
    "        stft_wm = torch.stft(\n",
    "            watermarked.squeeze(1), n_fft=self.win_size, hop_length=self.hop_size,\n",
    "            window=window, return_complex=True, normalized=True\n",
    "        )\n",
    "        mag_orig = stft_orig.abs()\n",
    "        mag_wm   = stft_wm.abs()\n",
    "        phase_orig = stft_orig.angle()\n",
    "        phase_wm   = stft_wm.angle()\n",
    "        \n",
    "        freq_bins = mag_orig.shape[1]\n",
    "        band_size = freq_bins // self.n_bands\n",
    "        \n",
    "        loudness_loss = 0.0\n",
    "        spectral_loss = 0.0\n",
    "        phase_loss = 0.0\n",
    "        \n",
    "        for b in range(self.n_bands):\n",
    "            start = b * band_size\n",
    "            end = freq_bins if (b == self.n_bands - 1) else (start + band_size)\n",
    "            band_orig = mag_orig[:, start:end, :]\n",
    "            band_wm   = mag_wm[:, start:end, :]\n",
    "\n",
    "            energy_orig = torch.sum(band_orig ** 2, dim=1)\n",
    "            energy_wm   = torch.sum(band_wm ** 2, dim=1)\n",
    "            loud_orig   = torch.log10(energy_orig + 1e-8)\n",
    "            loud_wm     = torch.log10(energy_wm + 1e-8)\n",
    "            loudness_loss += self.band_weights[b] * F.l1_loss(loud_wm, loud_orig)\n",
    "            spectral_loss += self.band_weights[b] * F.mse_loss(band_wm, band_orig)\n",
    "            phase_diff     = 1.0 - torch.cos(phase_wm[:, start:end, :] - phase_orig[:, start:end, :])\n",
    "            phase_loss    += self.band_weights[b] * phase_diff.mean()\n",
    "        \n",
    "        loudness_loss /= self.n_bands\n",
    "        spectral_loss /= self.n_bands\n",
    "        phase_loss    /= self.n_bands\n",
    "        \n",
    "        return loudness_loss + spectral_loss + 0.2 * phase_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# 2) DETECTION LOSS: Masked Localization (focal BCE)\n",
    "################################################################################\n",
    "\n",
    "def masked_localization_loss(detector_out, mask, smooth_eps=0.1):\n",
    "    \"\"\"\n",
    "    A binary cross-entropy style detection loss with focal weighting + label smoothing.\n",
    "    detector_out: (B,1,T) in [0,1]\n",
    "    mask: (B,1,T) => 1 for watermarked, 0 for clean\n",
    "    smooth_eps: label smoothing factor\n",
    "    \"\"\"\n",
    "    det_prob = detector_out  # shape (B,1,T)\n",
    "    smoothed_mask = mask * (1 - smooth_eps) + (1 - mask) * smooth_eps\n",
    "\n",
    "    # Focal weighting\n",
    "    pt = torch.where(mask > 0.5, det_prob, 1 - det_prob)  # probability of the \"correct\" label\n",
    "    focal_weight = (1 - pt) ** 2\n",
    "\n",
    "    bce_loss = F.binary_cross_entropy(det_prob, smoothed_mask, reduction='none')\n",
    "    focal_loss = focal_weight * bce_loss\n",
    "\n",
    "    return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim.lr_scheduler as lr_sched\n",
    "\n",
    "def train_generator_detector(\n",
    "    generator,\n",
    "    detector,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    device,\n",
    "    num_epochs=10,\n",
    "    batch_size=64,\n",
    "    lr=1e-3,\n",
    "    lambda_L1=1.0,\n",
    "    lambda_msspec=1.0,\n",
    "    lambda_loud=0.5,\n",
    "    lambda_loc=1.0,\n",
    "    use_robustness=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the (unfrozen) Generator + new Detector with a Cosine LR Scheduler \n",
    "    and validation each epoch.\n",
    "    \"\"\"\n",
    "    # 1) Optionally compile for speed\n",
    "    generator = torch.compile(generator)\n",
    "    detector  = torch.compile(detector)\n",
    "\n",
    "    # 2) Create DataLoaders with pin_memory\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # 3) Prepare losses + optimizer + scheduler\n",
    "    ms_mel_loss = SimpleMelLoss().to(device)\n",
    "    tf_loud_loss = TFLoudnessLoss().to(device)\n",
    "    optimizer = optim.Adam(list(generator.parameters()) + list(detector.parameters()), lr=lr)\n",
    "\n",
    "    # Here we set T_max = num_epochs, so it decays over the entire training.\n",
    "    # After 'num_epochs' calls to scheduler.step(), LR is near 'eta_min'.\n",
    "    scheduler = lr_sched.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=num_epochs,  # or total_iters if you want a per-iteration approach\n",
    "        eta_min=1e-5       # final LR\n",
    "    )\n",
    "\n",
    "    generator.train()\n",
    "    detector.train()\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        #######################\n",
    "        #      TRAIN LOOP\n",
    "        #######################\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\", leave=True)\n",
    "        train_total_loss = 0.0\n",
    "        train_steps = 0\n",
    "\n",
    "        for batch_data in train_pbar:\n",
    "            s = batch_data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            delta = generator(s)\n",
    "            s_w   = s + delta\n",
    "\n",
    "            if use_robustness:\n",
    "                for b_idx in range(s_w.shape[0]):\n",
    "                    s_w[b_idx] = watermark_masking_augmentation(s_w[b_idx])\n",
    "                    s_w[b_idx] = robustness_augmentations(s_w[b_idx])\n",
    "\n",
    "            # Create label mask: 1 for watermarked, 0 for clean\n",
    "            combined = torch.cat([s_w, s], dim=0)\n",
    "            label_mask = torch.cat([\n",
    "                torch.ones_like(s),\n",
    "                torch.zeros_like(s)\n",
    "            ], dim=0).to(device)\n",
    "\n",
    "            det_out = detector(combined)\n",
    "\n",
    "            # Losses\n",
    "            loss_l1     = F.l1_loss(delta, torch.zeros_like(delta))\n",
    "            loss_msspec = ms_mel_loss(s, s_w)\n",
    "            loss_loud   = tf_loud_loss(s, s_w)\n",
    "            loss_loc    = masked_localization_loss(det_out, label_mask, smooth_eps=0.1)\n",
    "\n",
    "            loss = (lambda_L1     * loss_l1 +\n",
    "                    lambda_msspec * loss_msspec +\n",
    "                    lambda_loud   * loss_loud +\n",
    "                    lambda_loc    * loss_loc)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_total_loss += loss.item()\n",
    "            train_steps += 1\n",
    "            train_pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        # End of train loop\n",
    "        train_avg_loss = train_total_loss / train_steps\n",
    "\n",
    "        #######################\n",
    "        #     VALIDATION LOOP\n",
    "        #######################\n",
    "        generator.eval()\n",
    "        detector.eval()\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch}/{num_epochs} [Val]\", leave=False)\n",
    "        val_total_loss = 0.0\n",
    "        val_steps = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_data in val_pbar:\n",
    "                s = batch_data.to(device)\n",
    "                delta = generator(s)\n",
    "                s_w   = s + delta\n",
    "\n",
    "                combined = torch.cat([s_w, s], dim=0)\n",
    "                label_mask = torch.cat([\n",
    "                    torch.ones_like(s),\n",
    "                    torch.zeros_like(s)\n",
    "                ], dim=0).to(device)\n",
    "\n",
    "                det_out = detector(combined)\n",
    "\n",
    "                loss_l1     = F.l1_loss(delta, torch.zeros_like(delta))\n",
    "                loss_msspec = ms_mel_loss(s, s_w)\n",
    "                loss_loud   = tf_loud_loss(s, s_w)\n",
    "                loss_loc    = masked_localization_loss(det_out, label_mask, smooth_eps=0.1)\n",
    "\n",
    "                val_loss = (lambda_L1     * loss_l1 +\n",
    "                            lambda_msspec * loss_msspec +\n",
    "                            lambda_loud   * loss_loud +\n",
    "                            lambda_loc    * loss_loc)\n",
    "\n",
    "                val_total_loss += val_loss.item()\n",
    "                val_steps += 1\n",
    "\n",
    "        val_avg_loss = val_total_loss / val_steps if val_steps>0 else 0.0\n",
    "\n",
    "        # Step the LR scheduler once after each epoch\n",
    "        scheduler.step()\n",
    "\n",
    "        generator.train()\n",
    "        detector.train()\n",
    "        \n",
    "        print(f\"\\nEpoch [{epoch}/{num_epochs}] Summary:\")\n",
    "        print(f\"  Train Avg Loss: {train_avg_loss:.4f}\")\n",
    "        print(f\"  Val   Avg Loss: {val_avg_loss:.4f}\")\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        print(f\"  Current LR: {current_lr:.6f}\\n\")\n",
    "\n",
    "    print(\"Finished training (Generator + Detector) with Cosine LR + validation each epoch.\")\n",
    "    return generator, detector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_detector(\n",
    "    generator,\n",
    "    detector,\n",
    "    test_dataset,\n",
    "    device,\n",
    "    threshold=0.5,\n",
    "    batch_size=16\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate the detector on a test set.\n",
    "    For each clip: watermarked => label=1, clean => label=0.\n",
    "    Computes confusion matrix, classification report, TPR, FPR, and accuracy.\n",
    "    \"\"\"\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    generator.eval()\n",
    "    detector.eval()\n",
    "\n",
    "    y_true = []\n",
    "    y_score = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for s in test_loader:\n",
    "            s = s.to(device)\n",
    "            B = s.shape[0]\n",
    "\n",
    "            delta = generator(s)\n",
    "            s_w   = s + delta\n",
    "\n",
    "            combined = torch.cat([s_w, s], dim=0)\n",
    "            labels = np.concatenate([np.ones(B), np.zeros(B)], axis=0)\n",
    "\n",
    "            det_out = detector(combined)  # (2B,1,T)\n",
    "            scores = det_out.mean(dim=2).squeeze(1).cpu().numpy()  # shape (2B,)\n",
    "\n",
    "            y_true.append(labels)\n",
    "            y_score.append(scores)\n",
    "\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_score = np.concatenate(y_score, axis=0)\n",
    "\n",
    "    y_pred = (y_score >= threshold).astype(int)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    clf_report = classification_report(y_true, y_pred, target_names=[\"Clean(0)\", \"Watermarked(1)\"])\n",
    "\n",
    "    tp = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    fp = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    tn = np.sum((y_pred == 0) & (y_true == 0))\n",
    "    fn = np.sum((y_pred == 0) & (y_true == 1))\n",
    "\n",
    "    tpr = tp / (tp + fn + 1e-8)\n",
    "    fpr = fp / (fp + tn + 1e-8)\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn + 1e-8)\n",
    "\n",
    "    print(f\"\\n--- Detector Evaluation (threshold={threshold}) ---\")\n",
    "    print(f\"Accuracy: {acc:.3f},  TPR: {tpr:.3f},  FPR: {fpr:.3f}\")\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    print(\"Classification Report:\\n\", clf_report)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"TPR\": tpr,\n",
    "        \"FPR\": fpr,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"classification_report\": clf_report\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Dataset Size: 1000\n",
      "Train/Val/Test Split: 800/100/100\n",
      "Dataset => Train: 800, Val: 100, Test: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3354/4192679983.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  gen_sd = torch.load(\"generator.pth\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Phase 1 Generator. Ready for Phase 2 training.\n",
      "Created new Detector instance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]: 100%|██████████| 25/25 [00:22<00:00,  1.12it/s, loss=0.3583]\n",
      "Epoch 1/10 [Val]:  50%|█████     | 2/4 [00:01<00:01,  1.22it/s]W0314 18:09:01.137000 3354 torch/_dynamo/convert_frame.py:844] [1/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "W0314 18:09:01.137000 3354 torch/_dynamo/convert_frame.py:844] [1/8]    function: 'forward' (/tmp/ipykernel_3354/1045307349.py:14)\n",
      "W0314 18:09:01.137000 3354 torch/_dynamo/convert_frame.py:844] [1/8]    last reason: 1/0: GLOBAL_STATE changed: grad_mode \n",
      "W0314 18:09:01.137000 3354 torch/_dynamo/convert_frame.py:844] [1/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0314 18:09:01.137000 3354 torch/_dynamo/convert_frame.py:844] [1/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/10] Summary:\n",
      "  Train Avg Loss: 0.3872\n",
      "  Val   Avg Loss: 0.3572\n",
      "  Current LR: 0.000976\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Train]: 100%|██████████| 25/25 [00:06<00:00,  3.64it/s, loss=0.3616]\n",
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2/10] Summary:\n",
      "  Train Avg Loss: 0.3564\n",
      "  Val   Avg Loss: 0.3569\n",
      "  Current LR: 0.000905\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Train]: 100%|██████████| 25/25 [00:06<00:00,  3.71it/s, loss=0.3674]\n",
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [3/10] Summary:\n",
      "  Train Avg Loss: 0.3534\n",
      "  Val   Avg Loss: 0.3713\n",
      "  Current LR: 0.000796\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Train]: 100%|██████████| 25/25 [00:06<00:00,  3.74it/s, loss=0.3611]\n",
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [4/10] Summary:\n",
      "  Train Avg Loss: 0.3575\n",
      "  Val   Avg Loss: 0.3650\n",
      "  Current LR: 0.000658\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Train]: 100%|██████████| 25/25 [00:06<00:00,  3.66it/s, loss=0.3483]\n",
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [5/10] Summary:\n",
      "  Train Avg Loss: 0.3533\n",
      "  Val   Avg Loss: 0.3548\n",
      "  Current LR: 0.000505\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Train]: 100%|██████████| 25/25 [00:06<00:00,  3.76it/s, loss=0.3491]\n",
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [6/10] Summary:\n",
      "  Train Avg Loss: 0.3492\n",
      "  Val   Avg Loss: 0.3486\n",
      "  Current LR: 0.000352\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Train]: 100%|██████████| 25/25 [00:06<00:00,  3.73it/s, loss=0.3479]\n",
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [7/10] Summary:\n",
      "  Train Avg Loss: 0.3490\n",
      "  Val   Avg Loss: 0.3485\n",
      "  Current LR: 0.000214\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Train]: 100%|██████████| 25/25 [00:06<00:00,  3.68it/s, loss=0.3493]\n",
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8/10] Summary:\n",
      "  Train Avg Loss: 0.3485\n",
      "  Val   Avg Loss: 0.3491\n",
      "  Current LR: 0.000105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Train]: 100%|██████████| 25/25 [00:06<00:00,  3.76it/s, loss=0.3493]\n",
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [9/10] Summary:\n",
      "  Train Avg Loss: 0.3488\n",
      "  Val   Avg Loss: 0.3486\n",
      "  Current LR: 0.000034\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Train]: 100%|██████████| 25/25 [00:06<00:00,  3.69it/s, loss=0.3479]\n",
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [10/10] Summary:\n",
      "  Train Avg Loss: 0.3482\n",
      "  Val   Avg Loss: 0.3482\n",
      "  Current LR: 0.000010\n",
      "\n",
      "Finished training (Generator + Detector) with Cosine LR + validation each epoch.\n",
      "Saved generator_phase2.pth and detector_phase2.pth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Detector Evaluation (threshold=0.5) ---\n",
      "Accuracy: 0.500,  TPR: 0.030,  FPR: 0.030\n",
      "Confusion Matrix:\n",
      " [[97  3]\n",
      " [97  3]]\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Clean(0)       0.50      0.97      0.66       100\n",
      "Watermarked(1)       0.50      0.03      0.06       100\n",
      "\n",
      "      accuracy                           0.50       200\n",
      "     macro avg       0.50      0.50      0.36       200\n",
      "  weighted avg       0.50      0.50      0.36       200\n",
      "\n",
      "\n",
      "Final Test Metrics: {'accuracy': 0.499999999975, 'TPR': 0.029999999997000003, 'FPR': 0.029999999997000003, 'confusion_matrix': array([[97,  3],\n",
      "       [97,  3]]), 'classification_report': '                precision    recall  f1-score   support\\n\\n      Clean(0)       0.50      0.97      0.66       100\\nWatermarked(1)       0.50      0.03      0.06       100\\n\\n      accuracy                           0.50       200\\n     macro avg       0.50      0.50      0.36       200\\n  weighted avg       0.50      0.50      0.36       200\\n'}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ############################################################################\n",
    "    # PHASE 2 MAIN\n",
    "    ############################################################################\n",
    "    data_root = \"data/200_speech_only\"\n",
    "    full_dataset = OneSecClipsDataset(root_dir=data_root, sample_rate=SAMPLE_RATE)\n",
    "    n = len(full_dataset)\n",
    "    # n_train = int(0.8 * n)\n",
    "    # n_val   = int(0.1 * n)\n",
    "    # n_test  = n - n_train - n_val\n",
    "    # train_ds, val_ds, test_ds = random_split(full_dataset, [n_train, n_val, n_test])\n",
    "    subset_size = 1000\n",
    "    subset_indices = list(range(min(subset_size, len(full_dataset))))\n",
    "    subset_dataset = torch.utils.data.Subset(full_dataset, subset_indices)\n",
    "\n",
    "    n = len(subset_dataset)\n",
    "    n_train = int(0.8 * n)\n",
    "    n_val   = int(0.1 * n)\n",
    "    n_test  = n - n_train - n_val\n",
    "    train_ds, val_ds, test_ds = random_split(subset_dataset, [n_train, n_val, n_test])\n",
    "    print(f\"Subset Dataset Size: {len(subset_dataset)}\")\n",
    "    print(f\"Train/Val/Test Split: {len(train_ds)}/{len(val_ds)}/{len(test_ds)}\")\n",
    "    print(f\"Dataset => Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")\n",
    "\n",
    "    # 1) Load Phase 1 Generator\n",
    "    generator = Generator().to(device)\n",
    "    gen_sd = torch.load(\"generator.pth\", map_location=device)\n",
    "\n",
    "    # Fix any \"_orig_mod.\" prefix\n",
    "    fixed_state_dict = {}\n",
    "    for k, v in gen_sd.items():\n",
    "        if k.startswith(\"_orig_mod.\"):\n",
    "            new_k = k.replace(\"_orig_mod.\", \"\")\n",
    "            fixed_state_dict[new_k] = v\n",
    "        else:\n",
    "            fixed_state_dict[k] = v\n",
    "\n",
    "    generator.load_state_dict(fixed_state_dict, strict=True)\n",
    "    generator.eval()\n",
    "    print(\"Loaded Phase 1 Generator. Ready for Phase 2 training.\")\n",
    "\n",
    "    # 2) Create new Detector\n",
    "    detector = Detector(in_channels=1, base_channels=32, strides=[2,4,5,8]).to(device)\n",
    "    detector.eval()\n",
    "    print(\"Created new Detector instance.\")\n",
    "\n",
    "    # 3) Train them together with validation\n",
    "    num_epochs_phase2 = 10\n",
    "    batch_size_phase2 = 32\n",
    "    gen2, det2 = train_generator_detector(\n",
    "        generator=generator,\n",
    "        detector=detector,\n",
    "        train_dataset=train_ds,\n",
    "        val_dataset=val_ds,\n",
    "        device=device,\n",
    "        num_epochs=num_epochs_phase2,\n",
    "        batch_size=batch_size_phase2,\n",
    "        lr=1e-3,\n",
    "        lambda_L1=1.0,\n",
    "        lambda_msspec=1.0,\n",
    "        lambda_loud=0.5,\n",
    "        lambda_loc=2.0,\n",
    "        use_robustness=False\n",
    "    )\n",
    "\n",
    "    # 4) Save the newly fine-tuned Generator + Detector\n",
    "    torch.save(gen2.state_dict(), \"generator_phase2.pth\")\n",
    "    torch.save(det2.state_dict(), \"detector_phase2.pth\")\n",
    "    print(\"Saved generator_phase2.pth and detector_phase2.pth.\")\n",
    "\n",
    "    # 5) Evaluate on the Test set (10% from above)\n",
    "    metrics = evaluate_detector(\n",
    "        generator=gen2,\n",
    "        detector=det2,\n",
    "        test_dataset=test_ds,\n",
    "        device=device,\n",
    "        threshold=0.5,\n",
    "        batch_size=16\n",
    "    )\n",
    "    print(\"\\nFinal Test Metrics:\", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
