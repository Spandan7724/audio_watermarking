{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ------------------- CHUNK 1: Environment Setup and Hyperparameters ------------------- #\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fix random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Hyperparameters\n",
    "SAMPLE_RATE = 16000        # Audio sample rate\n",
    "AUDIO_LEN   = 16000        # 1-second audio (16k samples)\n",
    "BATCH_SIZE  = 64           # Batch size for training\n",
    "LR          = 1e-3         # Learning rate\n",
    "HIDDEN_DIM  = 32           # Hidden dimension for LSTM in Generator\n",
    "NUM_BITS    = 16           # Number of message bits\n",
    "CHANNELS    = 32           # Initial convolution channels\n",
    "OUTPUT_CH   = 128          # Final conv channels for Generator\n",
    "STRIDES     = [2, 4, 5, 8]  # Downsampling strides for encoder/decoder blocks\n",
    "LSTM_LAYERS = 2            # Number of LSTM layers\n",
    "NUM_WORKERS = 16           # Number of DataLoader workers\n",
    "\n",
    "# Loss Weights (for the composite loss)\n",
    "lambda_L1     = 1.0\n",
    "lambda_msspec = 1.0\n",
    "lambda_adv    = 0.1\n",
    "lambda_loud   = 0.5\n",
    "lambda_loc    = 1.0\n",
    "lambda_dec    = 1.0\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CHUNK #2 completed: Dataset class and augmentation functions defined.\n"
     ]
    }
   ],
   "source": [
    "# ------------------- CHUNK 2: Dataset & Augmentations for 1-sec clips ------------------- #\n",
    "\n",
    "class OneSecClipsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Assumes each .wav file in root_dir is a ~1-sec clip (16k samples).\n",
    "    If sample_rate != 16000, it resamples to 16k.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, sample_rate=SAMPLE_RATE):\n",
    "        super().__init__()\n",
    "        self.filepaths = glob.glob(os.path.join(root_dir, '**', '*.wav'), recursive=True)\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav_path = self.filepaths[idx]\n",
    "        waveform, sr = torchaudio.load(wav_path)\n",
    "\n",
    "        # Convert to mono if multi-channel\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # Resample if needed\n",
    "        if sr != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        # Ensure the clip is 1 second (pad/crop if necessary)\n",
    "        if waveform.shape[1] > AUDIO_LEN:\n",
    "            waveform = waveform[:, :AUDIO_LEN]\n",
    "        elif waveform.shape[1] < AUDIO_LEN:\n",
    "            pad_amt = AUDIO_LEN - waveform.shape[1]\n",
    "            waveform = F.pad(waveform, (0, pad_amt))\n",
    "\n",
    "        return waveform  # shape: (1, AUDIO_LEN)\n",
    "\n",
    "def watermark_masking_augmentation(wav, p_replace_orig=0.4, p_replace_zero=0.2, p_replace_diff=0.2):\n",
    "    \"\"\"\n",
    "    Randomly masks portions of the audio:\n",
    "    - p_replace_orig: Placeholder for original (no-op here)\n",
    "    - p_replace_zero: Replace segment with zeros\n",
    "    - p_replace_diff: Replace segment with random noise\n",
    "    \"\"\"\n",
    "    T = wav.shape[1]\n",
    "    window_len = int(0.1 * SAMPLE_RATE)  # 0.1 second window\n",
    "    k = 5  # number of windows to apply augmentation\n",
    "    for _ in range(k):\n",
    "        start = random.randint(0, T - window_len)\n",
    "        end = start + window_len\n",
    "        choice = random.random()\n",
    "        if choice < p_replace_orig:\n",
    "            # No-op: placeholder for original\n",
    "            pass\n",
    "        elif choice < p_replace_orig + p_replace_zero:\n",
    "            # Replace with zeros\n",
    "            wav[:, start:end] = 0.0\n",
    "        elif choice < p_replace_orig + p_replace_zero + p_replace_diff:\n",
    "            # Replace with random noise (scaled)\n",
    "            wav[:, start:end] = 0.1 * torch.randn_like(wav[:, start:end])\n",
    "        else:\n",
    "            # Leave unchanged\n",
    "            pass\n",
    "    return wav\n",
    "\n",
    "def robustness_augmentations(wav):\n",
    "    \"\"\"\n",
    "    Adds small random noise for robustness.\n",
    "    \"\"\"\n",
    "    return wav + 0.005 * torch.randn_like(wav)\n",
    "\n",
    "print(\"\\nCHUNK #2 completed: Dataset class and augmentation functions defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CHUNK #3 completed: Generator and Detector models defined.\n"
     ]
    }
   ],
   "source": [
    "# ------------------- CHUNK 3: Model Definitions (Generator & Detector) ------------------- #\n",
    "\n",
    "def make_conv1d(in_ch, out_ch, kernel_size=3, stride=1, padding=1):\n",
    "    return nn.Conv1d(in_ch, out_ch, kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.downsample = (stride != 1 or in_ch != out_ch)\n",
    "        self.conv1 = make_conv1d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1)\n",
    "        self.conv2 = make_conv1d(out_ch, out_ch, kernel_size=3, stride=1, padding=1)\n",
    "        self.elu   = nn.ELU()\n",
    "        if self.downsample:\n",
    "            self.skip_conv = make_conv1d(in_ch, out_ch, kernel_size=1, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.elu(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.skip_conv(residual)\n",
    "        out = self.elu(out + residual)\n",
    "        return out\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels=1, \n",
    "                 base_channels=CHANNELS,\n",
    "                 hidden_dim=HIDDEN_DIM, \n",
    "                 message_bits=NUM_BITS,\n",
    "                 output_channels=OUTPUT_CH, \n",
    "                 strides=STRIDES):\n",
    "        super().__init__()\n",
    "        self.message_bits = message_bits\n",
    "        self.hidden_dim   = hidden_dim\n",
    "        \n",
    "        # Embedding layer for the watermark message\n",
    "        self.E = nn.Embedding(num_embeddings=(2**message_bits), embedding_dim=hidden_dim)\n",
    "\n",
    "        # ------------------- Encoder ------------------- #\n",
    "        self.init_conv = nn.Conv1d(in_channels, base_channels, kernel_size=7, stride=1, padding=3)\n",
    "        enc_blocks = []\n",
    "        ch = base_channels\n",
    "        for st in strides:\n",
    "            out_ch = ch * 2\n",
    "            enc_blocks.append(ResidualBlock(ch, out_ch, stride=st))\n",
    "            ch = out_ch\n",
    "        self.encoder_blocks = nn.Sequential(*enc_blocks)\n",
    "\n",
    "        # Project encoder output to hidden_dim (to prepare for LSTM)\n",
    "        self.proj = nn.Linear(ch, hidden_dim)  # ch is typically large after encoder\n",
    "\n",
    "        # LSTM to process the temporal sequence\n",
    "        self.lstm = nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=False)\n",
    "\n",
    "        self.final_conv_enc = nn.Conv1d(hidden_dim, output_channels, kernel_size=7, stride=1, padding=3)\n",
    "\n",
    "        # ------------------- Decoder ------------------- #\n",
    "        dec_blocks = []\n",
    "        rev_strides = list(reversed(strides))\n",
    "        in_ch = output_channels  # start with output_channels from encoder\n",
    "        for st in rev_strides:\n",
    "            out_ch = in_ch // 2  # reduce channels at each block\n",
    "            dec_blocks.append(nn.ConvTranspose1d(in_ch, out_ch, kernel_size=2*st, stride=st, padding=(st//2), output_padding=0))\n",
    "            dec_blocks.append(ResidualBlock(out_ch, out_ch, stride=1))\n",
    "            in_ch = out_ch\n",
    "        self.decoder_blocks = nn.Sequential(*dec_blocks)\n",
    "\n",
    "        # Final convolution to produce the delta (watermark perturbation)\n",
    "        self.final_conv_dec = nn.Conv1d(in_ch, 1, kernel_size=7, stride=1, padding=3)\n",
    "\n",
    "    def forward(self, s, message=None):\n",
    "        B, _, T = s.shape\n",
    "        x = self.init_conv(s)\n",
    "        x = self.encoder_blocks(x)\n",
    "        x_t = x.transpose(1, 2)  # shape: (B, T_after, ch)\n",
    "        x_t = self.proj(x_t)      # project to hidden_dim\n",
    "\n",
    "        if message is not None:\n",
    "            e = self.E(message)   # shape: (B, hidden_dim)\n",
    "            T_after = x_t.shape[1]\n",
    "            e_expanded = e.unsqueeze(1).expand(-1, T_after, -1)  # (B, T_after, hidden_dim)\n",
    "            x_t = x_t + e_expanded\n",
    "\n",
    "        lstm_out, _ = self.lstm(x_t)\n",
    "        lstm_out_t = lstm_out.transpose(1, 2)\n",
    "        latent = self.final_conv_enc(lstm_out_t)\n",
    "\n",
    "        x_dec = latent\n",
    "        x_dec = self.decoder_blocks(x_dec)\n",
    "        delta = self.final_conv_dec(x_dec)\n",
    "        if delta.shape[-1] != T:\n",
    "            min_len = min(delta.shape[-1], T)\n",
    "            delta = delta[:, :, :min_len]\n",
    "            if min_len < T:\n",
    "                pad_amt = T - min_len\n",
    "                delta = F.pad(delta, (0, pad_amt))\n",
    "        return delta\n",
    "\n",
    "class Detector(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels=1, \n",
    "                 base_channels=CHANNELS,\n",
    "                 hidden_dim=HIDDEN_DIM,\n",
    "                 message_bits=NUM_BITS,\n",
    "                 strides=STRIDES):\n",
    "        super().__init__()\n",
    "        self.message_bits = message_bits\n",
    "\n",
    "        self.init_conv = nn.Conv1d(in_channels, base_channels, kernel_size=7, stride=1, padding=3)\n",
    "        enc_blocks = []\n",
    "        ch = base_channels\n",
    "        for st in strides:\n",
    "            out_ch = ch * 2\n",
    "            enc_blocks.append(ResidualBlock(ch, out_ch, stride=st))\n",
    "            ch = out_ch\n",
    "        self.encoder_blocks = nn.Sequential(*enc_blocks)\n",
    "\n",
    "        dec_blocks = []\n",
    "        rev_strides = list(reversed(strides))\n",
    "        in_ch = ch\n",
    "        for st in rev_strides:\n",
    "            out_ch = in_ch // 2\n",
    "            dec_blocks.append(nn.ConvTranspose1d(in_ch, out_ch, kernel_size=2*st, stride=st, padding=(st//2), output_padding=0))\n",
    "            dec_blocks.append(ResidualBlock(out_ch, out_ch, stride=1))\n",
    "            in_ch = out_ch\n",
    "        self.upsample_blocks = nn.Sequential(*dec_blocks)\n",
    "\n",
    "        # Final convolution outputs 1 detection channel + message_bits channels\n",
    "        self.final_conv = nn.Conv1d(base_channels, 1 + message_bits, kernel_size=7, stride=1, padding=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_length = x.shape[-1]\n",
    "        x = self.init_conv(x)\n",
    "        x = self.encoder_blocks(x)\n",
    "        x = self.upsample_blocks(x)\n",
    "        out = self.final_conv(x)\n",
    "        if out.shape[-1] > original_length:\n",
    "            out = out[:, :, :original_length]\n",
    "        elif out.shape[-1] < original_length:\n",
    "            pad_amt = original_length - out.shape[-1]\n",
    "            out = F.pad(out, (0, pad_amt))\n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "print(\"\\nCHUNK #3 completed: Generator and Detector models defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CHUNK #4 completed: All loss functions defined.\n"
     ]
    }
   ],
   "source": [
    "# ------------------- CHUNK 4: Loss Functions ------------------- #\n",
    "\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "# --- Simple Single-Scale Mel Loss ---\n",
    "class SimpleMelLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple mel spectrogram loss that uses a single scale.\n",
    "    Compares the log-mel spectrograms of the original and watermarked audio.\n",
    "    \"\"\"\n",
    "    def __init__(self, sample_rate=SAMPLE_RATE, n_fft=1024, n_mels=80):\n",
    "        super(SimpleMelLoss, self).__init__()\n",
    "        self.mel_spec = T.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=n_fft // 4,\n",
    "            n_mels=n_mels,\n",
    "            normalized=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, original, watermarked):\n",
    "        mel_orig = torch.log(self.mel_spec(original) + 1e-5)\n",
    "        mel_wm   = torch.log(self.mel_spec(watermarked) + 1e-5)\n",
    "        loss = F.l1_loss(mel_orig, mel_wm)\n",
    "        return loss\n",
    "\n",
    "# --- TF-Loudness Loss ---\n",
    "class TFLoudnessLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Time-Frequency loudness loss that ensures perceptual similarity across \n",
    "    different frequency bands and time windows.\n",
    "    \n",
    "    It computes the STFT of the original and watermarked audio and compares \n",
    "    their loudness (energy), spectral shape, and phase differences.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_bands=8, window_size=2048, hop_size=512):\n",
    "        super(TFLoudnessLoss, self).__init__()\n",
    "        self.n_bands = n_bands\n",
    "        self.win_size = window_size\n",
    "        self.hop_size = hop_size\n",
    "        \n",
    "        # Perceptual weighting: higher weights for mid-frequency bands\n",
    "        weights = torch.ones(n_bands)\n",
    "        mid_band_idx = n_bands // 3\n",
    "        weights[mid_band_idx:2 * mid_band_idx] = 1.5\n",
    "        self.register_buffer('band_weights', weights)\n",
    "\n",
    "    def forward(self, original, watermarked):\n",
    "        window = torch.hann_window(self.win_size, device=original.device)\n",
    "        stft_orig = torch.stft(\n",
    "            original.squeeze(1), n_fft=self.win_size, hop_length=self.hop_size,\n",
    "            window=window, return_complex=True, normalized=True\n",
    "        )\n",
    "        stft_wm = torch.stft(\n",
    "            watermarked.squeeze(1), n_fft=self.win_size, hop_length=self.hop_size,\n",
    "            window=window, return_complex=True, normalized=True\n",
    "        )\n",
    "        mag_orig = stft_orig.abs()\n",
    "        mag_wm   = stft_wm.abs()\n",
    "        phase_orig = stft_orig.angle()\n",
    "        phase_wm   = stft_wm.angle()\n",
    "        \n",
    "        freq_bins = mag_orig.shape[1]\n",
    "        band_size = freq_bins // self.n_bands\n",
    "        \n",
    "        loudness_loss = 0.0\n",
    "        spectral_loss = 0.0\n",
    "        phase_loss = 0.0\n",
    "        \n",
    "        for b in range(self.n_bands):\n",
    "            start = b * band_size\n",
    "            end = freq_bins if (b == self.n_bands - 1) else (start + band_size)\n",
    "            band_orig = mag_orig[:, start:end, :]\n",
    "            band_wm = mag_wm[:, start:end, :]\n",
    "            \n",
    "            energy_orig = torch.sum(band_orig ** 2, dim=1)\n",
    "            energy_wm = torch.sum(band_wm ** 2, dim=1)\n",
    "            loud_orig = torch.log10(energy_orig + 1e-8)\n",
    "            loud_wm = torch.log10(energy_wm + 1e-8)\n",
    "            loudness_loss += self.band_weights[b] * F.l1_loss(loud_wm, loud_orig)\n",
    "            spectral_loss += self.band_weights[b] * F.mse_loss(band_wm, band_orig)\n",
    "            phase_diff = 1.0 - torch.cos(phase_wm[:, start:end, :] - phase_orig[:, start:end, :])\n",
    "            phase_loss += self.band_weights[b] * phase_diff.mean()\n",
    "        \n",
    "        loudness_loss /= self.n_bands\n",
    "        spectral_loss /= self.n_bands\n",
    "        phase_loss /= self.n_bands\n",
    "        \n",
    "        return loudness_loss + spectral_loss + 0.2 * phase_loss\n",
    "\n",
    "# --- Adversarial Loss ---\n",
    "class AdversarialLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Adversarial loss using a discriminator network that distinguishes between\n",
    "    original and watermarked audio.\n",
    "    \n",
    "    The generator is penalized if the discriminator can tell them apart,\n",
    "    encouraging imperceptible watermark embedding.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(AdversarialLoss, self).__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=15, stride=1, padding=7),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(16, 32, kernel_size=41, stride=4, padding=20),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(32, 64, kernel_size=41, stride=4, padding=20),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(64, 128, kernel_size=41, stride=4, padding=20),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(128, 1, kernel_size=41, stride=4, padding=20),\n",
    "        )\n",
    "        self.disc_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=1e-4)\n",
    "\n",
    "    def forward(self, original, watermarked, train_disc=True):\n",
    "        if train_disc:\n",
    "            self.disc_optimizer.zero_grad()\n",
    "            real_output = self.discriminator(original)\n",
    "            real_loss = F.binary_cross_entropy_with_logits(real_output, torch.ones_like(real_output))\n",
    "            fake_output = self.discriminator(watermarked.detach())\n",
    "            fake_loss = F.binary_cross_entropy_with_logits(fake_output, torch.zeros_like(fake_output))\n",
    "            disc_loss = real_loss + fake_loss\n",
    "            disc_loss.backward()\n",
    "            self.disc_optimizer.step()\n",
    "        fake_output = self.discriminator(watermarked)\n",
    "        gen_loss = F.binary_cross_entropy_with_logits(fake_output, torch.ones_like(fake_output))\n",
    "        return gen_loss\n",
    "\n",
    "# --- Masked Localization Loss ---\n",
    "def masked_localization_loss(detector_out, mask, smooth_eps=0.1):\n",
    "    \"\"\"\n",
    "    Localization loss with label smoothing and focal weighting.\n",
    "    \n",
    "    It compares the detector's detection probability (first channel) to the provided mask.\n",
    "    \"\"\"\n",
    "    det_prob = detector_out[:, 0:1, :]\n",
    "    smoothed_mask = mask * (1.0 - smooth_eps) + (1.0 - mask) * smooth_eps\n",
    "    pt = torch.where(mask > 0.5, det_prob, 1 - det_prob)\n",
    "    focal_weight = (1 - pt) ** 2\n",
    "    bce_loss = F.binary_cross_entropy(det_prob, smoothed_mask, reduction='none')\n",
    "    focal_loss = focal_weight * bce_loss\n",
    "    return focal_loss.mean()\n",
    "\n",
    "# --- Decoding Loss ---\n",
    "def decoding_loss(detector_out, message, mask=None, gamma=2.0):\n",
    "    \"\"\"\n",
    "    Decoding loss ensures the embedded watermark message is recoverable.\n",
    "    \n",
    "    It extracts bit probabilities from detector_out and compares them to the true message bits.\n",
    "    \"\"\"\n",
    "    B, channels, T = detector_out.shape\n",
    "    b = channels - 1  # number of bit channels\n",
    "    if b <= 0:\n",
    "        return torch.tensor(0.0, device=detector_out.device)\n",
    "    \n",
    "    bit_prob_map = detector_out[:, 1:, :]  # shape (B, b, T)\n",
    "    \n",
    "    if mask is not None:\n",
    "        expanded_mask = mask.expand(-1, b, -1)\n",
    "        masked_bit_map = bit_prob_map * expanded_mask\n",
    "        weights = expanded_mask.sum(dim=2, keepdim=True) + 1e-8\n",
    "        bit_prob = (masked_bit_map.sum(dim=2) / weights.squeeze(2))\n",
    "    else:\n",
    "        confidence = torch.abs(bit_prob_map - 0.5) * 2.0\n",
    "        attention = F.softmax(confidence * 5.0, dim=2)\n",
    "        bit_prob = (bit_prob_map * attention).sum(dim=2)\n",
    "    \n",
    "    # Convert message to bit vectors\n",
    "    msg_bits = []\n",
    "    for i in range(b):\n",
    "        bit_i = ((message >> i) & 1).float()\n",
    "        msg_bits.append(bit_i)\n",
    "    msg_bits = torch.stack(msg_bits, dim=1)  # shape (B, b)\n",
    "    \n",
    "    pt = torch.where(msg_bits > 0.5, bit_prob, 1 - bit_prob)\n",
    "    focal_weight = (1 - pt) ** gamma\n",
    "    bce = F.binary_cross_entropy(bit_prob, msg_bits, reduction='none')\n",
    "    focal_bce = focal_weight * bce\n",
    "    return focal_bce.mean()\n",
    "\n",
    "print(\"\\nCHUNK #4 completed: All loss functions defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CHUNK #5 completed: Training and validation functions defined.\n"
     ]
    }
   ],
   "source": [
    "# ------------------- CHUNK 5: Training & Validation Functions ------------------- #\n",
    "\n",
    "def generate_random_messages(batch_size, max_val=(2**NUM_BITS)):\n",
    "    \"\"\"\n",
    "    Generates random integers in the range [0, 2**NUM_BITS - 1] as watermark messages.\n",
    "    \"\"\"\n",
    "    return torch.randint(0, max_val, (batch_size,))\n",
    "\n",
    "def train_one_epoch(generator, detector, train_loader, optimizer, epoch, total_epochs, device):\n",
    "    \"\"\"\n",
    "    Runs one training epoch using the composite loss.\n",
    "    \"\"\"\n",
    "    generator.train()\n",
    "    detector.train()\n",
    "    total_loss = 0.0\n",
    "    total_steps = len(train_loader)\n",
    "    \n",
    "    # Instantiate loss modules (from CHUNK 4)\n",
    "    # Note: We use MultiScaleMelLoss defined in CHUNK 5 of previous code (or SimpleMelLoss as desired)\n",
    "    ms_mel_loss = SimpleMelLoss().to(device)\n",
    "    adv_loss_stub = AdversarialLoss().to(device)\n",
    "    tf_loud_loss = TFLoudnessLoss().to(device)\n",
    "    \n",
    "    pbar = tqdm(enumerate(train_loader), total=total_steps, desc=f\"Epoch [{epoch}/{total_epochs}]\")\n",
    "    for i, s in pbar:\n",
    "        s = s.to(device)  # shape: (B, 1, AUDIO_LEN)\n",
    "        B = s.shape[0]\n",
    "        msgs = generate_random_messages(B).to(device)\n",
    "        \n",
    "        # Forward pass: generate watermark delta and form watermarked audio\n",
    "        delta = generator(s, msgs)\n",
    "        s_w = s + delta\n",
    "        \n",
    "        # Apply optional augmentations for robustness\n",
    "        for b_idx in range(B):\n",
    "            s_w[b_idx] = watermark_masking_augmentation(s_w[b_idx])\n",
    "            s_w[b_idx] = robustness_augmentations(s_w[b_idx])\n",
    "        \n",
    "        # Detector forward pass\n",
    "        det_out = detector(s_w)\n",
    "        \n",
    "        # Compute each loss component:\n",
    "        loss_l1   = F.l1_loss(delta, torch.zeros_like(delta))\n",
    "        loss_msspec = ms_mel_loss(s, s_w)\n",
    "        loss_adv  = adv_loss_stub(s, s_w, train_disc=True)\n",
    "        loss_loud = tf_loud_loss(s, s_w)\n",
    "        \n",
    "        # Assume full watermark coverage for localization\n",
    "        mask = torch.ones((B, 1, s.shape[-1]), device=device)\n",
    "        loss_loc  = masked_localization_loss(det_out, mask)\n",
    "        loss_dec  = decoding_loss(det_out, msgs, mask)\n",
    "        \n",
    "        # Composite loss using predefined lambda weights\n",
    "        loss = (lambda_L1     * loss_l1 +\n",
    "                lambda_msspec * loss_msspec +\n",
    "                lambda_adv    * loss_adv +\n",
    "                lambda_loud   * loss_loud +\n",
    "                lambda_loc    * loss_loc +\n",
    "                lambda_dec    * loss_dec)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    avg_loss = total_loss / total_steps\n",
    "    return avg_loss\n",
    "\n",
    "def validate_one_epoch(generator, detector, val_loader, device):\n",
    "    \"\"\"\n",
    "    Runs validation over the validation set using the composite loss.\n",
    "    \"\"\"\n",
    "    generator.eval()\n",
    "    detector.eval()\n",
    "    total_loss = 0.0\n",
    "    steps = 0\n",
    "    # Instantiate loss modules\n",
    "    ms_mel_loss = SimpleMelLoss().to(device)\n",
    "    adv_loss_stub = AdversarialLoss().to(device)\n",
    "    tf_loud_loss = TFLoudnessLoss().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for s in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "            s = s.to(device)\n",
    "            B = s.shape[0]\n",
    "            msgs = generate_random_messages(B).to(device)\n",
    "            \n",
    "            delta = generator(s, msgs)\n",
    "            s_w = s + delta\n",
    "            det_out = detector(s_w)\n",
    "            \n",
    "            loss_l1   = F.l1_loss(delta, torch.zeros_like(delta))\n",
    "            loss_msspec = ms_mel_loss(s, s_w)\n",
    "            loss_adv  = adv_loss_stub(s, s_w, train_disc=False)\n",
    "            loss_loud = tf_loud_loss(s, s_w)\n",
    "            mask      = torch.ones((B, 1, s.shape[-1]), device=device)\n",
    "            loss_loc  = masked_localization_loss(det_out, mask)\n",
    "            loss_dec  = decoding_loss(det_out, msgs, mask)\n",
    "            \n",
    "            loss = (lambda_L1     * loss_l1 +\n",
    "                    lambda_msspec * loss_msspec +\n",
    "                    lambda_adv    * loss_adv +\n",
    "                    lambda_loud   * loss_loud +\n",
    "                    lambda_loc    * loss_loc +\n",
    "                    lambda_dec    * loss_dec)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            steps += 1\n",
    "    avg_loss = total_loss / steps if steps > 0 else 0.0\n",
    "    return avg_loss\n",
    "\n",
    "def train_model(generator, detector, train_dataset, val_dataset, num_epochs=10, lr=LR):\n",
    "    \"\"\"\n",
    "    Full training loop over multiple epochs.\n",
    "    \"\"\"\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    optimizer = optim.Adam(list(generator.parameters()) + list(detector.parameters()), lr=lr)\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        train_loss = train_one_epoch(generator, detector, train_loader, optimizer, epoch, num_epochs, device)\n",
    "        val_loss   = validate_one_epoch(generator, detector, val_loader, device)\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}]  TRAIN Loss: {train_loss:.4f}  |  VAL Loss: {val_loss:.4f}\")\n",
    "\n",
    "print(\"\\nCHUNK #5 completed: Training and validation functions defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CHUNK #6 completed: Inference and evaluation functions defined.\n"
     ]
    }
   ],
   "source": [
    "# ------------------- CHUNK 6: Inference & Evaluation Functions ------------------- #\n",
    "\n",
    "def detect_watermark(detector, audio, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Determines whether a watermark is present.\n",
    "    \n",
    "    Args:\n",
    "        detector: Trained detector model.\n",
    "        audio: Tensor of shape (B, 1, T).\n",
    "        threshold: Threshold on the averaged detection probability.\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of shape (B,) containing 1 if watermark detected, 0 otherwise.\n",
    "    \"\"\"\n",
    "    detector.eval()\n",
    "    with torch.no_grad():\n",
    "        out = detector(audio)  # shape: (B, 1+b, T)\n",
    "        det_prob = out[:, 0, :]  # detection probability channel\n",
    "        avg_prob = det_prob.mean(dim=1)\n",
    "        return (avg_prob > threshold).float()\n",
    "\n",
    "def localize_watermark(detector, audio, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Localizes the watermark in the audio.\n",
    "    \n",
    "    Args:\n",
    "        detector: Trained detector model.\n",
    "        audio: Tensor of shape (B, 1, T).\n",
    "        threshold: Threshold for detection.\n",
    "        \n",
    "    Returns:\n",
    "        A mask tensor of shape (B, T) with 1 indicating watermark presence.\n",
    "    \"\"\"\n",
    "    detector.eval()\n",
    "    with torch.no_grad():\n",
    "        out = detector(audio)\n",
    "        det_prob = out[:, 0, :]\n",
    "        return (det_prob > threshold).float()\n",
    "\n",
    "def decode_message(detector, audio):\n",
    "    \"\"\"\n",
    "    Decodes the watermark message from the audio.\n",
    "    \n",
    "    Args:\n",
    "        detector: Trained detector model.\n",
    "        audio: Tensor of shape (B, 1, T).\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of shape (B,) with the decoded message (as an integer).\n",
    "    \"\"\"\n",
    "    detector.eval()\n",
    "    with torch.no_grad():\n",
    "        out = detector(audio)  # shape: (B, 1+b, T)\n",
    "        bit_prob_map = out[:, 1:, :]  # bit probability channels\n",
    "        bit_prob = bit_prob_map.mean(dim=2)  # average over time -> (B, b)\n",
    "        bits_decoded = (bit_prob > 0.5).int()  # threshold to obtain bits\n",
    "        B, b = bits_decoded.shape\n",
    "        msg_int = torch.zeros(B, dtype=torch.long, device=bits_decoded.device)\n",
    "        for i in range(b):\n",
    "            msg_int |= (bits_decoded[:, i] << i)\n",
    "        return msg_int\n",
    "\n",
    "# --- Evaluation Metrics (Placeholders) ---\n",
    "def evaluate_si_snr_torch(original, reconstructed, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Computes SI-SNR (in dB) between original and reconstructed audio.\n",
    "    \"\"\"\n",
    "    if original.dim() == 3:\n",
    "        original = original.squeeze(1)\n",
    "    if reconstructed.dim() == 3:\n",
    "        reconstructed = reconstructed.squeeze(1)\n",
    "    \n",
    "    # Zero-mean signals\n",
    "    original_zm = original - original.mean(dim=1, keepdim=True)\n",
    "    recon_zm = reconstructed - reconstructed.mean(dim=1, keepdim=True)\n",
    "    \n",
    "    dot = (original_zm * recon_zm).sum(dim=1, keepdim=True)\n",
    "    norm_sq = (original_zm ** 2).sum(dim=1, keepdim=True) + eps\n",
    "    alpha = dot / norm_sq\n",
    "    \n",
    "    s_target = alpha * original_zm\n",
    "    e_noise = recon_zm - s_target\n",
    "    si_snr_val = 10 * torch.log10((s_target ** 2).sum(dim=1) / ((e_noise ** 2).sum(dim=1) + eps))\n",
    "    return si_snr_val\n",
    "\n",
    "def evaluate_pesq(original, reconstructed, sr=SAMPLE_RATE):\n",
    "    \"\"\"\n",
    "    Computes PESQ score (placeholder). Replace with actual PESQ calculation if available.\n",
    "    \"\"\"\n",
    "    # For now, return a dummy score\n",
    "    return 4.5 - random.random()\n",
    "\n",
    "def run_evaluation(generator, detector, test_dataset, device,\n",
    "                   batch_size=16, num_workers=4, sr=SAMPLE_RATE,\n",
    "                   num_bits=16, as_bits=True, \n",
    "                   compute_pesq_score=True, compute_si_snr_score=True):\n",
    "    \"\"\"\n",
    "    Evaluates the watermarking system on the test dataset.\n",
    "    \n",
    "    Steps:\n",
    "      1. Generate random messages.\n",
    "      2. Pass the original audio through the generator to create watermarked audio.\n",
    "      3. Use the detector to decode the watermark.\n",
    "      4. Compute SI-SNR, PESQ, and bit accuracy.\n",
    "    \n",
    "    Returns:\n",
    "      A dictionary containing average SI-SNR, PESQ, and bit accuracy.\n",
    "    \"\"\"\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    generator.eval()\n",
    "    detector.eval()\n",
    "    \n",
    "    si_snr_vals = []\n",
    "    pesq_vals = []\n",
    "    bit_acc_vals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for s in test_loader:\n",
    "            s = s.to(device)  # (B, 1, T)\n",
    "            B = s.shape[0]\n",
    "            msgs = generate_random_messages(B, num_bits=num_bits, as_bits=as_bits).to(device)\n",
    "            delta = generator(s, msgs)\n",
    "            s_w = s + delta\n",
    "            \n",
    "            # Decode message from watermarked audio\n",
    "            bit_prob_map = detector(s_w)  # shape (B, 1+b, T)\n",
    "            # For bit accuracy, compare decoded message with true message.\n",
    "            decoded = decode_message(detector, s_w)\n",
    "            if as_bits:\n",
    "                true_message = msgs  # Assuming bit vectors\n",
    "            else:\n",
    "                true_message = msgs  # Integers\n",
    "            matches = (decoded == true_message).float()\n",
    "            bit_acc_vals.extend(matches.cpu().numpy())\n",
    "            \n",
    "            # Audio quality metrics\n",
    "            if compute_si_snr_score:\n",
    "                si_snr_vals.extend(evaluate_si_snr_torch(s, s_w).cpu().numpy())\n",
    "            if compute_pesq_score:\n",
    "                for i in range(B):\n",
    "                    pesq_vals.append(evaluate_pesq(s[i], s_w[i], sr=sr))\n",
    "    \n",
    "    avg_si_snr = float(np.mean(si_snr_vals)) if si_snr_vals else 0.0\n",
    "    avg_pesq = float(np.mean(pesq_vals)) if pesq_vals else 0.0\n",
    "    avg_bit_acc = float(np.mean(bit_acc_vals)) if bit_acc_vals else 0.0\n",
    "    \n",
    "    print(\"\\n--- Evaluation Results ---\")\n",
    "    if compute_si_snr_score:\n",
    "        print(f\"Average SI-SNR: {avg_si_snr:.3f} dB\")\n",
    "    if compute_pesq_score:\n",
    "        print(f\"Average PESQ: {avg_pesq:.3f}\")\n",
    "    print(f\"Bit Accuracy: {avg_bit_acc*100:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        \"si_snr\": avg_si_snr,\n",
    "        \"pesq\": avg_pesq,\n",
    "        \"bit_accuracy\": avg_bit_acc\n",
    "    }\n",
    "\n",
    "print(\"\\nCHUNK #6 completed: Inference and evaluation functions defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]: 100%|██████████| 4508/4508 [30:22<00:00,  2.47it/s, loss=3.0912]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]  TRAIN Loss: 3.0892  |  VAL Loss: 0.3525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]: 100%|██████████| 4508/4508 [29:43<00:00,  2.53it/s, loss=2.8321]\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]  TRAIN Loss: 3.0964  |  VAL Loss: 0.2903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]: 100%|██████████| 4508/4508 [30:16<00:00,  2.48it/s, loss=4.0534]\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]  TRAIN Loss: 3.1645  |  VAL Loss: 0.3287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]: 100%|██████████| 4508/4508 [30:29<00:00,  2.46it/s, loss=3.1747]\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]  TRAIN Loss: 3.0825  |  VAL Loss: 0.3710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]: 100%|██████████| 4508/4508 [29:52<00:00,  2.52it/s, loss=3.0821]\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]  TRAIN Loss: 3.0108  |  VAL Loss: 0.2916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]: 100%|██████████| 4508/4508 [29:42<00:00,  2.53it/s, loss=3.9139]\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]  TRAIN Loss: 3.0935  |  VAL Loss: 0.3167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]:  12%|█▏        | 529/4508 [8:16:10<62:12:04, 56.28s/it, loss=2.8105]     \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Train the models on the full training set using the composite loss\u001b[39;00m\n\u001b[1;32m     20\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Adjust this value as needed for full training\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Save the trained model state dictionaries\u001b[39;00m\n\u001b[1;32m     24\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(generator\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 123\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(generator, detector, train_dataset, val_dataset, num_epochs, lr)\u001b[0m\n\u001b[1;32m    120\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mlist\u001b[39m(generator\u001b[38;5;241m.\u001b[39mparameters()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(detector\u001b[38;5;241m.\u001b[39mparameters()), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 123\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     val_loss   \u001b[38;5;241m=\u001b[39m validate_one_epoch(generator, detector, val_loader, device)\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]  TRAIN Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  |  VAL Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 62\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(generator, detector, train_loader, optimizer, epoch, total_epochs, device)\u001b[0m\n\u001b[1;32m     54\u001b[0m loss \u001b[38;5;241m=\u001b[39m (lambda_L1     \u001b[38;5;241m*\u001b[39m loss_l1 \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     55\u001b[0m         lambda_msspec \u001b[38;5;241m*\u001b[39m loss_msspec \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     56\u001b[0m         lambda_adv    \u001b[38;5;241m*\u001b[39m loss_adv \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     57\u001b[0m         lambda_loud   \u001b[38;5;241m*\u001b[39m loss_loud \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     58\u001b[0m         lambda_loc    \u001b[38;5;241m*\u001b[39m loss_loc \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     59\u001b[0m         lambda_dec    \u001b[38;5;241m*\u001b[39m loss_dec)\n\u001b[1;32m     61\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 62\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     64\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ------------------- CHUNK 7: Full Training and Evaluation Script ------------------- #\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the full dataset from the specified directory\n",
    "    data_root = \"data/100_all\"\n",
    "    full_dataset = OneSecClipsDataset(root_dir=data_root, sample_rate=SAMPLE_RATE)\n",
    "    \n",
    "    # Split the dataset: 80% train, 10% validation, 10% test\n",
    "    n = len(full_dataset)\n",
    "    n_train = int(0.8 * n)\n",
    "    n_val   = int(0.1 * n)\n",
    "    n_test  = n - n_train - n_val\n",
    "    train_ds, val_ds, test_ds = random_split(full_dataset, [n_train, n_val, n_test])\n",
    "    \n",
    "    # Instantiate the Generator and Detector models and move them to the device\n",
    "    generator = Generator().to(device)\n",
    "    detector  = Detector().to(device)\n",
    "    \n",
    "    # Train the models on the full training set using the composite loss\n",
    "    num_epochs = 10  # Adjust this value as needed for full training\n",
    "    train_model(generator, detector, train_ds, val_ds, num_epochs=num_epochs, lr=LR)\n",
    "    \n",
    "    # Save the trained model state dictionaries\n",
    "    torch.save(generator.state_dict(), \"generator.pth\")\n",
    "    torch.save(detector.state_dict(), \"detector.pth\")\n",
    "    \n",
    "    # Evaluate the trained models on the test dataset\n",
    "    run_evaluation(generator, detector, test_ds, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation(generator, detector, test_ds, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
