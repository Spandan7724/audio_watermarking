{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "CHUNK #1 completed: Environment and hyperparameters set.\n"
     ]
    }
   ],
   "source": [
    "# ------------------- CHUNK 1: Environment Setup ------------------- #\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fix random seeds if desired:\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ------------------ Hyperparameters from your instructions ------------------ #\n",
    "SAMPLE_RATE = 16000\n",
    "AUDIO_LEN   = 16000  # 1 second\n",
    "BATCH_SIZE  = 64\n",
    "LR          = 1e-3\n",
    "HIDDEN_DIM  = 32   # LSTM hidden dimension\n",
    "NUM_BITS    = 16   # message bits\n",
    "CHANNELS    = 32   # initial conv channels\n",
    "OUTPUT_CH   = 128  # final conv channels for the generator\n",
    "STRIDES     = [2, 4, 5, 8]  # downsampling strides\n",
    "LSTM_LAYERS = 2\n",
    "NUM_WORKERS = 16\n",
    "# Loss Weights\n",
    "lambda_L1 = 1.0\n",
    "lambda_msspec = 1.0\n",
    "lambda_adv = 0.1\n",
    "lambda_loud = 0.5\n",
    "lambda_loc = 1.0\n",
    "lambda_dec = 1.0\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "print(\"\\nCHUNK #1 completed: Environment and hyperparameters set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CHUNK #2 updated for 1-sec clips in 'data/100_all' is ready.\n"
     ]
    }
   ],
   "source": [
    "# ------------------- CHUNK 2 (UPDATED): Dataset & Augmentations for 1-sec clips ------------------- #\n",
    "class OneSecClipsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Assumes each .wav file in root_dir is a ~1-sec clip (16k samples).\n",
    "    If sample_rate != 16000, we'll resample to 16k.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, sample_rate=16000):\n",
    "        super().__init__()\n",
    "        self.filepaths = glob.glob(os.path.join(root_dir, '**', '*.wav'), recursive=True)\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav_path = self.filepaths[idx]\n",
    "        waveform, sr = torchaudio.load(wav_path)\n",
    "\n",
    "        # Convert to mono if multi-channel\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # Resample if needed\n",
    "        if sr != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate)\n",
    "            waveform  = resampler(waveform)\n",
    "\n",
    "        # By assumption, each clip is ~1 second at 16 kHz\n",
    "        # If there's a minor mismatch, you can pad/crop here, e.g.:\n",
    "        #   if waveform.shape[1] > 16000:\n",
    "        #       waveform = waveform[:, :16000]\n",
    "        #   elif waveform.shape[1] < 16000:\n",
    "        #       pad_len = 16000 - waveform.shape[1]\n",
    "        #       waveform = F.pad(waveform, (0, pad_len))\n",
    "        \n",
    "        return waveform  # shape: (1, ~16000)\n",
    "\n",
    "def watermark_masking_augmentation(wav, p_replace_orig=0.4, p_replace_zero=0.2, p_replace_diff=0.2):\n",
    "    T = wav.shape[1]\n",
    "    window_len = int(0.1 * 16000)  # 0.1 second if sample_rate=16k\n",
    "    k = 5\n",
    "    for _ in range(k):\n",
    "        start = random.randint(0, T - window_len)\n",
    "        end   = start + window_len\n",
    "        choice = random.random()\n",
    "        if choice < p_replace_orig:\n",
    "            # Replace with 'original' â€“ in a real pipeline you'd keep track \n",
    "            # of the pre-watermarked version. Here it's a placeholder no-op.\n",
    "            pass\n",
    "        elif choice < p_replace_orig + p_replace_zero:\n",
    "            # Replace with zeros\n",
    "            wav[:, start:end] = 0.0\n",
    "        elif choice < p_replace_orig + p_replace_zero + p_replace_diff:\n",
    "            # Replace with random noise as a placeholder\n",
    "            wav[:, start:end] = 0.1 * torch.randn_like(wav[:, start:end])\n",
    "        else:\n",
    "            # Leave unchanged\n",
    "            pass\n",
    "    return wav\n",
    "\n",
    "def robustness_augmentations(wav):\n",
    "    wav = wav + 0.005 * torch.randn_like(wav)\n",
    "    return wav\n",
    "\n",
    "print(\"\\nCHUNK #2 updated for 1-sec clips in 'data/100_all' is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CHUNK #3 completed: Generator model with residual blocks, LSTM, and message embedding defined.\n"
     ]
    }
   ],
   "source": [
    "# ------------------- CHUNK 3: Generator with Residual Blocks, LSTM, Message Embedding ------------------- #\n",
    "\n",
    "def make_conv1d(in_ch, out_ch, kernel_size=3, stride=1, padding=1):\n",
    "    return nn.Conv1d(in_ch, out_ch, kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.downsample = (stride != 1 or in_ch != out_ch)\n",
    "        self.conv1 = make_conv1d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1)\n",
    "        self.conv2 = make_conv1d(out_ch, out_ch, kernel_size=3, stride=1, padding=1)\n",
    "        self.elu   = nn.ELU()\n",
    "        \n",
    "        # If channels differ or stride>1, need a 1D conv to match shapes for skip\n",
    "        if self.downsample:\n",
    "            self.skip_conv = make_conv1d(in_ch, out_ch, kernel_size=1, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.elu(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.skip_conv(residual)\n",
    "        out = self.elu(out + residual)\n",
    "        return out\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels=1, \n",
    "                 base_channels=CHANNELS,\n",
    "                 hidden_dim=HIDDEN_DIM, \n",
    "                 message_bits=NUM_BITS,\n",
    "                 output_channels=OUTPUT_CH, \n",
    "                 strides=STRIDES):\n",
    "        super().__init__()\n",
    "        self.message_bits = message_bits\n",
    "        self.hidden_dim   = hidden_dim\n",
    "        \n",
    "        self.E = nn.Embedding(num_embeddings=(2**message_bits), embedding_dim=hidden_dim)\n",
    "\n",
    "        # ------------------- Encoder ------------------- #\n",
    "        self.init_conv = nn.Conv1d(in_channels, base_channels, kernel_size=7, stride=1, padding=3)\n",
    "\n",
    "        enc_blocks = []\n",
    "        ch = base_channels\n",
    "        for i, st in enumerate(strides):\n",
    "            out_ch = ch * 2\n",
    "            enc_blocks.append(ResidualBlock(ch, out_ch, stride=st))\n",
    "            ch = out_ch\n",
    "        self.encoder_blocks = nn.Sequential(*enc_blocks)\n",
    "\n",
    "        # Project encoder output to hidden_dim\n",
    "        self.proj = nn.Linear(ch, hidden_dim)  # ch is 512 here\n",
    "\n",
    "        # LSTM with input_size now matching hidden_dim\n",
    "        self.lstm = nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True, bidirectional=False)\n",
    "\n",
    "        self.final_conv_enc = nn.Conv1d(hidden_dim, output_channels, kernel_size=7, stride=1, padding=3)\n",
    "\n",
    "        # ------------------- Decoder ------------------- #\n",
    "\n",
    "        dec_blocks = []\n",
    "        rev_strides = list(reversed(strides))\n",
    "        in_ch = output_channels  # starting with 128\n",
    "        for st in rev_strides:\n",
    "            out_ch = in_ch // 2  # halving channels at each block\n",
    "            dec_blocks.append(nn.ConvTranspose1d(in_ch, out_ch, kernel_size=2*st, stride=st, padding=(st//2), output_padding=0))\n",
    "            dec_blocks.append(ResidualBlock(out_ch, out_ch, stride=1))\n",
    "            in_ch = out_ch  # update in_ch for next block\n",
    "        self.decoder_blocks = nn.Sequential(*dec_blocks)\n",
    "\n",
    "        # Change final_conv_dec to accept the correct number of channels from the decoder's output (in_ch)\n",
    "        self.final_conv_dec = nn.Conv1d(in_ch, 1, kernel_size=7, stride=1, padding=3)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, s, message=None):\n",
    "        B, _, T = s.shape\n",
    "        x = self.init_conv(s)\n",
    "        x = self.encoder_blocks(x)\n",
    "        x_t = x.transpose(1, 2)  # shape: (B, T_after, ch)\n",
    "\n",
    "        # Project to hidden_dim so we can add the message embedding\n",
    "        x_t = self.proj(x_t)  # shape: (B, T_after, hidden_dim)\n",
    "\n",
    "        if message is not None:\n",
    "            e = self.E(message)  # shape: (B, hidden_dim)\n",
    "            T_after = x_t.shape[1]\n",
    "            e_expanded = e.unsqueeze(1).expand(-1, T_after, -1)  # (B, T_after, hidden_dim)\n",
    "            x_t = x_t + e_expanded\n",
    "\n",
    "        lstm_out, _ = self.lstm(x_t)\n",
    "        lstm_out_t = lstm_out.transpose(1, 2)\n",
    "        latent = self.final_conv_enc(lstm_out_t)\n",
    "\n",
    "        x_dec = latent\n",
    "        x_dec = self.decoder_blocks(x_dec)\n",
    "        delta = self.final_conv_dec(x_dec)\n",
    "        if delta.shape[-1] != T:\n",
    "            min_len = min(delta.shape[-1], T)\n",
    "            delta = delta[:, :, :min_len]\n",
    "            if min_len < T:\n",
    "                pad_amt = T - min_len\n",
    "                delta = F.pad(delta, (0, pad_amt))\n",
    "        return delta\n",
    "\n",
    "\n",
    "print(\"\\nCHUNK #3 completed: Generator model with residual blocks, LSTM, and message embedding defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CHUNK #4 completed: Detector network (encoder + upsampling + classification) defined.\n"
     ]
    }
   ],
   "source": [
    "# ------------------- CHUNK 4: Detector Network ------------------- #\n",
    "class Detector(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels=1, \n",
    "                 base_channels=CHANNELS,\n",
    "                 hidden_dim=HIDDEN_DIM,\n",
    "                 message_bits=NUM_BITS,\n",
    "                 strides=STRIDES):\n",
    "        super().__init__()\n",
    "        self.message_bits = message_bits\n",
    "\n",
    "        self.init_conv = nn.Conv1d(in_channels, base_channels, kernel_size=7, stride=1, padding=3)\n",
    "\n",
    "        enc_blocks = []\n",
    "        ch = base_channels\n",
    "        for st in strides:\n",
    "            out_ch = ch * 2\n",
    "            enc_blocks.append(ResidualBlock(ch, out_ch, stride=st))\n",
    "            ch = out_ch\n",
    "        self.encoder_blocks = nn.Sequential(*enc_blocks)\n",
    "\n",
    "        dec_blocks = []\n",
    "        rev_strides = list(reversed(strides))\n",
    "        in_ch = ch\n",
    "        for st in rev_strides:\n",
    "            out_ch = in_ch // 2\n",
    "            dec_blocks.append(nn.ConvTranspose1d(in_ch, out_ch, kernel_size=2*st, stride=st, padding=(st//2), output_padding=0))\n",
    "            dec_blocks.append(ResidualBlock(out_ch, out_ch, stride=1))\n",
    "            in_ch = out_ch\n",
    "        self.upsample_blocks = nn.Sequential(*dec_blocks)\n",
    "\n",
    "        self.final_conv = nn.Conv1d(base_channels, 1 + message_bits, kernel_size=7, stride=1, padding=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Save original time dimension\n",
    "        original_length = x.shape[-1]\n",
    "\n",
    "        # Encoder\n",
    "        x = self.init_conv(x)\n",
    "        x = self.encoder_blocks(x)\n",
    "\n",
    "        # Upsample\n",
    "        x = self.upsample_blocks(x)\n",
    "        out = self.final_conv(x)\n",
    "\n",
    "        # Adjust output to match original time dimension\n",
    "        if out.shape[-1] > original_length:\n",
    "            out = out[:, :, :original_length]\n",
    "        elif out.shape[-1] < original_length:\n",
    "            pad_amt = original_length - out.shape[-1]\n",
    "            out = F.pad(out, (0, pad_amt))\n",
    "            \n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "\n",
    "print(\"\\nCHUNK #4 completed: Detector network (encoder + upsampling + classification) defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CHUNK #5 completed: stubs for multi-scale mel, adversarial, TF-loudness, localization, and decoding losses.\n"
     ]
    }
   ],
   "source": [
    "# ------------------- CHUNK 5: Loss Functions ------------------- #\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "# Define sample rate as a constant if not already defined\n",
    "SAMPLE_RATE = 16000  # Common value, adjust to match your actual sample rate\n",
    "\n",
    "class MultiScaleMelLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-scale mel spectrogram loss that compares spectrograms at different resolutions.\n",
    "    This helps capture both fine and coarse-grained audio details.\n",
    "    \"\"\"\n",
    "    def __init__(self, sample_rate=SAMPLE_RATE, n_ffts=[1024, 2048, 512], n_mels=80, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.mel_specs = nn.ModuleList([\n",
    "            T.MelSpectrogram(\n",
    "                sample_rate=sample_rate,\n",
    "                n_fft=n_fft,\n",
    "                hop_length=n_fft // 4,\n",
    "                n_mels=n_mels,\n",
    "                normalized=True\n",
    "            ) for n_fft in n_ffts\n",
    "        ])\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward(self, original, watermarked):\n",
    "        # original & watermarked: shape (B,1,T)\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        # Compute loss at each scale\n",
    "        for mel_spec in self.mel_specs:\n",
    "            # Log mel spectrograms\n",
    "            mel_orig = torch.log(mel_spec(original) + 1e-5)\n",
    "            mel_wm = torch.log(mel_spec(watermarked) + 1e-5)\n",
    "            \n",
    "            # Compute L1 and L2 losses\n",
    "            l1_loss = F.l1_loss(mel_wm, mel_orig)\n",
    "            l2_loss = F.mse_loss(mel_wm, mel_orig)\n",
    "            \n",
    "            # Combine losses\n",
    "            total_loss += l1_loss + self.alpha * l2_loss\n",
    "            \n",
    "        return total_loss / len(self.mel_specs)\n",
    "\n",
    "\n",
    "class AdversarialLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Adversarial loss using a discriminator network that tries to distinguish between\n",
    "    original and watermarked audio.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define a simple discriminator network\n",
    "        self.discriminator = nn.Sequential(\n",
    "            # Input: (B, 1, T)\n",
    "            nn.Conv1d(1, 16, kernel_size=15, stride=1, padding=7),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(16, 32, kernel_size=41, stride=4, padding=20),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(32, 64, kernel_size=41, stride=4, padding=20),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(64, 128, kernel_size=41, stride=4, padding=20),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(128, 1, kernel_size=41, stride=4, padding=20),\n",
    "        )\n",
    "        self.optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=0.0001)\n",
    "        \n",
    "    def forward(self, original, watermarked, train_disc=True):\n",
    "        # Train discriminator\n",
    "        if train_disc:\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Real samples should be classified as 1\n",
    "            real_output = self.discriminator(original)\n",
    "            real_loss = F.binary_cross_entropy_with_logits(\n",
    "                real_output, \n",
    "                torch.ones_like(real_output)\n",
    "            )\n",
    "            \n",
    "            # Watermarked samples should be classified as 0\n",
    "            fake_output = self.discriminator(watermarked.detach())\n",
    "            fake_loss = F.binary_cross_entropy_with_logits(\n",
    "                fake_output, \n",
    "                torch.zeros_like(fake_output)\n",
    "            )\n",
    "            \n",
    "            # Total discriminator loss\n",
    "            disc_loss = real_loss + fake_loss\n",
    "            disc_loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        # Generator loss (trick discriminator)\n",
    "        fake_output = self.discriminator(watermarked)\n",
    "        gen_loss = F.binary_cross_entropy_with_logits(\n",
    "            fake_output, \n",
    "            torch.ones_like(fake_output)\n",
    "        )\n",
    "        \n",
    "        return gen_loss\n",
    "\n",
    "\n",
    "class TFLoudnessLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Time-Frequency loudness loss that ensures perceptual similarity across\n",
    "    different frequency bands and time windows.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_bands=8, window_size=2048, hop_size=512):\n",
    "        super().__init__()\n",
    "        self.n_bands = n_bands\n",
    "        self.win_size = window_size\n",
    "        self.hop_size = hop_size\n",
    "        \n",
    "        # Perceptual weighting (approximating equal-loudness contours)\n",
    "        # Higher weights for mid-frequency bands where human hearing is most sensitive\n",
    "        weights = torch.ones(n_bands)\n",
    "        mid_band_idx = n_bands // 3\n",
    "        weights[mid_band_idx:2*mid_band_idx] = 1.5\n",
    "        self.register_buffer('band_weights', weights)\n",
    "\n",
    "    def forward(self, original, watermarked):\n",
    "        # Create Hanning window\n",
    "        window = torch.hann_window(self.win_size, device=original.device)\n",
    "        \n",
    "        # Compute STFTs\n",
    "        stft_orig = torch.stft(\n",
    "            original.squeeze(1), \n",
    "            n_fft=self.win_size, \n",
    "            hop_length=self.hop_size, \n",
    "            window=window, \n",
    "            return_complex=True,\n",
    "            normalized=True\n",
    "        )\n",
    "        stft_wm = torch.stft(\n",
    "            watermarked.squeeze(1), \n",
    "            n_fft=self.win_size, \n",
    "            hop_length=self.hop_size, \n",
    "            window=window, \n",
    "            return_complex=True,\n",
    "            normalized=True\n",
    "        )\n",
    "\n",
    "        # Compute magnitude spectrograms\n",
    "        mag_orig = stft_orig.abs()  # (B, freq, frames)\n",
    "        mag_wm = stft_wm.abs()\n",
    "        \n",
    "        # Compute phase spectrograms\n",
    "        phase_orig = stft_orig.angle()\n",
    "        phase_wm = stft_wm.angle()\n",
    "\n",
    "        # Divide frequency dimension into bands\n",
    "        freq_bins = mag_orig.shape[1]\n",
    "        band_size = freq_bins // self.n_bands\n",
    "\n",
    "        # Initialize losses\n",
    "        loudness_loss = 0.0\n",
    "        spectral_loss = 0.0\n",
    "        phase_loss = 0.0\n",
    "        \n",
    "        # Compute loss per band\n",
    "        for b in range(self.n_bands):\n",
    "            start = b * band_size\n",
    "            end = freq_bins if (b == self.n_bands-1) else (start + band_size)\n",
    "\n",
    "            # Extract band\n",
    "            band_orig = mag_orig[:, start:end, :]\n",
    "            band_wm = mag_wm[:, start:end, :]\n",
    "            \n",
    "            # Phase for this band\n",
    "            phase_band_orig = phase_orig[:, start:end, :]\n",
    "            phase_band_wm = phase_wm[:, start:end, :]\n",
    "            \n",
    "            # Compute band energy (loudness)\n",
    "            energy_orig = torch.sum(band_orig**2, dim=1)  # Sum over freq, shape (B, frames)\n",
    "            energy_wm = torch.sum(band_wm**2, dim=1)\n",
    "            \n",
    "            # Log-scale energy with stabilizing epsilon\n",
    "            loud_orig = torch.log10(energy_orig + 1e-8)\n",
    "            loud_wm = torch.log10(energy_wm + 1e-8)\n",
    "            \n",
    "            # Loudness difference (L1)\n",
    "            band_loudness_diff = F.l1_loss(loud_wm, loud_orig)\n",
    "            loudness_loss += self.band_weights[b] * band_loudness_diff\n",
    "            \n",
    "            # Spectral shape difference (L2)\n",
    "            band_spectral_diff = F.mse_loss(band_wm, band_orig)\n",
    "            spectral_loss += self.band_weights[b] * band_spectral_diff\n",
    "            \n",
    "            # Phase difference (important for transients)\n",
    "            # We use circular distance for phase\n",
    "            phase_diff = 1.0 - torch.cos(phase_band_wm - phase_band_orig)\n",
    "            phase_loss += self.band_weights[b] * phase_diff.mean()\n",
    "\n",
    "        # Normalize by number of bands\n",
    "        loudness_loss /= self.n_bands\n",
    "        spectral_loss /= self.n_bands\n",
    "        phase_loss /= self.n_bands\n",
    "        \n",
    "        # Weight the different components\n",
    "        # Phase is less important for many watermarking applications\n",
    "        return loudness_loss + spectral_loss + 0.2 * phase_loss\n",
    "\n",
    "\n",
    "def masked_localization_loss(detector_out, mask, smooth_eps=0.1):\n",
    "    \"\"\"\n",
    "    Localization loss with label smoothing and focal weighting to improve \n",
    "    detector performance.\n",
    "    \n",
    "    Args:\n",
    "        detector_out: shape (B, 1+b, T). The first channel is detection => (B,1,T).\n",
    "        mask: shape (B,1,T), 1=watermarked region, 0=original\n",
    "        smooth_eps: Label smoothing epsilon\n",
    "    \"\"\"\n",
    "    det_prob = detector_out[:, 0:1, :]  # shape (B,1,T)\n",
    "    \n",
    "    # Apply label smoothing\n",
    "    smoothed_mask = mask * (1.0 - smooth_eps) + (1.0 - mask) * smooth_eps\n",
    "    \n",
    "    # Focal loss weighting\n",
    "    # Reduce weight for easy examples, focus on hard examples\n",
    "    pt = torch.where(mask > 0.5, det_prob, 1 - det_prob)\n",
    "    focal_weight = (1 - pt) ** 2\n",
    "    \n",
    "    # BCE loss\n",
    "    bce_loss = F.binary_cross_entropy(det_prob, smoothed_mask, reduction='none')\n",
    "    focal_loss = focal_weight * bce_loss\n",
    "    \n",
    "    # Average loss\n",
    "    avg_loss = focal_loss.mean()\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def decoding_loss(detector_out, message, mask=None, gamma=2.0):\n",
    "    \"\"\"\n",
    "    Improved decoding loss that focuses on watermarked regions and uses\n",
    "    a more sophisticated bit extraction method.\n",
    "    \n",
    "    Args:\n",
    "        detector_out: shape (B, 1+b, T)\n",
    "        message: shape (B,) or (B,b) containing integers in [0..2^b-1]\n",
    "        mask: shape (B,1,T), 1=watermarked region, 0=original\n",
    "        gamma: Focal loss parameter\n",
    "    \"\"\"\n",
    "    B, channels, T = detector_out.shape\n",
    "    b = channels - 1\n",
    "    if b <= 0:\n",
    "        return torch.tensor(0.0, device=detector_out.device)\n",
    "\n",
    "    # Extract bit probability maps\n",
    "    bit_prob_map = detector_out[:, 1:, :]  # shape (B,b,T)\n",
    "    \n",
    "    # If mask is provided, use it to focus on watermarked regions\n",
    "    if mask is not None:\n",
    "        # Expand mask to match bit channels\n",
    "        expanded_mask = mask.expand(-1, b, -1)  # (B,b,T)\n",
    "        \n",
    "        # Apply mask: zero out non-watermarked regions\n",
    "        masked_bit_map = bit_prob_map * expanded_mask\n",
    "        \n",
    "        # Calculate weighted average over time dimension\n",
    "        # This gives more weight to regions that are strongly watermarked\n",
    "        weights = expanded_mask.sum(dim=2, keepdim=True) + 1e-8\n",
    "        bit_prob = (masked_bit_map.sum(dim=2) / weights.squeeze(2))\n",
    "    else:\n",
    "        # If no mask, use attention-like mechanism to focus on relevant time steps\n",
    "        # Look for time steps with highest confidence\n",
    "        confidence = torch.abs(bit_prob_map - 0.5) * 2  # Scale to [0,1]\n",
    "        attention = F.softmax(confidence * 5.0, dim=2)  # Sharpen and normalize\n",
    "        bit_prob = (bit_prob_map * attention).sum(dim=2)  # Weighted average\n",
    "\n",
    "    # Convert message integers to binary bits\n",
    "    msg_bits = []\n",
    "    for i in range(b):\n",
    "        bit_i = ((message >> i) & 1).float()\n",
    "        msg_bits.append(bit_i)\n",
    "    msg_bits = torch.stack(msg_bits, dim=1)  # shape (B,b)\n",
    "    \n",
    "    # Binary cross entropy with focal loss weighting\n",
    "    pt = torch.where(msg_bits > 0.5, bit_prob, 1 - bit_prob)\n",
    "    focal_weight = (1 - pt) ** gamma\n",
    "    \n",
    "    bce = F.binary_cross_entropy(bit_prob, msg_bits, reduction='none')\n",
    "    focal_bce = focal_weight * bce\n",
    "    \n",
    "    return focal_bce.mean()\n",
    "\n",
    "print(\"\\nCHUNK #5 completed: stubs for multi-scale mel, adversarial, TF-loudness, localization, and decoding losses.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CHUNK #6 completed: training functions defined (multi-objective).\n"
     ]
    }
   ],
   "source": [
    "# ------------------- CHUNK 6: Training Loop ------------------- #\n",
    "\n",
    "def generate_random_messages(batch_size, max_val=(2**NUM_BITS)):\n",
    "    # random integer in [0, 2^b -1]\n",
    "    return torch.randint(0, max_val, (batch_size,))\n",
    "\n",
    "# We'll define a combined function for one training step\n",
    "def train_one_epoch(\n",
    "    generator,\n",
    "    detector,\n",
    "    train_loader,\n",
    "    optimizer,\n",
    "    epoch,\n",
    "    total_epochs,\n",
    "    device\n",
    "):\n",
    "    generator.train()\n",
    "    detector.train()\n",
    "    \n",
    "    # Instantiate the various loss modules\n",
    "    ms_mel_loss   = MultiScaleMelLoss().to(device)\n",
    "    adv_loss_stub = AdversarialLoss().to(device)\n",
    "    tf_loud_loss  = TFLoudnessLoss().to(device)\n",
    "\n",
    "    total_steps = len(train_loader)\n",
    "    pbar = tqdm(enumerate(train_loader), total=total_steps, desc=f\"Epoch [{epoch}/{total_epochs}]\")\n",
    "    \n",
    "    sum_loss = 0.0\n",
    "    for i, s in pbar:\n",
    "        s = s.to(device)  # shape (B,1,16000)\n",
    "\n",
    "        B = s.shape[0]\n",
    "        # generate random messages\n",
    "        msgs = generate_random_messages(B).to(device)  # shape (B,)\n",
    "\n",
    "        # forward pass\n",
    "        delta = generator(s, msgs)\n",
    "        s_w   = s + delta  # watermarked audio\n",
    "\n",
    "        # optional watermark masking augmentation\n",
    "        # for demonstration, do it in a loop\n",
    "        for b_idx in range(B):\n",
    "            s_w[b_idx] = watermark_masking_augmentation(s_w[b_idx])\n",
    "            s_w[b_idx] = robustness_augmentations(s_w[b_idx])\n",
    "\n",
    "        # detection\n",
    "        det_out = detector(s_w)\n",
    "\n",
    "        # 1) L1 on watermark\n",
    "        loss_l1 = F.l1_loss(delta, torch.zeros_like(delta))  # or you can do F.l1_loss(s_w, s)\n",
    "\n",
    "        # 2) multi-scale mel\n",
    "        loss_msspec = ms_mel_loss(s, s_w)\n",
    "\n",
    "        # 3) adversarial stub\n",
    "        loss_adv = adv_loss_stub(s, s_w)\n",
    "\n",
    "        # 4) tf-loudness\n",
    "        loss_loud = tf_loud_loss(s, s_w)\n",
    "\n",
    "        # 5) localization => we build a mask=1 for the entire clip if it's watermarked\n",
    "        # In real partial scenario, you'd know which samples are watermarked\n",
    "        mask = torch.ones((B,1,s.shape[-1]), device=device)\n",
    "        loss_loc = masked_localization_loss(det_out, mask)\n",
    "\n",
    "        # 6) decoding => ensure the b channels decode the same bits we embedded\n",
    "        loss_dec = decoding_loss(det_out, msgs)\n",
    "\n",
    "        # Weighted sum\n",
    "        loss = (lambda_L1     * loss_l1\n",
    "              + lambda_msspec * loss_msspec\n",
    "              + lambda_adv    * loss_adv\n",
    "              + lambda_loud   * loss_loud\n",
    "              + lambda_loc    * loss_loc\n",
    "              + lambda_dec    * loss_dec)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        sum_loss += loss.item()\n",
    "        pbar.set_postfix({\n",
    "            \"loss_l1\": f\"{loss_l1.item():.3f}\",\n",
    "            \"loss_loc\": f\"{loss_loc.item():.3f}\",\n",
    "            \"loss_dec\": f\"{loss_dec.item():.3f}\",\n",
    "            \"total\": f\"{loss.item():.3f}\"\n",
    "        })\n",
    "    avg_loss = sum_loss / (total_steps if total_steps>0 else 1)\n",
    "    return avg_loss\n",
    "\n",
    "def validate_one_epoch(\n",
    "    generator,\n",
    "    detector,\n",
    "    val_loader,\n",
    "    device\n",
    "):\n",
    "    generator.eval()\n",
    "    detector.eval()\n",
    "\n",
    "    ms_mel_loss   = MultiScaleMelLoss().to(device)\n",
    "    adv_loss_stub = AdversarialLoss().to(device)\n",
    "    tf_loud_loss  = TFLoudnessLoss().to(device)\n",
    "\n",
    "    sum_loss = 0.0\n",
    "    steps = 0\n",
    "    with torch.no_grad():\n",
    "        for s in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "            s = s.to(device)\n",
    "            B = s.shape[0]\n",
    "            msgs = generate_random_messages(B).to(device)\n",
    "\n",
    "            delta = generator(s, msgs)\n",
    "            s_w   = s + delta\n",
    "            det_out = detector(s_w)\n",
    "\n",
    "            # same losses\n",
    "            loss_l1   = F.l1_loss(delta, torch.zeros_like(delta))\n",
    "            loss_msspec = ms_mel_loss(s, s_w)\n",
    "            loss_adv = adv_loss_stub(s, s_w)\n",
    "            loss_loud = tf_loud_loss(s, s_w)\n",
    "            mask      = torch.ones((B,1,s.shape[-1]), device=device)\n",
    "            loss_loc  = masked_localization_loss(det_out, mask)\n",
    "            loss_dec  = decoding_loss(det_out, msgs)\n",
    "\n",
    "            loss = (lambda_L1     * loss_l1\n",
    "                  + lambda_msspec * loss_msspec\n",
    "                  + lambda_adv    * loss_adv\n",
    "                  + lambda_loud   * loss_loud\n",
    "                  + lambda_loc    * loss_loc\n",
    "                  + lambda_dec    * loss_dec)\n",
    "\n",
    "            sum_loss += loss.item()\n",
    "            steps += 1\n",
    "    return sum_loss / (steps if steps>0 else 1)\n",
    "\n",
    "def train_model(\n",
    "    generator,\n",
    "    detector,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    num_epochs=10,\n",
    "    lr=LR\n",
    "):\n",
    "    # Dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        list(generator.parameters()) + list(detector.parameters()),\n",
    "        lr=lr\n",
    "    )\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        train_loss = train_one_epoch(generator, detector, train_loader, optimizer, epoch, num_epochs, device)\n",
    "        val_loss   = validate_one_epoch(generator, detector, val_loader, device)\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}]  TRAIN Loss: {train_loss:.4f}  |  VAL Loss: {val_loss:.4f}\")\n",
    "\n",
    "print(\"\\nCHUNK #6 completed: training functions defined (multi-objective).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CHUNK #7 completed: Inference (detection, localization, decode) & placeholder evaluation done.\n"
     ]
    }
   ],
   "source": [
    "# ------------------- CHUNK 7: Inference & Evaluation ------------------- #\n",
    "\n",
    "def detect_watermark(detector, audio, threshold=0.5):\n",
    "    \"\"\"\n",
    "    audio: shape (B,1,T)\n",
    "    Returns boolean indicating if watermark is present (avg>threshold)\n",
    "    \"\"\"\n",
    "    detector.eval()\n",
    "    with torch.no_grad():\n",
    "        out = detector(audio)  # shape (B, 1+b, T)\n",
    "        det_prob = out[:, 0, :]  # (B, T)\n",
    "        avg_prob = det_prob.mean(dim=1)  # (B,)\n",
    "        return (avg_prob > threshold).float()\n",
    "\n",
    "def localize_watermark(detector, audio, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Return a mask of shape (B,T) indicating where watermark is detected\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        out = detector(audio)  # shape (B, 1+b, T)\n",
    "        det_prob = out[:, 0, :]  # (B, T)\n",
    "        return (det_prob > threshold).float()\n",
    "\n",
    "def decode_message(detector, audio):\n",
    "    \"\"\"\n",
    "    audio: shape (B,1,T)\n",
    "    We average bit predictions across time -> get (B,b).\n",
    "    Convert to 0/1 by threshold=0.5\n",
    "    Return integer or bit array\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        out = detector(audio)\n",
    "        bit_prob_map = out[:, 1:, :]  # shape (B,b,T)\n",
    "        bit_prob = bit_prob_map.mean(dim=2)  # (B,b)\n",
    "        bits_decoded = (bit_prob>0.5).int()  # shape (B,b)\n",
    "        # convert to integer\n",
    "        # bits_decoded[:,0] => least significant or whichever order you prefer\n",
    "        B, b = bits_decoded.shape\n",
    "        msg_int = torch.zeros(B, dtype=torch.long, device=bits_decoded.device)\n",
    "        for i in range(b):\n",
    "            msg_int |= (bits_decoded[:, i] << i)\n",
    "        return msg_int\n",
    "\n",
    "# Placeholders for objective metrics\n",
    "def evaluate_si_snr(original, reconstructed):\n",
    "    # Real SI-SNR calculation is more involved. We'll do a placeholder\n",
    "    return random.random()\n",
    "\n",
    "def evaluate_pesq(original, reconstructed, sr=SAMPLE_RATE):\n",
    "    # Typically use an external library (pesq package). We'll do a placeholder\n",
    "    return 4.5 - random.random()\n",
    "\n",
    "def run_evaluation(generator, detector, test_dataset):\n",
    "    print(\"Running final evaluation on test dataset (placeholder).\")\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    si_snr_list  = []\n",
    "    pesq_list    = []\n",
    "    \n",
    "    generator.eval()\n",
    "    detector.eval()\n",
    "    with torch.no_grad():\n",
    "        for s in test_loader:\n",
    "            s = s.to(device)\n",
    "            B = s.shape[0]\n",
    "            msgs = generate_random_messages(B).to(device)\n",
    "            delta = generator(s, msgs)\n",
    "            s_w   = s + delta\n",
    "\n",
    "            # Evaluate SI-SNR, PESQ, etc.\n",
    "            for i in range(B):\n",
    "                si_snr_list.append(evaluate_si_snr(s[i], s_w[i]))\n",
    "                pesq_list.append(evaluate_pesq(s[i], s_w[i]))\n",
    "\n",
    "    avg_si_snr = sum(si_snr_list)/len(si_snr_list) if si_snr_list else 0.0\n",
    "    avg_pesq   = sum(pesq_list)/len(pesq_list)     if pesq_list else 0.0\n",
    "    print(f\"Average SI-SNR: {avg_si_snr:.3f}\")\n",
    "    print(f\"Average PESQ:   {avg_pesq:.3f}\")\n",
    "\n",
    "print(\"\\nCHUNK #7 completed: Inference (detection, localization, decode) & placeholder evaluation done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Loss: 2.2188 - LR: 0.000100\n",
      "Epoch 2/100 - Loss: 1.6791 - LR: 0.000100\n",
      "Epoch 3/100 - Loss: 1.4266 - LR: 0.000100\n",
      "Epoch 4/100 - Loss: 1.2096 - LR: 0.000100\n",
      "Epoch 5/100 - Loss: 1.0645 - LR: 0.000100\n",
      "Epoch 6/100 - Loss: 0.9874 - LR: 0.000100\n",
      "Epoch 7/100 - Loss: 0.9197 - LR: 0.000100\n",
      "Epoch 8/100 - Loss: 0.8505 - LR: 0.000100\n",
      "Epoch 9/100 - Loss: 0.7706 - LR: 0.000100\n",
      "Epoch 10/100 - Loss: 0.7296 - LR: 0.000050\n",
      "Epoch 11/100 - Loss: 0.6766 - LR: 0.000050\n",
      "Epoch 12/100 - Loss: 0.6244 - LR: 0.000050\n",
      "Epoch 13/100 - Loss: 0.5213 - LR: 0.000050\n",
      "Epoch 14/100 - Loss: 0.5402 - LR: 0.000050\n",
      "Epoch 15/100 - Loss: 0.5312 - LR: 0.000050\n",
      "Epoch 16/100 - Loss: 0.6248 - LR: 0.000050\n",
      "Epoch 17/100 - Loss: 0.6335 - LR: 0.000050\n",
      "Epoch 18/100 - Loss: 0.6383 - LR: 0.000050\n",
      "Epoch 19/100 - Loss: 0.6269 - LR: 0.000050\n",
      "Epoch 20/100 - Loss: 0.6169 - LR: 0.000025\n",
      "Epoch 21/100 - Loss: 0.6049 - LR: 0.000025\n",
      "Epoch 22/100 - Loss: 0.5926 - LR: 0.000025\n",
      "Epoch 23/100 - Loss: 0.5817 - LR: 0.000025\n",
      "Epoch 24/100 - Loss: 0.5645 - LR: 0.000025\n",
      "Epoch 25/100 - Loss: 0.5353 - LR: 0.000025\n",
      "Epoch 26/100 - Loss: 0.4649 - LR: 0.000025\n",
      "Epoch 27/100 - Loss: 0.4939 - LR: 0.000025\n",
      "Epoch 28/100 - Loss: 0.5226 - LR: 0.000025\n",
      "Epoch 29/100 - Loss: 0.4894 - LR: 0.000025\n",
      "Epoch 30/100 - Loss: 0.4696 - LR: 0.000013\n",
      "Epoch 31/100 - Loss: 0.5370 - LR: 0.000013\n",
      "Epoch 32/100 - Loss: 0.5356 - LR: 0.000013\n",
      "Epoch 33/100 - Loss: 0.5239 - LR: 0.000013\n",
      "Epoch 34/100 - Loss: 0.5015 - LR: 0.000013\n",
      "Epoch 35/100 - Loss: 0.4554 - LR: 0.000013\n",
      "Epoch 36/100 - Loss: 0.4370 - LR: 0.000013\n",
      "Epoch 37/100 - Loss: 0.4596 - LR: 0.000013\n",
      "Epoch 38/100 - Loss: 0.4219 - LR: 0.000013\n",
      "Epoch 39/100 - Loss: 0.4449 - LR: 0.000013\n",
      "Epoch 40/100 - Loss: 0.3958 - LR: 0.000006\n",
      "Epoch 41/100 - Loss: 0.3978 - LR: 0.000006\n",
      "Epoch 42/100 - Loss: 0.4203 - LR: 0.000006\n",
      "Epoch 43/100 - Loss: 0.3947 - LR: 0.000006\n",
      "Epoch 44/100 - Loss: 0.4435 - LR: 0.000006\n",
      "Epoch 45/100 - Loss: 0.4443 - LR: 0.000006\n",
      "Epoch 46/100 - Loss: 0.3938 - LR: 0.000006\n",
      "Epoch 47/100 - Loss: 0.3682 - LR: 0.000006\n",
      "Epoch 48/100 - Loss: 0.3651 - LR: 0.000006\n",
      "Epoch 49/100 - Loss: 0.3826 - LR: 0.000006\n",
      "Epoch 50/100 - Loss: 0.3572 - LR: 0.000003\n",
      "Epoch 51/100 - Loss: 0.3485 - LR: 0.000003\n",
      "Epoch 52/100 - Loss: 0.3656 - LR: 0.000003\n",
      "Epoch 53/100 - Loss: 0.3528 - LR: 0.000003\n",
      "Epoch 54/100 - Loss: 0.3546 - LR: 0.000003\n",
      "Epoch 55/100 - Loss: 0.3313 - LR: 0.000003\n",
      "Epoch 56/100 - Loss: 0.3205 - LR: 0.000003\n",
      "Epoch 57/100 - Loss: 0.3383 - LR: 0.000003\n",
      "Epoch 58/100 - Loss: 0.3317 - LR: 0.000003\n",
      "Epoch 59/100 - Loss: 0.3358 - LR: 0.000003\n",
      "Epoch 60/100 - Loss: 0.3271 - LR: 0.000002\n",
      "Epoch 61/100 - Loss: 0.3166 - LR: 0.000002\n",
      "Epoch 62/100 - Loss: 0.3116 - LR: 0.000002\n",
      "Epoch 63/100 - Loss: 0.3210 - LR: 0.000002\n",
      "Epoch 64/100 - Loss: 0.3126 - LR: 0.000002\n",
      "Epoch 65/100 - Loss: 0.3140 - LR: 0.000002\n",
      "Epoch 66/100 - Loss: 0.3138 - LR: 0.000002\n",
      "Epoch 67/100 - Loss: 0.3213 - LR: 0.000002\n",
      "Epoch 68/100 - Loss: 0.3249 - LR: 0.000002\n",
      "Epoch 69/100 - Loss: 0.3219 - LR: 0.000002\n",
      "Epoch 70/100 - Loss: 0.3251 - LR: 0.000001\n",
      "Epoch 71/100 - Loss: 0.3163 - LR: 0.000001\n",
      "Epoch 72/100 - Loss: 0.3116 - LR: 0.000001\n",
      "Epoch 73/100 - Loss: 0.3125 - LR: 0.000001\n",
      "Epoch 74/100 - Loss: 0.3146 - LR: 0.000001\n",
      "Epoch 75/100 - Loss: 0.3129 - LR: 0.000001\n",
      "Epoch 76/100 - Loss: 0.3111 - LR: 0.000001\n",
      "Epoch 77/100 - Loss: 0.3101 - LR: 0.000001\n",
      "Epoch 78/100 - Loss: 0.3027 - LR: 0.000001\n",
      "Epoch 79/100 - Loss: 0.3024 - LR: 0.000001\n",
      "Epoch 80/100 - Loss: 0.3021 - LR: 0.000000\n",
      "Epoch 81/100 - Loss: 0.3019 - LR: 0.000000\n",
      "Epoch 82/100 - Loss: 0.3021 - LR: 0.000000\n",
      "Epoch 83/100 - Loss: 0.3023 - LR: 0.000000\n",
      "Epoch 84/100 - Loss: 0.3021 - LR: 0.000000\n",
      "Epoch 85/100 - Loss: 0.3022 - LR: 0.000000\n",
      "Epoch 86/100 - Loss: 0.3022 - LR: 0.000000\n",
      "Epoch 87/100 - Loss: 0.3022 - LR: 0.000000\n",
      "Epoch 88/100 - Loss: 0.3021 - LR: 0.000000\n",
      "Epoch 89/100 - Loss: 0.3019 - LR: 0.000000\n",
      "Epoch 90/100 - Loss: 0.3018 - LR: 0.000000\n",
      "Epoch 91/100 - Loss: 0.3017 - LR: 0.000000\n",
      "Epoch 92/100 - Loss: 0.3017 - LR: 0.000000\n",
      "Epoch 93/100 - Loss: 0.3016 - LR: 0.000000\n",
      "Epoch 94/100 - Loss: 0.3016 - LR: 0.000000\n",
      "Epoch 95/100 - Loss: 0.3017 - LR: 0.000000\n",
      "Epoch 96/100 - Loss: 0.3017 - LR: 0.000000\n",
      "Epoch 97/100 - Loss: 0.3017 - LR: 0.000000\n",
      "Epoch 98/100 - Loss: 0.3015 - LR: 0.000000\n",
      "Epoch 99/100 - Loss: 0.3016 - LR: 0.000000\n",
      "Epoch 100/100 - Loss: 0.3018 - LR: 0.000000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Assumed to be defined elsewhere:\n",
    "# - set_seed(seed)\n",
    "# - OneSecClipsDataset(root_dir, sample_rate)\n",
    "# - Generator(), Detector()\n",
    "# - generate_random_messages(batch_size)\n",
    "# - device, LR\n",
    "\n",
    "# For reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# --- Simple Single-Scale Mel Loss ---\n",
    "class SimpleMelLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple mel spectrogram loss that uses a single scale.\n",
    "    \"\"\"\n",
    "    def __init__(self, sample_rate=16000, n_fft=1024, n_mels=80):\n",
    "        super(SimpleMelLoss, self).__init__()\n",
    "        self.mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=n_fft // 4,\n",
    "            n_mels=n_mels,\n",
    "            normalized=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, original, watermarked):\n",
    "        # original and watermarked: shape (B, 1, T)\n",
    "        mel_orig = torch.log(self.mel_spec(original) + 1e-5)\n",
    "        mel_wm   = torch.log(self.mel_spec(watermarked) + 1e-5)\n",
    "        loss = F.l1_loss(mel_orig, mel_wm)\n",
    "        return loss\n",
    "\n",
    "# --- TF-Loudness Loss ---\n",
    "class TFLoudnessLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Time-Frequency loudness loss that ensures perceptual similarity across\n",
    "    different frequency bands and time windows.\n",
    "    \n",
    "    It computes the STFT of the original and watermarked audio and compares their\n",
    "    loudness, spectral shape, and phase differences.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_bands=8, window_size=2048, hop_size=512):\n",
    "        super(TFLoudnessLoss, self).__init__()\n",
    "        self.n_bands = n_bands\n",
    "        self.win_size = window_size\n",
    "        self.hop_size = hop_size\n",
    "        \n",
    "        # Perceptual weighting: higher weights for mid-frequency bands\n",
    "        weights = torch.ones(n_bands)\n",
    "        mid_band_idx = n_bands // 3\n",
    "        weights[mid_band_idx:2 * mid_band_idx] = 1.5\n",
    "        self.register_buffer('band_weights', weights)\n",
    "\n",
    "    def forward(self, original, watermarked):\n",
    "        window = torch.hann_window(self.win_size, device=original.device)\n",
    "        stft_orig = torch.stft(\n",
    "            original.squeeze(1), n_fft=self.win_size, hop_length=self.hop_size,\n",
    "            window=window, return_complex=True, normalized=True\n",
    "        )\n",
    "        stft_wm = torch.stft(\n",
    "            watermarked.squeeze(1), n_fft=self.win_size, hop_length=self.hop_size,\n",
    "            window=window, return_complex=True, normalized=True\n",
    "        )\n",
    "        mag_orig = stft_orig.abs()\n",
    "        mag_wm   = stft_wm.abs()\n",
    "        phase_orig = stft_orig.angle()\n",
    "        phase_wm   = stft_wm.angle()\n",
    "        \n",
    "        freq_bins = mag_orig.shape[1]\n",
    "        band_size = freq_bins // self.n_bands\n",
    "        \n",
    "        loudness_loss = 0.0\n",
    "        spectral_loss = 0.0\n",
    "        phase_loss    = 0.0\n",
    "        \n",
    "        for b in range(self.n_bands):\n",
    "            start = b * band_size\n",
    "            end = freq_bins if (b == self.n_bands - 1) else (start + band_size)\n",
    "            band_orig = mag_orig[:, start:end, :]\n",
    "            band_wm   = mag_wm[:, start:end, :]\n",
    "            \n",
    "            energy_orig = torch.sum(band_orig ** 2, dim=1)\n",
    "            energy_wm   = torch.sum(band_wm ** 2, dim=1)\n",
    "            loud_orig   = torch.log10(energy_orig + 1e-8)\n",
    "            loud_wm     = torch.log10(energy_wm + 1e-8)\n",
    "            loudness_loss += self.band_weights[b] * F.l1_loss(loud_wm, loud_orig)\n",
    "            \n",
    "            spectral_loss += self.band_weights[b] * F.mse_loss(band_wm, band_orig)\n",
    "            \n",
    "            phase_diff = 1.0 - torch.cos(phase_wm[:, start:end, :] - phase_orig[:, start:end, :])\n",
    "            phase_loss += self.band_weights[b] * phase_diff.mean()\n",
    "        \n",
    "        loudness_loss /= self.n_bands\n",
    "        spectral_loss /= self.n_bands\n",
    "        phase_loss    /= self.n_bands\n",
    "        \n",
    "        return loudness_loss + spectral_loss + 0.2 * phase_loss\n",
    "\n",
    "# --- Adversarial Loss ---\n",
    "class AdversarialLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Adversarial loss using a discriminator network that distinguishes between\n",
    "    original and watermarked audio.\n",
    "    \n",
    "    The generator is penalized if the discriminator can tell the difference,\n",
    "    forcing it to produce watermarked audio that is as realistic as the original.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(AdversarialLoss, self).__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=15, stride=1, padding=7),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(16, 32, kernel_size=41, stride=4, padding=20),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(32, 64, kernel_size=41, stride=4, padding=20),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(64, 128, kernel_size=41, stride=4, padding=20),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(128, 1, kernel_size=41, stride=4, padding=20),\n",
    "        )\n",
    "        self.disc_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=1e-4)\n",
    "\n",
    "    def forward(self, original, watermarked, train_disc=True):\n",
    "        if train_disc:\n",
    "            self.disc_optimizer.zero_grad()\n",
    "            real_output = self.discriminator(original)\n",
    "            real_loss = F.binary_cross_entropy_with_logits(real_output, torch.ones_like(real_output))\n",
    "            fake_output = self.discriminator(watermarked.detach())\n",
    "            fake_loss = F.binary_cross_entropy_with_logits(fake_output, torch.zeros_like(fake_output))\n",
    "            disc_loss = real_loss + fake_loss\n",
    "            disc_loss.backward()\n",
    "            self.disc_optimizer.step()\n",
    "        fake_output = self.discriminator(watermarked)\n",
    "        gen_loss = F.binary_cross_entropy_with_logits(fake_output, torch.ones_like(fake_output))\n",
    "        return gen_loss\n",
    "\n",
    "# --- Masked Localization Loss ---\n",
    "def masked_localization_loss(detector_out, mask, smooth_eps=0.1):\n",
    "    \"\"\"\n",
    "    Localization loss with label smoothing and focal loss weighting.\n",
    "    \n",
    "    It uses the detector's first output channel (detection probability per sample)\n",
    "    and compares it to a mask indicating where the watermark is present.\n",
    "    \"\"\"\n",
    "    det_prob = detector_out[:, 0:1, :]  # detection channel\n",
    "    smoothed_mask = mask * (1.0 - smooth_eps) + (1.0 - mask) * smooth_eps\n",
    "    pt = torch.where(mask > 0.5, det_prob, 1 - det_prob)\n",
    "    focal_weight = (1 - pt) ** 2\n",
    "    bce_loss = F.binary_cross_entropy(det_prob, smoothed_mask, reduction='none')\n",
    "    focal_loss = focal_weight * bce_loss\n",
    "    return focal_loss.mean()\n",
    "\n",
    "# --- Decoding Loss ---\n",
    "def decoding_loss(detector_out, message, mask=None, gamma=2.0):\n",
    "    \"\"\"\n",
    "    Decoding loss that ensures the watermark message embedded by the generator\n",
    "    can be accurately recovered from the detector's output.\n",
    "    \n",
    "    Args:\n",
    "        detector_out: Tensor of shape (B, 1+b, T) where the first channel is the detection\n",
    "                      probability and the remaining b channels contain bit probabilities.\n",
    "        message: Tensor of shape (B,) or (B, b) containing the true watermark message (as integers or bits).\n",
    "        mask: Optional tensor of shape (B, 1, T) that indicates watermarked regions.\n",
    "        gamma: Focal loss parameter for weighting hard examples.\n",
    "    \"\"\"\n",
    "    B, channels, T = detector_out.shape\n",
    "    b = channels - 1  # number of bit channels\n",
    "    if b <= 0:\n",
    "        return torch.tensor(0.0, device=detector_out.device)\n",
    "    \n",
    "    bit_prob_map = detector_out[:, 1:, :]  # shape (B, b, T)\n",
    "    \n",
    "    # If a mask is provided, focus on watermarked regions.\n",
    "    if mask is not None:\n",
    "        expanded_mask = mask.expand(-1, b, -1)  # shape (B, b, T)\n",
    "        masked_bit_map = bit_prob_map * expanded_mask\n",
    "        weights = expanded_mask.sum(dim=2, keepdim=True) + 1e-8\n",
    "        bit_prob = (masked_bit_map.sum(dim=2) / weights.squeeze(2))\n",
    "    else:\n",
    "        # Otherwise, use an attention-like mechanism to focus on time steps with high confidence.\n",
    "        confidence = torch.abs(bit_prob_map - 0.5) * 2.0\n",
    "        attention = F.softmax(confidence * 5.0, dim=2)\n",
    "        bit_prob = (bit_prob_map * attention).sum(dim=2)\n",
    "    \n",
    "    # Convert message integers to binary bit vectors.\n",
    "    msg_bits = []\n",
    "    for i in range(b):\n",
    "        bit_i = ((message >> i) & 1).float()\n",
    "        msg_bits.append(bit_i)\n",
    "    msg_bits = torch.stack(msg_bits, dim=1)  # shape (B, b)\n",
    "    \n",
    "    # Compute focal weighted binary cross-entropy loss.\n",
    "    pt = torch.where(msg_bits > 0.5, bit_prob, 1 - bit_prob)\n",
    "    focal_weight = (1 - pt) ** gamma\n",
    "    bce = F.binary_cross_entropy(bit_prob, msg_bits, reduction='none')\n",
    "    focal_bce = focal_weight * bce\n",
    "    return focal_bce.mean()\n",
    "\n",
    "# --- Training Loop with All Losses and Learning Rate Scheduler ---\n",
    "def train_one_epoch_full(generator, detector, loader, optimizer, device,\n",
    "                         lambda_tf_loud=0.5, lambda_adv=0.1, lambda_loc=1.0, lambda_dec=1.0):\n",
    "    generator.train()\n",
    "    detector.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # Instantiate loss modules.\n",
    "    simple_mel_loss = SimpleMelLoss().to(device)\n",
    "    tf_loud_loss = TFLoudnessLoss().to(device)\n",
    "    adv_loss_module = AdversarialLoss().to(device)\n",
    "    \n",
    "    for s in loader:\n",
    "        s = s.to(device)\n",
    "        B = s.shape[0]\n",
    "        msgs = generate_random_messages(B).to(device)\n",
    "        \n",
    "        # Generate watermarked audio.\n",
    "        delta = generator(s, msgs)\n",
    "        s_w = s + delta\n",
    "        \n",
    "        # Compute primary losses.\n",
    "        loss_l1  = F.l1_loss(delta, torch.zeros_like(delta))\n",
    "        loss_mel = simple_mel_loss(s, s_w)\n",
    "        loss_tf  = tf_loud_loss(s, s_w)\n",
    "        loss_adv = adv_loss_module(s, s_w, train_disc=True)\n",
    "        \n",
    "        # Compute localization loss using the detector network.\n",
    "        det_out = detector(s_w)\n",
    "        mask = torch.ones((B, 1, s.shape[-1]), device=device)  # assume full watermark coverage\n",
    "        loss_loc = masked_localization_loss(det_out, mask)\n",
    "        \n",
    "        # Compute decoding loss to ensure watermark message recoverability.\n",
    "        loss_dec = decoding_loss(det_out, msgs, mask)\n",
    "        \n",
    "        # Total loss is the weighted sum of all components.\n",
    "        loss = loss_l1 + loss_mel + lambda_tf_loud * loss_tf + lambda_adv * loss_adv + lambda_loc * loss_loc + lambda_dec * loss_dec\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss\n",
    "\n",
    "def overfit_on_small_subset_full_scheduler():\n",
    "    data_root = \"data/100_all\"\n",
    "    full_dataset = OneSecClipsDataset(root_dir=data_root, sample_rate=16000)\n",
    "    small_dataset = Subset(full_dataset, list(range(64)))\n",
    "    loader = DataLoader(small_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "    \n",
    "    generator = Generator().to(device)\n",
    "    detector  = Detector().to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(list(generator.parameters()) + list(detector.parameters()), lr=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    \n",
    "    num_epochs = 100\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        loss = train_one_epoch_full(generator, detector, loader, optimizer, device,\n",
    "                                    lambda_tf_loud=0.5, lambda_adv=0.1, lambda_loc=1.0, lambda_dec=1.0)\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch}/{num_epochs} - Loss: {loss:.4f} - LR: {current_lr:.6f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    overfit_on_small_subset_full_scheduler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]:   6%|â–Œ         | 280/4508 [02:11<33:06,  2.13it/s, loss_l1=0.001, loss_loc=0.000, loss_dec=0.175, total=10.007]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m\n",
      "\u001b[1;32m     16\u001b[0m detector  \u001b[38;5;241m=\u001b[39m Detector()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n",
      "\u001b[0;32m---> 19\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLR\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     21\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(generator\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m     22\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(detector\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetector.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n",
      "\n",
      "Cell \u001b[0;32mIn[6], line 154\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(generator, detector, train_dataset, val_dataset, num_epochs, lr)\u001b[0m\n",
      "\u001b[1;32m    148\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\n",
      "\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mlist\u001b[39m(generator\u001b[38;5;241m.\u001b[39mparameters()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(detector\u001b[38;5;241m.\u001b[39mparameters()),\n",
      "\u001b[1;32m    150\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr\n",
      "\u001b[1;32m    151\u001b[0m )\n",
      "\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;32m--> 154\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    155\u001b[0m     val_loss   \u001b[38;5;241m=\u001b[39m validate_one_epoch(generator, detector, val_loader, device)\n",
      "\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]  TRAIN Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  |  VAL Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "Cell \u001b[0;32mIn[6], line 78\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(generator, detector, train_loader, optimizer, epoch, total_epochs, device)\u001b[0m\n",
      "\u001b[1;32m     70\u001b[0m loss \u001b[38;5;241m=\u001b[39m (lambda_L1     \u001b[38;5;241m*\u001b[39m loss_l1\n",
      "\u001b[1;32m     71\u001b[0m       \u001b[38;5;241m+\u001b[39m lambda_msspec \u001b[38;5;241m*\u001b[39m loss_msspec\n",
      "\u001b[1;32m     72\u001b[0m       \u001b[38;5;241m+\u001b[39m lambda_adv    \u001b[38;5;241m*\u001b[39m loss_adv\n",
      "\u001b[1;32m     73\u001b[0m       \u001b[38;5;241m+\u001b[39m lambda_loud   \u001b[38;5;241m*\u001b[39m loss_loud\n",
      "\u001b[1;32m     74\u001b[0m       \u001b[38;5;241m+\u001b[39m lambda_loc    \u001b[38;5;241m*\u001b[39m loss_loc\n",
      "\u001b[1;32m     75\u001b[0m       \u001b[38;5;241m+\u001b[39m lambda_dec    \u001b[38;5;241m*\u001b[39m loss_dec)\n",
      "\u001b[1;32m     77\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;32m---> 78\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     79\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[1;32m     81\u001b[0m sum_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n",
      "\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n",
      "\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n",
      "\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n",
      "\u001b[1;32m    580\u001b[0m     )\n",
      "\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n",
      "\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n",
      "\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n",
      "\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n",
      "\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n",
      "\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n",
      "\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n",
      "\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n",
      "\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n",
      "\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create dataset\n",
    "    data_root = \"data/100_all\"\n",
    "    full_dataset = OneSecClipsDataset(root_dir=data_root, sample_rate=16000)\n",
    "\n",
    "\n",
    "    # Train/Val split\n",
    "    n = len(full_dataset)\n",
    "    n_train = int(0.8*n)\n",
    "    n_val   = int(0.1*n)\n",
    "    n_test  = n - n_train - n_val\n",
    "    train_ds, val_ds, test_ds = random_split(full_dataset, [n_train, n_val, n_test])\n",
    "\n",
    "    # Instantiate models\n",
    "    generator = Generator().to(device)\n",
    "    detector  = Detector().to(device)\n",
    "\n",
    "    # Train\n",
    "    train_model(generator, detector, train_ds, val_ds, num_epochs=10, lr=LR)\n",
    "\n",
    "    torch.save(generator.state_dict(), \"generator.pth\")\n",
    "    torch.save(detector.state_dict(), \"detector.pth\")  \n",
    "    # Evaluate\n",
    "    run_evaluation(generator, detector, test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the state dictionary of the generator and detector\n",
    "# torch.save(generator.state_dict(), \"generator.pth\")\n",
    "# torch.save(detector.state_dict(), \"detector.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Subset\n",
    "\n",
    "                                                            \n",
    "# if __name__ == \"__main__\":\n",
    "#     # Create dataset\n",
    "#     data_root = \"data/100_all\"\n",
    "#     full_dataset = OneSecClipsDataset(root_dir=data_root, sample_rate=16000)\n",
    "\n",
    "\n",
    "#     # Train/Val split\n",
    "#     n = len(full_dataset)\n",
    "#     n_train = int(0.8*n)\n",
    "#     n_val   = int(0.1*n)\n",
    "#     n_test  = n - n_train - n_val\n",
    "#     train_ds, val_ds, test_ds = random_split(full_dataset, [n_train, n_val, n_test])\n",
    "\n",
    "#     # Instantiate models\n",
    "#     generator = Generator().to(device)\n",
    "#     detector  = Detector().to(device)\n",
    "\n",
    "# # Create a subset of 100 samples (adjust as needed)\n",
    "#     small_subset_indices = list(range(1000))\n",
    "#     small_train_ds = Subset(full_dataset, small_subset_indices)\n",
    "#     small_train_loader = DataLoader(small_train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "#     # Now train using the small subset\n",
    "#     train_model(generator, detector, small_train_ds, val_ds, num_epochs=10, lr=LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from pesq import pesq\n",
    "except ImportError:\n",
    "    pesq = None\n",
    "    print(\"WARNING: pesq library not found. PESQ will be unavailable.\")\n",
    "\n",
    "######################################################################\n",
    "# 1) SI-SNR\n",
    "######################################################################\n",
    "def evaluate_si_snr_torch(original, reconstructed, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Torch-based SI-SNR for a batch:\n",
    "      original, reconstructed: shape (B, 1, T) or (B, T).\n",
    "    Returns: Tensor of shape (B,) with SI-SNR(dB) per sample in the batch.\n",
    "    \"\"\"\n",
    "    if original.dim() == 3:\n",
    "        # shape (B,1,T) => (B,T)\n",
    "        original = original.squeeze(1)\n",
    "    if reconstructed.dim() == 3:\n",
    "        reconstructed = reconstructed.squeeze(1)\n",
    "\n",
    "    # Zero-mean along time\n",
    "    original_zm = original - torch.mean(original, dim=1, keepdim=True)\n",
    "    recon_zm    = reconstructed - torch.mean(reconstructed, dim=1, keepdim=True)\n",
    "\n",
    "    # Project recon onto original\n",
    "    dot = torch.sum(original_zm * recon_zm, dim=1, keepdim=True)\n",
    "    norm_sq = torch.sum(original_zm**2, dim=1, keepdim=True) + eps\n",
    "    alpha = dot / norm_sq\n",
    "\n",
    "    s_target = alpha * original_zm           # shape (B,T)\n",
    "    e_noise  = recon_zm - s_target\n",
    "    num  = torch.sum(s_target**2, dim=1) + eps\n",
    "    den  = torch.sum(e_noise**2,  dim=1) + eps\n",
    "    si_snr_val = 10 * torch.log10(num / den)\n",
    "    return si_snr_val\n",
    "\n",
    "######################################################################\n",
    "# 2) PESQ\n",
    "######################################################################\n",
    "def evaluate_pesq(original, reconstructed, sr=16000):\n",
    "    \"\"\"\n",
    "    Single-sample PESQ in wide-band mode (for sr=16000).\n",
    "    - original, reconstructed: shape (T,) or (1,T) as torch Tensors or numpy arrays\n",
    "    - returns float PESQ or 0.0 if error/no library\n",
    "    \"\"\"\n",
    "    if pesq is None:\n",
    "        print(\"PESQ library not available.\")\n",
    "        return 0.0\n",
    "\n",
    "    # Convert to CPU numpy\n",
    "    if isinstance(original, torch.Tensor):\n",
    "        original = original.squeeze().detach().cpu().numpy()\n",
    "    if isinstance(reconstructed, torch.Tensor):\n",
    "        reconstructed = reconstructed.squeeze().detach().cpu().numpy()\n",
    "\n",
    "    # Ensure valid sample rate for PESQ\n",
    "    assert sr in [8000, 16000], \"PESQ only supports 8k or 16k sample rates.\"\n",
    "\n",
    "    # Normalize if needed\n",
    "    if np.max(np.abs(original)) > 1.0:\n",
    "        original = original / np.max(np.abs(original))\n",
    "    if np.max(np.abs(reconstructed)) > 1.0:\n",
    "        reconstructed = reconstructed / np.max(np.abs(reconstructed))\n",
    "\n",
    "    mode = 'wb' if sr == 16000 else 'nb'\n",
    "    try:\n",
    "        score = pesq(sr, original, reconstructed, mode)\n",
    "    except Exception as e:\n",
    "        print(f\"PESQ error: {e}\")\n",
    "        score = 0.0\n",
    "    return score\n",
    "\n",
    "######################################################################\n",
    "# 3) Multi-bit message generation\n",
    "######################################################################\n",
    "def generate_random_messages(batch_size, num_bits=16, as_bits=True):\n",
    "    \"\"\"\n",
    "    If as_bits=True, return shape (B, num_bits) of {0,1}.\n",
    "    If as_bits=False, return shape (B,) of integers in [0..2^num_bits-1].\n",
    "    \"\"\"\n",
    "    if as_bits:\n",
    "        return torch.randint(0, 2, (batch_size, num_bits), dtype=torch.float)\n",
    "    else:\n",
    "        max_val = 2**num_bits\n",
    "        return torch.randint(0, max_val, (batch_size,), dtype=torch.long)\n",
    "\n",
    "######################################################################\n",
    "# 4) Decoding from Detector\n",
    "######################################################################\n",
    "def decode_bits_from_detector(detector_out):\n",
    "    \"\"\"\n",
    "    If the detector outputs shape (B, 1+b, T):\n",
    "      - channel 0 = detection probability per sample\n",
    "      - channels [1..b] = bit probability per sample\n",
    "    We'll average across time => shape (B,b)\n",
    "    Then threshold at 0.5 => bits\n",
    "    Returns a float tensor shape (B,b) in {0,1}.\n",
    "    \"\"\"\n",
    "    B, channels, T = detector_out.shape\n",
    "    b = channels - 1\n",
    "    if b <= 0:\n",
    "        return torch.zeros((B,0), device=detector_out.device)\n",
    "\n",
    "    bit_prob_map = detector_out[:, 1:, :]  # (B,b,T)\n",
    "    bit_prob = bit_prob_map.mean(dim=2)    # (B,b)\n",
    "    bit_pred = (bit_prob > 0.5).float()    # (B,b)\n",
    "    return bit_pred\n",
    "\n",
    "######################################################################\n",
    "# 5) Evaluate the entire pipeline\n",
    "######################################################################\n",
    "def run_evaluation(generator, detector, test_dataset, device,\n",
    "                   batch_size=16, num_workers=4, sr=16000, \n",
    "                   num_bits=16, as_bits=True, \n",
    "                   compute_pesq_score=True, compute_si_snr_score=True):\n",
    "    \"\"\"\n",
    "    Evaluate on test_dataset:\n",
    "      1) We generate random messages (bit vector or integer).\n",
    "      2) Generator => watermarked\n",
    "      3) Detector => decode bits\n",
    "      4) Compare with ground truth\n",
    "      5) Compute SI-SNR & PESQ\n",
    "\n",
    "    generator, detector: your trained models\n",
    "    test_dataset: dataset of shape (B,1,T)\n",
    "    device: \"cuda\" or \"cpu\"\n",
    "    batch_size: ...\n",
    "    sr: sample rate for PESQ\n",
    "    num_bits: used to generate messages\n",
    "    as_bits: if True => shape (B,b), else => shape (B,)\n",
    "    compute_pesq_score, compute_si_snr_score: toggles\n",
    "\n",
    "    Returns a dict with average SI-SNR, PESQ, and bit accuracy.\n",
    "    \"\"\"\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    generator.eval()\n",
    "    detector.eval()\n",
    "\n",
    "    si_snr_vals  = []\n",
    "    pesq_vals    = []\n",
    "    bit_acc_vals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for s in test_loader:\n",
    "            s = s.to(device)  # (B,1,T)\n",
    "            B = s.shape[0]\n",
    "\n",
    "            # 1) Generate random messages\n",
    "            msgs = generate_random_messages(B, num_bits=num_bits, as_bits=as_bits).to(device)\n",
    "\n",
    "            # 2) Generator forward\n",
    "            delta = generator(s, msgs)\n",
    "            s_w   = s + delta\n",
    "\n",
    "            # 3) Detector forward => shape (B,1+b,T)\n",
    "            det_out = detector(s_w)\n",
    "            # decode bits\n",
    "            bit_pred = decode_bits_from_detector(det_out)  # (B,b)\n",
    "\n",
    "            # ground truth bits\n",
    "            if as_bits:\n",
    "                bit_true = msgs  # shape (B,b)\n",
    "            else:\n",
    "                # if we have an integer, we must convert to bits\n",
    "                # shape (B,b)\n",
    "                bit_true = []\n",
    "                for i in range(num_bits):\n",
    "                    bit_i = ((msgs >> i) & 1).float()\n",
    "                    bit_true.append(bit_i)\n",
    "                bit_true = torch.stack(bit_true, dim=1)\n",
    "\n",
    "            # compute bitwise accuracy per sample\n",
    "            # shape (B,b)\n",
    "            matches = (bit_pred == bit_true).float()\n",
    "            sample_acc = matches.mean(dim=1)  # (B,) average bits => per-sample accuracy\n",
    "            bit_acc_vals.extend(sample_acc.cpu().numpy())\n",
    "\n",
    "            # 4) Audio quality metrics\n",
    "            if compute_si_snr_score:\n",
    "                si_snr_batch = evaluate_si_snr_torch(s, s_w)  # shape (B,)\n",
    "                si_snr_vals.extend(si_snr_batch.cpu().numpy())\n",
    "\n",
    "            if compute_pesq_score:\n",
    "                # PESQ is per sample\n",
    "                for i in range(B):\n",
    "                    pesq_score = evaluate_pesq(s[i], s_w[i], sr=sr)\n",
    "                    pesq_vals.append(pesq_score)\n",
    "\n",
    "    # Aggregate\n",
    "    avg_si_snr = float(np.mean(si_snr_vals)) if len(si_snr_vals)>0 else 0.0\n",
    "    avg_pesq   = float(np.mean(pesq_vals))   if len(pesq_vals)>0 else 0.0\n",
    "    avg_bit_acc= float(np.mean(bit_acc_vals))if len(bit_acc_vals)>0 else 0.0\n",
    "\n",
    "    print(\"\\n--- Evaluation Results ---\")\n",
    "    if compute_si_snr_score:\n",
    "        print(f\"SI-SNR:  {avg_si_snr:.3f} dB\")\n",
    "    if compute_pesq_score:\n",
    "        print(f\"PESQ:    {avg_pesq:.3f}\")\n",
    "    print(f\"Bit Accuracy: {avg_bit_acc*100:.2f}%\")\n",
    "\n",
    "    return {\n",
    "        \"si_snr\": avg_si_snr,\n",
    "        \"pesq\": avg_pesq,\n",
    "        \"bit_accuracy\": avg_bit_acc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_evaluation(\n",
    "    generator, detector, \n",
    "    test_dataset=test_ds, \n",
    "    device=device,\n",
    "    batch_size=16,\n",
    "    num_workers=4,\n",
    "    sr=16000,\n",
    "    num_bits=16,\n",
    "    as_bits=False,   # or True if your generator uses bit vectors\n",
    "    compute_pesq_score=True,\n",
    "    compute_si_snr_score=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(watermarked_audio.abs().max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing losses...\n",
      "âœ“ MultiScaleMelLoss: 0.006972\n",
      "âœ“ AdversarialLoss: 0.705468\n",
      "âœ“ TFLoudnessLoss: 0.000844\n",
      "âœ“ masked_localization_loss: 0.324282\n",
      "âœ“ decoding_loss: 0.172981\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Set up a device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def test_losses():\n",
    "    # Create dummy input data\n",
    "    batch_size = 2\n",
    "    audio_length = 16000  # 1 second at 16kHz\n",
    "    \n",
    "    # Original and watermarked audio\n",
    "    original = torch.randn(batch_size, 1, audio_length, device=device)\n",
    "    watermarked = original + 0.01 * torch.randn(batch_size, 1, audio_length, device=device)\n",
    "    \n",
    "    # Detector output (1 detection channel + 8 bit channels)\n",
    "    num_bits = 8\n",
    "    detector_out = torch.rand(batch_size, 1 + num_bits, audio_length, device=device)\n",
    "    detector_out[:, 0, :] = torch.sigmoid(torch.randn(batch_size, audio_length, device=device))  # detection probs\n",
    "    \n",
    "    # Binary mask and message\n",
    "    mask = torch.ones(batch_size, 1, audio_length, device=device)\n",
    "    message = torch.randint(0, 2**num_bits - 1, (batch_size,), device=device)\n",
    "    \n",
    "    # Test each loss\n",
    "    print(\"Testing losses...\")\n",
    "    \n",
    "    try:\n",
    "        # Test MultiScaleMelLoss\n",
    "        mel_loss = MultiScaleMelLoss().to(device)\n",
    "        loss_value = mel_loss(original, watermarked)\n",
    "        print(f\"âœ“ MultiScaleMelLoss: {loss_value.item():.6f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— MultiScaleMelLoss error: {str(e)}\")\n",
    "    \n",
    "    try:\n",
    "        # Test AdversarialLoss\n",
    "        adv_loss = AdversarialLoss().to(device)\n",
    "        loss_value = adv_loss(original, watermarked)\n",
    "        print(f\"âœ“ AdversarialLoss: {loss_value.item():.6f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— AdversarialLoss error: {str(e)}\")\n",
    "    \n",
    "    try:\n",
    "        # Test TFLoudnessLoss\n",
    "        tf_loss = TFLoudnessLoss().to(device)\n",
    "        loss_value = tf_loss(original, watermarked)\n",
    "        print(f\"âœ“ TFLoudnessLoss: {loss_value.item():.6f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— TFLoudnessLoss error: {str(e)}\")\n",
    "    \n",
    "    try:\n",
    "        # Test masked_localization_loss\n",
    "        loss_value = masked_localization_loss(detector_out, mask)\n",
    "        print(f\"âœ“ masked_localization_loss: {loss_value.item():.6f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— masked_localization_loss error: {str(e)}\")\n",
    "    \n",
    "    try:\n",
    "        # Test decoding_loss\n",
    "        loss_value = decoding_loss(detector_out, message, mask)\n",
    "        print(f\"âœ“ decoding_loss: {loss_value.item():.6f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— decoding_loss error: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gradients():\n",
    "    # Original and watermarked audio with requires_grad=True\n",
    "    original = torch.randn(2, 1, 16000, device=device, requires_grad=True)\n",
    "    watermarked = original + 0.01 * torch.randn(2, 1, 16000, device=device, requires_grad=True)\n",
    "    \n",
    "    # Test gradient flow for each loss\n",
    "    losses_to_test = [\n",
    "        (\"MultiScaleMelLoss\", lambda: MultiScaleMelLoss().to(device)(original, watermarked)),\n",
    "        (\"AdversarialLoss\", lambda: AdversarialLoss().to(device)(original, watermarked)),\n",
    "        (\"TFLoudnessLoss\", lambda: TFLoudnessLoss().to(device)(original, watermarked))\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nTesting gradient flow...\")\n",
    "    \n",
    "    for loss_name, loss_fn in losses_to_test:\n",
    "        try:\n",
    "            # Forward pass\n",
    "            loss = loss_fn()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Check if gradients exist\n",
    "            if watermarked.grad is not None:\n",
    "                grad_norm = watermarked.grad.norm().item()\n",
    "                print(f\"âœ“ {loss_name} gradients: {grad_norm:.6f}\")\n",
    "            else:\n",
    "                print(f\"âœ— {loss_name} no gradients\")\n",
    "                \n",
    "            # Reset gradients for next test\n",
    "            original.grad = None\n",
    "            watermarked.grad = None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âœ— {loss_name} gradient error: {str(e)}\")\n",
    "            original.grad = None\n",
    "            watermarked.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss_behavior():\n",
    "    print(\"\\nTesting loss behavior...\")\n",
    "    \n",
    "    # Create audio with varying levels of difference\n",
    "    original = torch.randn(1, 1, 16000, device=device)\n",
    "    \n",
    "    # Test with increasing levels of perturbation\n",
    "    perturbation_levels = [0.001, 0.01, 0.1, 0.5]\n",
    "    \n",
    "    for level in perturbation_levels:\n",
    "        # Create watermarked version with controlled perturbation\n",
    "        noise = torch.randn(1, 1, 16000, device=device)\n",
    "        watermarked = original + level * noise\n",
    "        \n",
    "        # Compute losses\n",
    "        mel_loss = MultiScaleMelLoss().to(device)(original, watermarked).item()\n",
    "        tf_loss = TFLoudnessLoss().to(device)(original, watermarked).item()\n",
    "        \n",
    "        print(f\"Perturbation {level:.3f}: Mel Loss = {mel_loss:.6f}, TF Loss = {tf_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing losses...\n",
      "âœ“ MultiScaleMelLoss: 0.006790\n",
      "âœ“ AdversarialLoss: 0.700679\n",
      "âœ“ TFLoudnessLoss: 0.000801\n",
      "âœ“ masked_localization_loss: 0.328260\n",
      "âœ“ decoding_loss: 0.173435\n",
      "\n",
      "Testing gradient flow...\n",
      "âœ— MultiScaleMelLoss no gradients\n",
      "âœ— AdversarialLoss gradient error: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.\n",
      "âœ— TFLoudnessLoss gradient error: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.\n",
      "\n",
      "Testing loss behavior...\n",
      "Perturbation 0.001: Mel Loss = 0.000639, TF Loss = 0.000067\n",
      "Perturbation 0.010: Mel Loss = 0.006575, TF Loss = 0.000772\n",
      "Perturbation 0.100: Mel Loss = 0.076570, TF Loss = 0.014562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9064/3621405596.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  if watermarked.grad is not None:\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [30,0,0], thread: [96,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [30,0,0], thread: [99,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [30,0,0], thread: [101,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [30,0,0], thread: [102,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [30,0,0], thread: [104,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [30,0,0], thread: [105,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [30,0,0], thread: [109,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [30,0,0], thread: [110,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [30,0,0], thread: [112,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [30,0,0], thread: [114,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [30,0,0], thread: [116,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [30,0,0], thread: [117,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [30,0,0], thread: [119,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [30,0,0], thread: [121,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [30,0,0], thread: [124,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [30,0,0], thread: [127,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbation 0.500: Mel Loss = 0.555924, TF Loss = 0.195192\n",
      "\n",
      "Testing mini training loop...\n",
      "âœ— Mini training failed: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_mini_training():\n",
    "    print(\"\\nTesting mini training loop...\")\n",
    "    \n",
    "    # Create simple models for testing\n",
    "    class SimpleGenerator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv = nn.Conv1d(1, 1, 3, padding=1)\n",
    "            \n",
    "        def forward(self, audio, message):\n",
    "            # Ignore message for simplicity\n",
    "            return self.conv(audio)\n",
    "    \n",
    "    class SimpleDetector(nn.Module):\n",
    "        def __init__(self, num_bits=8):\n",
    "            super().__init__()\n",
    "            self.conv = nn.Conv1d(1, 1 + num_bits, 3, padding=1)\n",
    "            \n",
    "        def forward(self, audio):\n",
    "            return self.conv(audio)\n",
    "    \n",
    "    # Instantiate models\n",
    "    generator = SimpleGenerator().to(device)\n",
    "    detector = SimpleDetector(num_bits=8).to(device)\n",
    "    optimizer = torch.optim.Adam(list(generator.parameters()) + list(detector.parameters()), lr=0.001)\n",
    "    \n",
    "    # Loss functions\n",
    "    ms_mel_loss = MultiScaleMelLoss().to(device)\n",
    "    adv_loss = AdversarialLoss().to(device)\n",
    "    tf_loud_loss = TFLoudnessLoss().to(device)\n",
    "    \n",
    "    # Mini batch\n",
    "    original = torch.randn(2, 1, 16000, device=device)\n",
    "    message = torch.randint(0, 255, (2,), device=device)\n",
    "    \n",
    "    # Training step\n",
    "    try:\n",
    "        # Forward pass\n",
    "        delta = generator(original, message)\n",
    "        watermarked = original + delta\n",
    "        det_out = detector(watermarked)\n",
    "        \n",
    "        # Compute losses\n",
    "        loss_l1 = F.l1_loss(delta, torch.zeros_like(delta))\n",
    "        loss_msspec = ms_mel_loss(original, watermarked)\n",
    "        loss_adv = adv_loss(original, watermarked)\n",
    "        loss_loud = tf_loud_loss(original, watermarked)\n",
    "        mask = torch.ones((2, 1, 16000), device=device)\n",
    "        loss_loc = masked_localization_loss(det_out, mask)\n",
    "        loss_dec = decoding_loss(det_out, message, mask)\n",
    "        \n",
    "        # Total loss\n",
    "        lambda_weights = [1.0, 1.0, 0.1, 0.5, 1.0, 1.0]\n",
    "        loss = (\n",
    "            lambda_weights[0] * loss_l1 +\n",
    "            lambda_weights[1] * loss_msspec +\n",
    "            lambda_weights[2] * loss_adv +\n",
    "            lambda_weights[3] * loss_loud +\n",
    "            lambda_weights[4] * loss_loc +\n",
    "            lambda_weights[5] * loss_dec\n",
    "        )\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"âœ“ Mini training successful with total loss: {loss.item():.6f}\")\n",
    "        print(f\"  L1: {loss_l1.item():.4f}, Mel: {loss_msspec.item():.4f}, Adv: {loss_adv.item():.4f}\")\n",
    "        print(f\"  Loud: {loss_loud.item():.4f}, Loc: {loss_loc.item():.4f}, Dec: {loss_dec.item():.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Mini training failed: {str(e)}\")\n",
    "\n",
    "# Run all tests\n",
    "test_losses()\n",
    "test_gradients()\n",
    "test_loss_behavior()\n",
    "test_mini_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
