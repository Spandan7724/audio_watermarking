{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import glob\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import Counter\n",
    "import librosa\n",
    "# For inline plots in Jupyter Notebook\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Set the root directory for the preprocessed audio clips\n",
    "dataset_dir = \"data/100_all\"\n",
    "\n",
    "# Recursively list all .wav files in the directory\n",
    "filepaths = glob.glob(os.path.join(dataset_dir, '**', '*.wav'), recursive=True)\n",
    "print(\"Total number of audio clips:\", len(filepaths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Load a sample file to inspect its properties\n",
    "sample_path = filepaths[0]\n",
    "waveform, sr = torchaudio.load(sample_path)\n",
    "print(\"Sample file:\", sample_path)\n",
    "print(\"Waveform shape:\", waveform.shape)  # Expected shape: [1, 16000] for 1-second mono clips\n",
    "print(\"Sample rate:\", sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Plot the waveform of the sample file\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(waveform.squeeze().numpy())\n",
    "plt.title(\"Waveform of a Sample 1-Second Clip\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Compute a Mel Spectrogram and convert it to decibels\n",
    "mel_transform = torchaudio.transforms.MelSpectrogram(sample_rate=sr, n_mels=64)\n",
    "spec = mel_transform(waveform)\n",
    "spec_db = torchaudio.transforms.AmplitudeToDB()(spec)\n",
    "\n",
    "# Plot the Mel Spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(spec_db.squeeze().numpy(), origin=\"lower\", aspect=\"auto\", cmap=\"viridis\")\n",
    "plt.title(\"Mel Spectrogram (dB) of a Sample Clip\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Mel Frequency\")\n",
    "plt.colorbar(label=\"dB\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Take a random sample of 50 files (or all if less than 50) to compute basic statistics\n",
    "subset_paths = random.sample(filepaths, min(50, len(filepaths)))\n",
    "durations = []         # All clips should be around 1 second, but we compute for verification\n",
    "max_amplitudes = []    # Maximum absolute amplitude per clip\n",
    "\n",
    "for path in subset_paths:\n",
    "    wf, _ = torchaudio.load(path)\n",
    "    durations.append(wf.shape[-1] / sr)  # Duration in seconds\n",
    "    max_amplitudes.append(np.max(np.abs(wf.numpy())))\n",
    "\n",
    "print(\"Average duration (should be ~1 second):\", np.mean(durations))\n",
    "print(\"Average max amplitude:\", np.mean(max_amplitudes))\n",
    "\n",
    "# Plot histogram of maximum amplitudes\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(max_amplitudes, bins=20, edgecolor='black')\n",
    "plt.title(\"Histogram of Max Amplitudes (Sampled Clips)\")\n",
    "plt.xlabel(\"Max Amplitude\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def compute_rms(waveform):\n",
    "    return np.sqrt(np.mean(waveform**2))\n",
    "\n",
    "def compute_zcr(waveform):\n",
    "    # Zero crossing rate using librosa's function\n",
    "    return np.mean(librosa.feature.zero_crossing_rate(waveform)[0])\n",
    "\n",
    "# Get a list of file paths from your preprocessed directory\n",
    "filepaths = glob.glob(os.path.join(\"data/100_all\", '**', '*.wav'), recursive=True)\n",
    "print(\"Total number of audio clips:\", len(filepaths))\n",
    "\n",
    "# Compute RMS and ZCR for a sample of clips (say 1000 clips)\n",
    "sample_filepaths = np.random.choice(filepaths, min(1000, len(filepaths)), replace=False)\n",
    "\n",
    "rms_list = []\n",
    "zcr_list = []\n",
    "\n",
    "for path in sample_filepaths:\n",
    "    waveform, sr = librosa.load(path, sr=16000)\n",
    "    rms_list.append(compute_rms(waveform))\n",
    "    zcr_list.append(compute_zcr(waveform))\n",
    "\n",
    "# Plot histogram for RMS\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(rms_list, bins=30, edgecolor='black')\n",
    "plt.title(\"Histogram of RMS Energy\")\n",
    "plt.xlabel(\"RMS Energy\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Plot histogram for ZCR\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(zcr_list, bins=30, edgecolor='black')\n",
    "plt.title(\"Histogram of Zero Crossing Rate\")\n",
    "plt.xlabel(\"ZCR\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm  # For progress bar\n",
    "\n",
    "def get_audio_properties(filepath):\n",
    "    \"\"\"Extract various properties from an audio file\"\"\"\n",
    "    try:\n",
    "        # Load the audio file\n",
    "        waveform, sr = librosa.load(filepath, sr=None)  # Using sr=None to get the native sample rate\n",
    "        \n",
    "        # Calculate properties\n",
    "        duration = librosa.get_duration(y=waveform, sr=sr)\n",
    "        rms = compute_rms(waveform)\n",
    "        zcr = compute_zcr(waveform)\n",
    "        spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=waveform, sr=sr)[0])\n",
    "        spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=waveform, sr=sr)[0])\n",
    "        \n",
    "        return {\n",
    "            'filepath': filepath,\n",
    "            'sample_rate': sr,\n",
    "            'duration': duration,\n",
    "            'num_samples': len(waveform),\n",
    "            'rms': rms,\n",
    "            'zcr': zcr, \n",
    "            'spectral_centroid': spectral_centroid,\n",
    "            'spectral_bandwidth': spectral_bandwidth\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filepath}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Get a list of file paths from your preprocessed directory\n",
    "filepaths = glob.glob(os.path.join(\"data/100_all\", '**', '*.wav'), recursive=True)\n",
    "print(f\"Total number of audio clips: {len(filepaths)}\")\n",
    "\n",
    "# Sample files to analyze (adjust the number as needed)\n",
    "sample_size = min(500, len(filepaths))\n",
    "sample_filepaths = np.random.choice(filepaths, sample_size, replace=False)\n",
    "\n",
    "# Analyze audio files with progress bar\n",
    "audio_properties = []\n",
    "for path in tqdm(sample_filepaths, desc=\"Analyzing audio files\"):\n",
    "    props = get_audio_properties(path)\n",
    "    if props:\n",
    "        audio_properties.append(props)\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "df = pd.DataFrame(audio_properties)\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n--- Dataset Properties Summary ---\")\n",
    "print(f\"Number of files analyzed: {len(df)}\")\n",
    "print(\"\\nSample Rate Statistics:\")\n",
    "sr_counts = df['sample_rate'].value_counts()\n",
    "for sr, count in sr_counts.items():\n",
    "    print(f\"  {sr} Hz: {count} files ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nDuration Statistics:\")\n",
    "print(f\"  Min duration: {df['duration'].min():.2f} seconds\")\n",
    "print(f\"  Max duration: {df['duration'].max():.2f} seconds\")\n",
    "print(f\"  Mean duration: {df['duration'].mean():.2f} seconds\")\n",
    "print(f\"  Median duration: {df['duration'].median():.2f} seconds\")\n",
    "\n",
    "print(\"\\nOther Audio Properties (mean values):\")\n",
    "print(f\"  RMS energy: {df['rms'].mean():.5f}\")\n",
    "print(f\"  Zero crossing rate: {df['zcr'].mean():.5f}\")\n",
    "print(f\"  Spectral centroid: {df['spectral_centroid'].mean():.2f} Hz\")\n",
    "print(f\"  Spectral bandwidth: {df['spectral_bandwidth'].mean():.2f} Hz\")\n",
    "\n",
    "# Plot histograms for key properties\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "axes[0, 0].hist(df['duration'], bins=30, edgecolor='black')\n",
    "axes[0, 0].set_title(\"Distribution of Audio Duration\")\n",
    "axes[0, 0].set_xlabel(\"Duration (seconds)\")\n",
    "axes[0, 0].set_ylabel(\"Frequency\")\n",
    "\n",
    "axes[0, 1].hist(df['rms'], bins=30, edgecolor='black')\n",
    "axes[0, 1].set_title(\"Distribution of RMS Energy\")\n",
    "axes[0, 1].set_xlabel(\"RMS\")\n",
    "axes[0, 1].set_ylabel(\"Frequency\")\n",
    "\n",
    "axes[1, 0].hist(df['zcr'], bins=30, edgecolor='black')\n",
    "axes[1, 0].set_title(\"Distribution of Zero Crossing Rate\")\n",
    "axes[1, 0].set_xlabel(\"ZCR\")\n",
    "axes[1, 0].set_ylabel(\"Frequency\")\n",
    "\n",
    "axes[1, 1].hist(df['spectral_centroid'], bins=30, edgecolor='black')\n",
    "axes[1, 1].set_title(\"Distribution of Spectral Centroid\")\n",
    "axes[1, 1].set_xlabel(\"Frequency (Hz)\")\n",
    "axes[1, 1].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.filepaths = glob.glob(os.path.join(root_dir, '**', '*.wav'), recursive=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filepath = self.filepaths[idx]\n",
    "        waveform, sample_rate = torchaudio.load(filepath)\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "        return waveform, sample_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "dataset_dir = \"data/100_all\"\n",
    "audio_dataset = AudioDataset(root_dir=dataset_dir)\n",
    "\n",
    "print(\"Total number of audio clips in dataset:\", len(audio_dataset))\n",
    "\n",
    "dataloader = DataLoader(audio_dataset, batch_size=32, shuffle=True, num_workers=16)\n",
    "\n",
    "for batch in dataloader:\n",
    "    waveforms, sample_rates = batch\n",
    "    # print(\"Batch waveforms shape:\", waveforms.shape)\n",
    "    # print(\"Sample rates:\", sample_rates)\n",
    "    # break  # Just process one batch for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# # Example usage of the WatermarkGenerator and WatermarkDetector\n",
    "# if __name__ == \"__main__\":\n",
    "#     dummy_input = torch.randn(4, 1, 16000)\n",
    "    \n",
    "#     generator = WatermarkGenerator()\n",
    "#     detector = WatermarkDetector()\n",
    "#     watermark_delta = generator(dummy_input)\n",
    "#     print(\"Watermark delta shape:\", watermark_delta.shape) \n",
    "#     watermarked_audio = dummy_input + watermark_delta\n",
    "    \n",
    "#     # Run detector on the watermarked audio\n",
    "#     detection_output = detector(watermarked_audio)\n",
    "#     print(\"Detection output shape:\", detection_output.shape)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "\n",
    "# Assume audio_dataset is your complete dataset instance (e.g., AudioDataset)\n",
    "dataset_size = len(audio_dataset)\n",
    "subset_size = int(0.01 * dataset_size)  # 1% of the data\n",
    "\n",
    "# Randomly select subset_size indices without replacement\n",
    "subset_indices = np.random.choice(dataset_size, subset_size, replace=False)\n",
    "\n",
    "# Create the subset dataset\n",
    "subset_dataset = Subset(audio_dataset, subset_indices)\n",
    "\n",
    "# Create a DataLoader for the subset\n",
    "subset_dataloader = DataLoader(subset_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "print(f\"Using {len(subset_dataset)} samples out of {dataset_size} for testing (approx. 1%).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def compute_all_metrics(pred_probs, target, threshold=0.5):\n",
    "    pred_flat = pred_probs.view(-1).detach().cpu().numpy()\n",
    "    target_flat = target.view(-1).detach().cpu().numpy()\n",
    "    \n",
    "    pred_labels = (pred_flat >= threshold).astype(int)\n",
    "    target_labels = target_flat.astype(int)\n",
    "    \n",
    "    acc = accuracy_score(target_labels, pred_labels)\n",
    "    roc_auc = roc_auc_score(target_labels, pred_flat)\n",
    "    precision = precision_score(target_labels, pred_labels, zero_division=0)\n",
    "    recall = recall_score(target_labels, pred_labels, zero_division=0)\n",
    "    f1 = f1_score(target_labels, pred_labels, zero_division=0)\n",
    "    cm = confusion_matrix(target_labels, pred_labels)\n",
    "    \n",
    "    metrics = {\n",
    "        \"accuracy\": acc,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"confusion_matrix\": cm\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# import random\n",
    "# # Set up device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"Using device:\", device)\n",
    "\n",
    "# #############################\n",
    "# # 1. Model Architectures\n",
    "# #############################\n",
    "\n",
    "# # --- Original WatermarkGenerator (without message) ---\n",
    "# class WatermarkGenerator(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(WatermarkGenerator, self).__init__()\n",
    "#         # Encoder: Downsample input from 16000 -> 4000 samples\n",
    "#         self.encoder = nn.Sequential(\n",
    "#             nn.Conv1d(1, 16, kernel_size=15, stride=1, padding=7), \n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(16, 32, kernel_size=15, stride=2, padding=7),  \n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(32, 64, kernel_size=15, stride=2, padding=7),  \n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "#         # Bottleneck: LSTM to capture temporal dependencies\n",
    "#         self.lstm = nn.LSTM(input_size=64, hidden_size=64, num_layers=1, batch_first=True)\n",
    "#         # Decoder: Upsample back from 4000 -> 16000\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.ConvTranspose1d(64, 32, kernel_size=15, stride=2, padding=7, output_padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose1d(32, 16, kernel_size=15, stride=2, padding=7, output_padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(16, 1, kernel_size=15, stride=1, padding=7)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         # x: (batch, 1, 16000)\n",
    "#         encoded = self.encoder(x)  # -> (batch, 64, 4000)\n",
    "#         encoded_transposed = encoded.transpose(1, 2)  # -> (batch, 4000, 64)\n",
    "#         lstm_out, _ = self.lstm(encoded_transposed)      # -> (batch, 4000, 64)\n",
    "#         lstm_out = lstm_out.transpose(1, 2)                # -> (batch, 64, 4000)\n",
    "#         decoded = self.decoder(lstm_out)                   # -> (batch, 1, 16000)\n",
    "#         watermark_delta = 0.01 * torch.tanh(decoded)\n",
    "#         return watermark_delta\n",
    "\n",
    "# # --- Extended Generator with Optional 16-bit Message Embedding ---\n",
    "# class WatermarkGeneratorWithMessage(WatermarkGenerator):\n",
    "#     def __init__(self, message_bits=16):\n",
    "#         super(WatermarkGeneratorWithMessage, self).__init__()\n",
    "#         self.use_message = True\n",
    "#         self.message_bits = message_bits\n",
    "#         # Create a learnable embedding table for message bits:\n",
    "#         # For each bit (0 or 1) and for each bit position, produce an adjustment vector of size 64.\n",
    "#         # Shape: (2, message_bits, 64)\n",
    "#         self.embedding = nn.Parameter(torch.randn(2, message_bits, 64) * 0.01)\n",
    "        \n",
    "#     def forward(self, x, message):\n",
    "#         \"\"\"\n",
    "#         x: (batch, 1, 16000)\n",
    "#         message: (batch, message_bits) containing binary values (0 or 1)\n",
    "#         \"\"\"\n",
    "#         encoded = self.encoder(x)            # (batch, 64, 4000)\n",
    "#         encoded_transposed = encoded.transpose(1, 2)  # (batch, 4000, 64)\n",
    "#         lstm_out, _ = self.lstm(encoded_transposed)      # (batch, 4000, 64)\n",
    "#         # Incorporate message embedding:\n",
    "#         # For each sample, average embeddings for each bit.\n",
    "#         batch_size, t, feat = lstm_out.shape\n",
    "#         message = message.long()  # ensure integers 0 or 1\n",
    "#         message_embs = []\n",
    "#         for i in range(self.message_bits):\n",
    "#             # For each bit position i, select the corresponding embedding vector\n",
    "#             emb_i = self.embedding[:, i, :]  # shape (2, 64)\n",
    "#             # message[:, i] selects row 0 or 1 for each sample\n",
    "#             message_emb_i = emb_i[message[:, i]]  # shape (batch, 64)\n",
    "#             message_embs.append(message_emb_i)\n",
    "#         # Average over the message bits: shape (batch, 64)\n",
    "#         message_emb = torch.stack(message_embs, dim=1).mean(dim=1)\n",
    "#         # Expand message_emb to add to every timestep in the LSTM output:\n",
    "#         message_emb_expanded = message_emb.unsqueeze(1).expand(-1, t, -1)  # (batch, 4000, 64)\n",
    "#         lstm_out = lstm_out + message_emb_expanded\n",
    "#         lstm_out = lstm_out.transpose(1, 2)  # (batch, 64, 4000)\n",
    "#         decoded = self.decoder(lstm_out)  # (batch, 1, 16000)\n",
    "#         watermark_delta = 0.01 * torch.tanh(decoded)\n",
    "#         return watermark_delta\n",
    "\n",
    "# # --- Extended Detector to also Decode the 16-bit Message ---\n",
    "# class WatermarkDetectorWithMessage(nn.Module):\n",
    "#     def __init__(self, message_bits=16):\n",
    "#         super(WatermarkDetectorWithMessage, self).__init__()\n",
    "#         self.message_bits = message_bits\n",
    "#         # Use a similar encoder as before\n",
    "#         self.encoder = nn.Sequential(\n",
    "#             nn.Conv1d(1, 16, kernel_size=15, stride=1, padding=7),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(16, 32, kernel_size=15, stride=2, padding=7),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(32, 64, kernel_size=15, stride=2, padding=7),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "#         # Upsampling head: output 1 channel for detection, and message_bits channels for message decoding.\n",
    "#         self.upsample = nn.Sequential(\n",
    "#             nn.ConvTranspose1d(64, 32, kernel_size=15, stride=2, padding=7, output_padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose1d(32, 16, kernel_size=15, stride=2, padding=7, output_padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(16, 1 + message_bits, kernel_size=15, stride=1, padding=7)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # x: (batch, 1, 16000)\n",
    "#         encoded = self.encoder(x)   # (batch, 64, 4000)\n",
    "#         out = self.upsample(encoded)  # (batch, 1+message_bits, 16000)\n",
    "#         # First channel for detection probability\n",
    "#         detection = torch.sigmoid(out[:, 0:1, :])\n",
    "#         # The remaining channels for message decoding:\n",
    "#         # We average over the time dimension to get one prediction per bit.\n",
    "#         message_logits = out[:, 1:, :].mean(dim=2)  # (batch, message_bits)\n",
    "#         message_prob = torch.sigmoid(message_logits)  # Probabilities for each bit\n",
    "#         return detection, message_prob\n",
    "\n",
    "# #############################\n",
    "# # 2. Augmentation Functions\n",
    "# #############################\n",
    "\n",
    "# def apply_watermark_masking_balanced(s, s_w, num_segments=10):\n",
    "#     \"\"\"\n",
    "#     A revised augmentation function to produce a more balanced ground truth mask.\n",
    "#     For each sample, divide the audio into 'num_segments' segments and randomly \n",
    "#     set half of them to 0 (watermark dropped) and the other half remain 1.\n",
    "    \n",
    "#     Args:\n",
    "#         s: original audio tensor, shape (batch, 1, T)\n",
    "#         s_w: watermarked audio tensor (s + delta), shape (batch, 1, T)\n",
    "#         num_segments: total segments to partition each sample\n",
    "    \n",
    "#     Returns:\n",
    "#         s_w_aug: augmented watermarked audio (with some segments reverted)\n",
    "#         mask: ground truth mask of same shape (1 means watermark is intact, 0 means dropped)\n",
    "#     \"\"\"\n",
    "#     batch_size, channels, T = s_w.shape\n",
    "#     s_w_aug = s_w.clone()\n",
    "#     mask = torch.ones_like(s_w)  # start with all ones\n",
    "#     seg_length = T // num_segments\n",
    "    \n",
    "#     for b in range(batch_size):\n",
    "#         # Randomly select half the segments to drop watermark\n",
    "#         indices = list(range(num_segments))\n",
    "#         random.shuffle(indices)\n",
    "#         drop_indices = indices[:num_segments//2]  # drop watermark in half the segments\n",
    "#         for idx in drop_indices:\n",
    "#             start = idx * seg_length\n",
    "#             end = start + seg_length\n",
    "#             # Option: revert segment to original audio s (i.e., watermark removed)\n",
    "#             s_w_aug[b, :, start:end] = s[b, :, start:end]\n",
    "#             mask[b, :, start:end] = 0.0\n",
    "#     return s_w_aug, mask\n",
    "\n",
    "# def apply_adversarial_augmentation(x, noise_std=0.005):\n",
    "#     \"\"\"\n",
    "#     Apply a simple differentiable adversarial-like augmentation,\n",
    "#     such as adding random Gaussian noise or a simple filtering.\n",
    "#     This helps simulate real-world distortions.\n",
    "    \n",
    "#     Args:\n",
    "#         x: input audio tensor, shape (batch, 1, T)\n",
    "#         noise_std: standard deviation of Gaussian noise\n",
    "        \n",
    "#     Returns:\n",
    "#         x_aug: augmented audio tensor.\n",
    "#     \"\"\"\n",
    "#     noise = noise_std * torch.randn_like(x)\n",
    "#     x_aug = x + noise\n",
    "#     return x_aug\n",
    "\n",
    "# #############################\n",
    "# # 3. Training Loop\n",
    "# #############################\n",
    "\n",
    "# # Choose whether to use the message-embedding variant.\n",
    "# use_message = True\n",
    "\n",
    "# if use_message:\n",
    "#     # Instantiate extended models with 16-bit messages.\n",
    "#     generator = WatermarkGeneratorWithMessage(message_bits=16).to(device)\n",
    "#     detector  = WatermarkDetectorWithMessage(message_bits=16).to(device)\n",
    "#     # For demonstration, generate random binary messages for the batch.\n",
    "#     # In practice, these could come from the user.\n",
    "# else:\n",
    "#     generator = WatermarkGenerator().to(device)\n",
    "#     detector  = WatermarkDetector().to(device)\n",
    "\n",
    "# # Define loss functions:\n",
    "# criterion_perc = nn.L1Loss()    # Perceptual loss (original vs watermarked)\n",
    "# criterion_det  = nn.BCELoss()   # Detection loss for watermark presence\n",
    "# if use_message:\n",
    "#     criterion_msg = nn.BCELoss()   # For message decoding; we'll treat each bit as binary\n",
    "\n",
    "# # Loss weight factors (you can tune these)\n",
    "# lambda_perc = 10.0\n",
    "# lambda_det  = 1.0\n",
    "# lambda_msg  = 1.0  # weight for message decoding loss\n",
    "\n",
    "# optimizer = optim.Adam(list(generator.parameters()) + list(detector.parameters()), lr=1e-4)\n",
    "\n",
    "# num_epochs = 10\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "#     generator.train()\n",
    "#     detector.train()\n",
    "    \n",
    "#     for i, (s, _) in enumerate(subset_dataloader):  # using our 1% subset DataLoader\n",
    "#         s = s.to(device)  # (batch, 1, 16000)\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # Optionally, apply adversarial augmentation to original audio:\n",
    "#         s_aug = apply_adversarial_augmentation(s, noise_std=0.005)\n",
    "        \n",
    "#         # Generate watermark delta:\n",
    "#         if use_message:\n",
    "#             # Generate a random binary message for each sample, shape (batch, 16)\n",
    "#             batch_size = s.size(0)\n",
    "#             random_message = torch.randint(0, 2, (batch_size, 16)).float().to(device)\n",
    "#             delta = generator(s_aug, random_message)\n",
    "#         else:\n",
    "#             delta = generator(s_aug)\n",
    "        \n",
    "#         s_w = s + delta  # watermarked audio\n",
    "        \n",
    "#         # Apply watermark masking to get a balanced ground truth mask:\n",
    "#         s_w_aug, mask = apply_watermark_masking_balanced(s, s_w, num_segments=10)\n",
    "#         mask = mask.to(device)\n",
    "        \n",
    "#         # Perceptual loss: encourage watermarked audio to be similar to original\n",
    "#         loss_perc = criterion_perc(s_w, s)\n",
    "        \n",
    "#         # Detection loss: the detector should output 1 where watermark is present and 0 where dropped.\n",
    "#         if use_message:\n",
    "#             detection_output, message_output = detector(s_w_aug)\n",
    "#         else:\n",
    "#             detection_output = detector(s_w_aug)\n",
    "        \n",
    "#         loss_det = criterion_det(detection_output, mask)\n",
    "        \n",
    "#         if use_message:\n",
    "#             # For message loss, target is the random_message (broadcasted over batch)\n",
    "#             # Here we assume the detector outputs probabilities for each bit (shape: (batch, 16))\n",
    "#             loss_msg = criterion_msg(message_output, random_message)\n",
    "#             total_loss = lambda_perc * loss_perc + lambda_det * loss_det + lambda_msg * loss_msg\n",
    "#         else:\n",
    "#             total_loss = lambda_perc * loss_perc + lambda_det * loss_det\n",
    "        \n",
    "#         total_loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += total_loss.item()\n",
    "        \n",
    "#         if (i+1) % 20 == 0:\n",
    "#             print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(subset_dataloader)}], Loss: {total_loss.item():.4f}\")\n",
    "    \n",
    "#     avg_loss = running_loss / len(subset_dataloader)\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}] completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# # Save model weights after training\n",
    "# if use_message:\n",
    "#     torch.save(generator.state_dict(), \"watermark_generator_with_message.pth\")\n",
    "#     torch.save(detector.state_dict(), \"watermark_detector_with_message.pth\")\n",
    "# else:\n",
    "#     torch.save(generator.state_dict(), \"watermark_generator.pth\")\n",
    "#     torch.save(detector.state_dict(), \"watermark_detector.pth\")\n",
    "\n",
    "# #############################\n",
    "# # 4. Evaluation\n",
    "# #############################\n",
    "\n",
    "# generator.eval()\n",
    "# detector.eval()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for s, _ in subset_dataloader:\n",
    "#         s = s.to(device)\n",
    "#         if use_message:\n",
    "#             batch_size = s.size(0)\n",
    "#             random_message = torch.randint(0, 2, (batch_size, 16)).float().to(device)\n",
    "#             delta = generator(s, random_message)\n",
    "#         else:\n",
    "#             delta = generator(s)\n",
    "#         s_w = s + delta\n",
    "#         s_w_aug, mask = apply_watermark_masking_balanced(s, s_w, num_segments=10)\n",
    "#         mask = mask.to(device)\n",
    "#         if use_message:\n",
    "#             detection_output, message_output = detector(s_w_aug)\n",
    "#         else:\n",
    "#             detection_output = detector(s_w_aug)\n",
    "#         break\n",
    "\n",
    "# # Compute detection metrics (using our previous functions)\n",
    "# metrics = compute_all_metrics(detection_output, mask, threshold=0.5)\n",
    "# print(\"Detection Metrics:\")\n",
    "# print(f\"Accuracy: {metrics['accuracy']*100:.2f}%\")\n",
    "# print(f\"ROC AUC Score: {metrics['roc_auc']:.4f}\")\n",
    "# print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "# print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "# print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(metrics[\"confusion_matrix\"])\n",
    "\n",
    "# if use_message:\n",
    "#     # For message decoding, let's compute accuracy per bit\n",
    "#     # Binarize message_output with threshold 0.5\n",
    "#     pred_message = (message_output >= 0.5).float()\n",
    "#     # Assume random_message from evaluation above is our target message:\n",
    "#     target_message = random_message\n",
    "#     message_acc = (pred_message == target_message).float().mean().item()\n",
    "#     print(f\"Message Decoding Accuracy: {message_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from tqdm import tqdm \n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "#############################\n",
    "# 1. Model Architectures\n",
    "#############################\n",
    "\n",
    "# --- Original WatermarkGenerator (without message) ---\n",
    "class WatermarkGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WatermarkGenerator, self).__init__()\n",
    "        # Encoder: Downsample input from 16000 -> 4000 samples\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=15, stride=1, padding=7), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 32, kernel_size=15, stride=2, padding=7),  \n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=15, stride=2, padding=7),  \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Bottleneck: LSTM to capture temporal dependencies\n",
    "        self.lstm = nn.LSTM(input_size=64, hidden_size=64, num_layers=1, batch_first=True)\n",
    "        # Decoder: Upsample back from 4000 -> 16000\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=15, stride=2, padding=7, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(32, 16, kernel_size=15, stride=2, padding=7, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 1, kernel_size=15, stride=1, padding=7)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, 1, 16000)\n",
    "        encoded = self.encoder(x)  # -> (batch, 64, 4000)\n",
    "        encoded_transposed = encoded.transpose(1, 2)  # -> (batch, 4000, 64)\n",
    "        lstm_out, _ = self.lstm(encoded_transposed)      # -> (batch, 4000, 64)\n",
    "        lstm_out = lstm_out.transpose(1, 2)                # -> (batch, 64, 4000)\n",
    "        decoded = self.decoder(lstm_out)                   # -> (batch, 1, 16000)\n",
    "        watermark_delta = 0.01 * torch.tanh(decoded)\n",
    "        return watermark_delta\n",
    "\n",
    "# --- Extended Generator with Optional 16-bit Message Embedding ---\n",
    "class WatermarkGeneratorWithMessage(WatermarkGenerator):\n",
    "    def __init__(self, message_bits=16):\n",
    "        super(WatermarkGeneratorWithMessage, self).__init__()\n",
    "        self.use_message = True\n",
    "        self.message_bits = message_bits\n",
    "        # Create a learnable embedding table for message bits:\n",
    "        # For each bit (0 or 1) and for each bit position, produce an adjustment vector of size 64.\n",
    "        # Shape: (2, message_bits, 64)\n",
    "        self.embedding = nn.Parameter(torch.randn(2, message_bits, 64) * 0.01)\n",
    "        \n",
    "    def forward(self, x, message):\n",
    "        \"\"\"\n",
    "        x: (batch, 1, 16000)\n",
    "        message: (batch, message_bits) containing binary values (0 or 1)\n",
    "        \"\"\"\n",
    "        encoded = self.encoder(x)            # (batch, 64, 4000)\n",
    "        encoded_transposed = encoded.transpose(1, 2)  # (batch, 4000, 64)\n",
    "        lstm_out, _ = self.lstm(encoded_transposed)      # (batch, 4000, 64)\n",
    "        # Incorporate message embedding:\n",
    "        # For each sample, average embeddings for each bit.\n",
    "        batch_size, t, feat = lstm_out.shape\n",
    "        message = message.long()  # ensure integers 0 or 1\n",
    "        message_embs = []\n",
    "        for i in range(self.message_bits):\n",
    "            # For each bit position i, select the corresponding embedding vector\n",
    "            emb_i = self.embedding[:, i, :]  # shape (2, 64)\n",
    "            # message[:, i] selects row 0 or 1 for each sample\n",
    "            message_emb_i = emb_i[message[:, i]]  # shape (batch, 64)\n",
    "            message_embs.append(message_emb_i)\n",
    "        # Average over the message bits: shape (batch, 64)\n",
    "        message_emb = torch.stack(message_embs, dim=1).mean(dim=1)\n",
    "        # Expand message_emb to add to every timestep in the LSTM output:\n",
    "        message_emb_expanded = message_emb.unsqueeze(1).expand(-1, t, -1)  # (batch, 4000, 64)\n",
    "        lstm_out = lstm_out + message_emb_expanded\n",
    "        lstm_out = lstm_out.transpose(1, 2)  # (batch, 64, 4000)\n",
    "        decoded = self.decoder(lstm_out)  # (batch, 1, 16000)\n",
    "        watermark_delta = 0.01 * torch.tanh(decoded)\n",
    "        return watermark_delta\n",
    "\n",
    "# --- Extended Detector to also Decode the 16-bit Message ---\n",
    "class WatermarkDetectorWithMessage(nn.Module):\n",
    "    def __init__(self, message_bits=16):\n",
    "        super(WatermarkDetectorWithMessage, self).__init__()\n",
    "        self.message_bits = message_bits\n",
    "        # Use a similar encoder as before\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=15, stride=1, padding=7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 32, kernel_size=15, stride=2, padding=7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=15, stride=2, padding=7),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Upsampling head: output 1 channel for detection, and message_bits channels for message decoding.\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=15, stride=2, padding=7, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(32, 16, kernel_size=15, stride=2, padding=7, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 1 + message_bits, kernel_size=15, stride=1, padding=7)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, 1, 16000)\n",
    "        encoded = self.encoder(x)   # (batch, 64, 4000)\n",
    "        out = self.upsample(encoded)  # (batch, 1+message_bits, 16000)\n",
    "        # First channel for detection probability\n",
    "        detection = torch.sigmoid(out[:, 0:1, :])\n",
    "        # The remaining channels for message decoding:\n",
    "        # We average over the time dimension to get one prediction per bit.\n",
    "        message_logits = out[:, 1:, :].mean(dim=2)  # (batch, message_bits)\n",
    "        message_prob = torch.sigmoid(message_logits)  # Probabilities for each bit\n",
    "        return detection, message_prob\n",
    "\n",
    "#############################\n",
    "# 2. Augmentation Functions\n",
    "#############################\n",
    "\n",
    "def apply_watermark_masking_balanced(s, s_w, num_segments=10):\n",
    "    batch_size, channels, T = s_w.shape\n",
    "    s_w_aug = s_w.clone()\n",
    "    mask = torch.ones_like(s_w)  # start with all ones\n",
    "    seg_length = T // num_segments\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        # Randomly select half the segments to drop watermark\n",
    "        indices = list(range(num_segments))\n",
    "        random.shuffle(indices)\n",
    "        drop_indices = indices[:num_segments//2]  # drop watermark in half the segments\n",
    "        for idx in drop_indices:\n",
    "            start = idx * seg_length\n",
    "            end = start + seg_length\n",
    "            # Option: revert segment to original audio s (i.e., watermark removed)\n",
    "            s_w_aug[b, :, start:end] = s[b, :, start:end]\n",
    "            mask[b, :, start:end] = 0.0\n",
    "    return s_w_aug, mask\n",
    "\n",
    "def apply_adversarial_augmentation(x, noise_std=0.005):\n",
    "    \"\"\"\n",
    "    Apply a simple differentiable adversarial-like augmentation,\n",
    "    such as adding random Gaussian noise or a simple filtering.\n",
    "    This helps simulate real-world distortions.\n",
    "    \"\"\"\n",
    "    noise = noise_std * torch.randn_like(x)\n",
    "    x_aug = x + noise\n",
    "    return x_aug\n",
    "\n",
    "#############################\n",
    "# 3. Training Loop\n",
    "#############################\n",
    "\n",
    "# Choose whether to use the message-embedding variant.\n",
    "use_message = True\n",
    "\n",
    "if use_message:\n",
    "    generator = WatermarkGeneratorWithMessage(message_bits=16).to(device)\n",
    "    detector  = WatermarkDetectorWithMessage(message_bits=16).to(device)\n",
    "else:\n",
    "    generator = WatermarkGenerator().to(device)\n",
    "    detector  = WatermarkDetector().to(device)\n",
    "\n",
    "# Define loss functions:\n",
    "criterion_perc = nn.L1Loss()    # Perceptual loss (original vs watermarked)\n",
    "criterion_det  = nn.BCELoss()   # Detection loss for watermark presence\n",
    "if use_message:\n",
    "    criterion_msg = nn.BCELoss()   # For message decoding; we'll treat each bit as binary\n",
    "\n",
    "# Loss weight factors (you can tune these)\n",
    "lambda_perc = 10.0\n",
    "lambda_det  = 1.0\n",
    "lambda_msg  = 1.0  # weight for message decoding loss\n",
    "\n",
    "optimizer = optim.Adam(list(generator.parameters()) + list(detector.parameters()), lr=1e-4)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    generator.train()\n",
    "    detector.train()\n",
    "    \n",
    "    # Create a tqdm progress bar for the current epoch\n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    for i, (s, _) in pbar:\n",
    "        s = s.to(device)  # (batch, 1, 16000)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Optionally, apply adversarial augmentation to original audio:\n",
    "        s_aug = apply_adversarial_augmentation(s, noise_std=0.005)\n",
    "        \n",
    "        # Generate watermark delta:\n",
    "        if use_message:\n",
    "            # Generate a random binary message for each sample, shape (batch, 16)\n",
    "            batch_size = s.size(0)\n",
    "            random_message = torch.randint(0, 2, (batch_size, 16)).float().to(device)\n",
    "            delta = generator(s_aug, random_message)\n",
    "        else:\n",
    "            delta = generator(s_aug)\n",
    "        \n",
    "        s_w = s + delta  # watermarked audio\n",
    "        \n",
    "        # Apply watermark masking to get a balanced ground truth mask:\n",
    "        s_w_aug, mask = apply_watermark_masking_balanced(s, s_w, num_segments=10)\n",
    "        mask = mask.to(device)\n",
    "        \n",
    "        # Perceptual loss: encourage watermarked audio to be similar to original\n",
    "        loss_perc = criterion_perc(s_w, s)\n",
    "        \n",
    "        # Detection loss: the detector should output 1 where watermark is present and 0 where dropped.\n",
    "        if use_message:\n",
    "            detection_output, message_output = detector(s_w_aug)\n",
    "        else:\n",
    "            detection_output = detector(s_w_aug)\n",
    "        loss_det = criterion_det(detection_output, mask)\n",
    "        \n",
    "        if use_message:\n",
    "            # For message loss, target is the random_message\n",
    "            loss_msg = criterion_msg(message_output, random_message)\n",
    "            total_loss = lambda_perc * loss_perc + lambda_det * loss_det + lambda_msg * loss_msg\n",
    "        else:\n",
    "            total_loss = lambda_perc * loss_perc + lambda_det * loss_det\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += total_loss.item()\n",
    "        \n",
    "        # Update the progress bar with the current loss value\n",
    "        pbar.set_postfix(loss=total_loss.item())\n",
    "    \n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "# Save model weights after training\n",
    "if use_message:\n",
    "    torch.save(generator.state_dict(), \"watermark_generator_with_message.pth\")\n",
    "    torch.save(detector.state_dict(), \"watermark_detector_with_message.pth\")\n",
    "else:\n",
    "    torch.save(generator.state_dict(), \"watermark_generator.pth\")\n",
    "    torch.save(detector.state_dict(), \"watermark_detector.pth\")\n",
    "\n",
    "#############################\n",
    "# 4. Evaluation\n",
    "#############################\n",
    "\n",
    "generator.eval()\n",
    "detector.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for s, _ in dataloader:\n",
    "        s = s.to(device)\n",
    "        if use_message:\n",
    "            batch_size = s.size(0)\n",
    "            random_message = torch.randint(0, 2, (batch_size, 16)).float().to(device)\n",
    "            delta = generator(s, random_message)\n",
    "        else:\n",
    "            delta = generator(s)\n",
    "        s_w = s + delta\n",
    "        s_w_aug, mask = apply_watermark_masking_balanced(s, s_w, num_segments=10)\n",
    "        mask = mask.to(device)\n",
    "        if use_message:\n",
    "            detection_output, message_output = detector(s_w_aug)\n",
    "        else:\n",
    "            detection_output = detector(s_w_aug)\n",
    "        break\n",
    "\n",
    "# Compute detection metrics (using our previous functions)\n",
    "metrics = compute_all_metrics(detection_output, mask, threshold=0.5)\n",
    "print(\"Detection Metrics:\")\n",
    "print(f\"Accuracy: {metrics['accuracy']*100:.2f}%\")\n",
    "print(f\"ROC AUC Score: {metrics['roc_auc']:.4f}\")\n",
    "print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(metrics[\"confusion_matrix\"])\n",
    "\n",
    "if use_message:\n",
    "    pred_message = (message_output >= 0.5).float()\n",
    "    # Assume random_message from evaluation above is our target message:\n",
    "    target_message = random_message\n",
    "    message_acc = (pred_message == target_message).float().mean().item()\n",
    "    print(f\"Message Decoding Accuracy: {message_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# Assuming 'audio_dataset' is an instance of your AudioDataset class\n",
    "dataset_size = len(audio_dataset)\n",
    "test_size = int(0.1 * dataset_size)   # For example, use 10% of the data for testing\n",
    "train_size = dataset_size - test_size\n",
    "\n",
    "# Split the dataset randomly into training and testing sets\n",
    "train_dataset, test_dataset = random_split(audio_dataset, [train_size, test_size])\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "print(\"Test DataLoader created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_message = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Define SI-SNR computation (for imperceptibility evaluation)\n",
    "def compute_si_snr(s, s_hat, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Compute the Scale-Invariant Signal-to-Noise Ratio (SI-SNR) between original s and watermarked s_hat.\n",
    "    s, s_hat: tensors of shape (batch, 1, T)\n",
    "    Returns: average SI-SNR in dB.\n",
    "    \"\"\"\n",
    "    # Flatten each audio sample to (batch, T)\n",
    "    s = s.view(s.size(0), -1)\n",
    "    s_hat = s_hat.view(s_hat.size(0), -1)\n",
    "    # Compute the scaling factor\n",
    "    scale = torch.sum(s_hat * s, dim=1, keepdim=True) / (torch.sum(s * s, dim=1, keepdim=True) + eps)\n",
    "    s_target = scale * s\n",
    "    e = s_hat - s_target\n",
    "    si_snr = 10 * torch.log10(torch.sum(s_target ** 2, dim=1) / (torch.sum(e ** 2, dim=1) + eps))\n",
    "    return si_snr.mean().item()\n",
    "\n",
    "# ============================\n",
    "# Comprehensive Evaluation Code\n",
    "# ============================\n",
    "\n",
    "# Assume your trained models are saved or currently loaded and in evaluation mode.\n",
    "generator.eval()\n",
    "detector.eval()\n",
    "\n",
    "# Choose your evaluation DataLoader. Here we use 'test_dataloader'\n",
    "# It can be your holdout set or the subset you reserved for evaluation.\n",
    "si_snr_total = 0.0\n",
    "batch_count = 0\n",
    "\n",
    "# Containers to aggregate detection predictions and targets over the test set\n",
    "all_detection_preds = []\n",
    "all_detection_targets = []\n",
    "if use_message:\n",
    "    all_message_preds = []\n",
    "    all_message_targets = []\n",
    "\n",
    "# Iterate over the evaluation dataset\n",
    "with torch.no_grad():\n",
    "    for s, _ in test_dataloader:  # Replace 'test_dataloader' with your evaluation loader\n",
    "        s = s.to(device)  # (batch, 1, 16000)\n",
    "        if use_message:\n",
    "            batch_size = s.size(0)\n",
    "            # Generate a random binary message for evaluation; shape: (batch, 16)\n",
    "            message = torch.randint(0, 2, (batch_size, 16)).float().to(device)\n",
    "            delta = generator(s, message)\n",
    "        else:\n",
    "            delta = generator(s)\n",
    "        s_w = s + delta\n",
    "        \n",
    "        # Apply balanced watermark masking to get a balanced ground truth mask\n",
    "        s_w_aug, mask = apply_watermark_masking_balanced(s, s_w, num_segments=10)\n",
    "        mask = mask.to(device)\n",
    "        \n",
    "        # Obtain detector output; if using message embedding, get both detection and message outputs.\n",
    "        if use_message:\n",
    "            detection_output, message_output = detector(s_w_aug)\n",
    "        else:\n",
    "            detection_output = detector(s_w_aug)\n",
    "        \n",
    "        # Compute SI-SNR for this batch (comparing original vs. watermarked audio)\n",
    "        batch_si_snr = compute_si_snr(s, s_w)\n",
    "        si_snr_total += batch_si_snr\n",
    "        batch_count += 1\n",
    "        \n",
    "        # Collect detection outputs and ground truth masks\n",
    "        all_detection_preds.append(detection_output)\n",
    "        all_detection_targets.append(mask)\n",
    "        \n",
    "        # Collect message outputs and targets if using message embedding\n",
    "        if use_message:\n",
    "            all_message_preds.append(message_output)\n",
    "            all_message_targets.append(message)\n",
    "        \n",
    "        break  # Process one batch for this example\n",
    "\n",
    "# Concatenate outputs across batches\n",
    "all_detection_preds = torch.cat(all_detection_preds, dim=0)\n",
    "all_detection_targets = torch.cat(all_detection_targets, dim=0)\n",
    "detection_metrics = compute_all_metrics(all_detection_preds, all_detection_targets, threshold=0.5)\n",
    "average_si_snr = si_snr_total / batch_count\n",
    "\n",
    "print(f\"Average SI-SNR over evaluation set: {average_si_snr:.2f} dB\")\n",
    "print(\"Detection Metrics:\")\n",
    "print(f\"Accuracy: {detection_metrics['accuracy']*100:.2f}%\")\n",
    "print(f\"ROC AUC Score: {detection_metrics['roc_auc']:.4f}\")\n",
    "print(f\"Precision: {detection_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {detection_metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score: {detection_metrics['f1']:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(detection_metrics[\"confusion_matrix\"])\n",
    "\n",
    "if use_message:\n",
    "    # Concatenate message predictions and targets and compute accuracy\n",
    "    all_message_preds = torch.cat(all_message_preds, dim=0)\n",
    "    all_message_targets = torch.cat(all_message_targets, dim=0)\n",
    "    pred_message = (all_message_preds >= 0.5).float()\n",
    "    message_acc = (pred_message == all_message_targets).float().mean().item()\n",
    "    print(f\"Message Decoding Accuracy: {message_acc*100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "#############################\n",
    "# Utility Functions for Segmentation & Reassembly\n",
    "#############################\n",
    "\n",
    "def segment_audio(waveform, segment_length=16000):\n",
    "    segments = []\n",
    "    total_samples = waveform.shape[1]\n",
    "    for start in range(0, total_samples, segment_length):\n",
    "        end = start + segment_length\n",
    "        segment = waveform[:, start:end]\n",
    "        if segment.shape[1] < segment_length:\n",
    "            pad_amount = segment_length - segment.shape[1]\n",
    "            segment = F.pad(segment, (0, pad_amount))\n",
    "        segments.append(segment)\n",
    "    return segments\n",
    "\n",
    "def reassemble_audio(segments):\n",
    "    return torch.cat(segments, dim=1)\n",
    "\n",
    "use_message = True  # Set to True to use the message embedding variant\n",
    "\n",
    "if use_message:\n",
    "    generator = WatermarkGeneratorWithMessage(message_bits=16).to(device)\n",
    "    detector  = WatermarkDetectorWithMessage(message_bits=16).to(device)\n",
    "    # Optionally, load saved weights:\n",
    "    # generator.load_state_dict(torch.load(\"watermark_generator_with_message.pth\"))\n",
    "    # detector.load_state_dict(torch.load(\"watermark_detector_with_message.pth\"))\n",
    "else:\n",
    "    generator = WatermarkGenerator().to(device)\n",
    "    detector  = WatermarkDetector().to(device)\n",
    "    # Optionally, load saved weights:\n",
    "    # generator.load_state_dict(torch.load(\"watermark_generator.pth\"))\n",
    "    # detector.load_state_dict(torch.load(\"watermark_detector.pth\"))\n",
    "\n",
    "generator.eval()\n",
    "detector.eval()\n",
    "\n",
    "#############################\n",
    "# 2. Load an Audio File and Preprocess\n",
    "#############################\n",
    "audio_filepath = \"file_example_WAV_1MG.wav\"  # Update with your file path\n",
    "waveform, sample_rate = torchaudio.load(audio_filepath)\n",
    "print(\"Original audio shape:\", waveform.shape, \"Sample Rate:\", sample_rate)\n",
    "\n",
    "# Convert to mono if stereo (average channels)\n",
    "if waveform.shape[0] > 1:\n",
    "    waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    print(\"Converted to mono:\", waveform.shape)\n",
    "\n",
    "# Resample to 16 kHz if needed.\n",
    "if sample_rate != 16000:\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "    waveform = resampler(waveform)\n",
    "    sample_rate = 16000\n",
    "    print(\"Resampled audio shape:\", waveform.shape)\n",
    "\n",
    "# IMPORTANT: Do not crop/pad the waveform to 1 second.\n",
    "# We want to segment the entire waveform.\n",
    "total_length = waveform.shape[1]\n",
    "print(f\"Total audio length (in samples): {total_length}, which is {total_length/16000:.2f} seconds.\")\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Segment the Audio into 1-second Clips\n",
    "# ---------------------------\n",
    "segments = segment_audio(waveform, segment_length=16000)\n",
    "print(f\"Audio segmented into {len(segments)} segments.\")\n",
    "\n",
    "#############################\n",
    "# 4. Watermark Each Segment and Reassemble\n",
    "#############################\n",
    "watermarked_segments = []\n",
    "for segment in segments:\n",
    "    # Add batch dimension: shape becomes (1, 1, 16000)\n",
    "    segment_batch = segment.unsqueeze(0).to(device)\n",
    "    if use_message:\n",
    "        # Define a custom 16-bit message for each segment.\n",
    "        # For example, here we use the same custom message for all segments.\n",
    "        custom_message_list = [1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0]\n",
    "        custom_message = torch.tensor(custom_message_list, dtype=torch.float).unsqueeze(0).to(device)\n",
    "        delta = generator(segment_batch, custom_message)\n",
    "    else:\n",
    "        delta = generator(segment_batch)\n",
    "    watermarked_segment = segment_batch + delta\n",
    "    watermarked_segments.append(watermarked_segment.squeeze(0).cpu().detach())\n",
    "\n",
    "# Reassemble the watermarked segments\n",
    "watermarked_audio_full = reassemble_audio(watermarked_segments)\n",
    "print(\"Full watermarked audio shape (samples):\", watermarked_audio_full.shape)\n",
    "print(\"Full watermarked audio duration (seconds):\", watermarked_audio_full.shape[1]/16000)\n",
    "\n",
    "# Save the full watermarked audio to disk\n",
    "torchaudio.save(\"watermarked_audio.wav\", watermarked_audio_full, sample_rate)\n",
    "print(\"Watermarked audio saved as 'watermarked_audio.wav'.\")\n",
    "\n",
    "#############################\n",
    "# 5. Run the Detector on the Full Watermarked Audio (Segment-wise Processing)\n",
    "#############################\n",
    "# Here, we'll process each segment with the detector and then reassemble the detector outputs.\n",
    "detector_outputs = []\n",
    "decoded_messages = []  # Only if using message embedding\n",
    "\n",
    "for segment in watermarked_segments:\n",
    "    segment_batch = segment.unsqueeze(0).to(device)  # (1, 1, 16000)\n",
    "    if use_message:\n",
    "        detection_output, message_output = detector(segment_batch)\n",
    "        decoded_messages.append(message_output.cpu().detach())\n",
    "    else:\n",
    "        detection_output = detector(segment_batch)\n",
    "    detector_outputs.append(detection_output.cpu().detach())\n",
    "\n",
    "# Reassemble detector outputs along the time dimension\n",
    "detector_full_output = reassemble_audio([seg.squeeze(0) for seg in detector_outputs])\n",
    "print(\"Full detector output shape:\", detector_full_output.shape)\n",
    "\n",
    "#############################\n",
    "# 6. Visualization for the First Segment\n",
    "#############################\n",
    "watermarked_np = watermarked_segments[0].squeeze(0).detach().cpu().numpy()\n",
    "detection_np   = detector_outputs[0].squeeze(0).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(watermarked_np)\n",
    "plt.title(\"Watermarked Audio Waveform (Segment 1)\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(detection_np)\n",
    "plt.title(\"Detector Output (Probability) (Segment 1)\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#############################\n",
    "# 7. (If Using Message Embedding) Evaluate Message Decoding for the First Segment\n",
    "#############################\n",
    "if use_message:\n",
    "    decoded_message = (decoded_messages[0] >= 0.5).float()\n",
    "    print(\"Custom input message:\")\n",
    "    print(custom_message[0].cpu().numpy())\n",
    "    print(\"Decoded message from detector (Segment 1):\")\n",
    "    print(decoded_message[0].cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
